---
title: "Proyecto_parte2"
author: "Grupo 6: Gonzalo Cruz Gómez, Samuel Martínez Lorente, Jorge Tordesillas García"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: flatly # Tema
    code_folding: hide
    number_sections: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = 'center')

# Cargamos las librerías 
library(tidyverse)
library(data.table)
library(knitr)
library(caret)
library(pROC)
library(gbm)
library(xgboost)
# library(adabag)
library(rpart)
library(ggplot2)
library(randomForest)
```

# Indice

-   Introducción
-   Carga de datos
-   Regresión logística (Jorge)
-   K-nn (Gonzalo)
-   Análisis discriminante lineal (Jorge)
-   Árboles de decisión (Jorge)
-   Bagging (Samuel)
-   Boosting (Gonzalo)
-   Naive Bayes (Samuel)
-   Evaluación y comparación
-   Conclusiones

# 1. Introducción

# 2. Carga de datos

Cargamos los datos y hacemos la division train-test

```{r cargar datos}

# Cargar datos
bank_data <- read.csv('Bank Customer Churn Prediction.csv')
head(bank_data)

# Verificar estructura de los datos
str(bank_data)
```

```{r split train-test}
set.seed(42)
train_index <- createDataPartition(bank_data$churn, p = 0.8, list = FALSE, times = 1)
train_data <- bank_data[train_index, ]
test_data <- bank_data[-train_index, ]

cat("Dimensiones - Train Data:", dim(train_data), "\n")
cat("Dimensiones - Test Data:", dim(test_data), "\n")
```

# 3. Regresión logística


Estudiamos la influencia de active_member en churn
```{r}
tabla1 <- table(train_data$active_member, train_data$churn)
tabla1
```

```{r}
chisq.test(tabla1)
```
Podemos ver que existe una relación relevante entre ambas variables ya que es p-valor es claramente menor a 0.05. 

Usamos la regresión logística para cuantificar la relación:
```{r}
# Modelo con active_member
logit1 <- glm(churn ~ active_member, 
              data = train_data, 
              family = binomial(link = "logit"))

summary(logit1)

# Odds Ratios
exp(cbind(OR = coef(logit1), confint.default(logit1)))
```
Como vemos la variable active_members tiene alta significatividad en la variable respuesta churn. La estimación es negativa, lo que indica que ser miembro activo disminuye la probabilidad de churn(irse del bank_data).

Como el OR de active_member es menor que 1, es un factor protector contra el churn. Ser miembro activo del bank_data reduce de forma significativa la probabilidad de que un cliente se dé de baja del bank_data. La odds ratioo de 0.43... indica que las probabilidades de churn se reducen casi a la mitad cuando el cliente es activo.

```{r}
confint.default(logit1)
```


Ahora vamos a hacer un modelo con cada variable con la variable objetivo churn para ver como se comportan.
```{r}
# Convertir variables categóricas
convertir_variables <- function(data) {
  data %>%
    mutate(
      active_member = factor(active_member, levels = c(0, 1)),
      credit_card = factor(credit_card, levels = c(0, 1)),
      country = factor(country),
      gender = factor(gender)
    )
}

train_data <- convertir_variables(train_data)
test_data <- convertir_variables(test_data)

# Lista de variables predictoras
variables <- c("age", "tenure", "credit_score", "country", 
               "gender", "balance", "products_number", 
               "credit_card", "active_member", "estimated_salary")

# Análisis univariante
resultados_univariante <- list()

for (var in variables) {
  formula <- as.formula(paste("churn ~", var))
  modelo <- glm(formula, data = train_data, family = binomial)
  
  # Almacenar resultados
  resultados_univariante[[var]] <- list(
    summary = summary(modelo),
    OR = exp(cbind(OR = coef(modelo), confint.default(modelo))))
}

# Mostrar resultado
for (var in variables) {
  cat("\nVariable:", var, "\n")
  print(resultados_univariante[[var]]$summary)
  cat("\nOdds Ratio e IC 95%:\n")
  print(resultados_univariante[[var]]$OR)
}

```
Interpretación de resultados:
  
  Variable: age
p-value < 2e-16 → altamente significativa
OR = 1.072 → Por cada año adicional, las odds de abandono (churn) aumentan un 7.2%
Significativa

Variable: tenure
p-value = 0.126 → no significativa
OR = 0.98 → ligera disminución del riesgo con más años, pero no concluyente
No significativa

Variable: credit_score
p-value = 0.417 → no significativa
OR ≈ 1 → efecto mínimo/neutro
No significativa

Variable: country
countryGermany: p-value < 2e-16, OR = 2.49 → los clientes alemanes tienen 149% más odds de churn que los de referencia
countrySpain: p-value = 0.246, OR = 1.11 → no significativa
Solo countryGermany es significativa

Variable: gender
p-value < 2e-16, OR = 0.577 → ser hombre reduce las odds de churn en un 42.3%
Significativa

Variable: balance
p-value < 2e-16, OR muy cercana a 1 pero significativa → valores altos de balance se asocian positivamente con churn
Significativa

Variable: products_number
p-value = 0.006, OR = 0.846 → cada producto extra reduce las odds de churn en un 15.4%
Significativa

Variable: credit_card
p-value = 0.206, OR = 0.91 → no significativa
No significativa

Variable: active_member
p-value < 2e-16, OR = 0.47 → ser miembro activo reduce las odds de churn un 53%
Significativa

Variable: estimated_salary
p-value = 0.493, OR ≈ 1 → sin efecto aparente
No significativa

Conclusión general:
  Variables significativas individualmente:
  
  age

countryGermany

gender

balance

products_number

active_member

Variables no significativas:
  
  tenure

credit_score

countrySpain

credit_card

estimated_salary

A continuación. ajustamos un modelo de regresión logística con todas las carecterísticas significativas.
```{r}
modelo_completo <- glm(churn ~ age + country + gender + balance + products_number + active_member, data = train_data, family = "binomial")
summary(modelo_completo)
```

En un modelo con todas las variables deja de ser significante la variable de countrySpain al producirse la interacción con las demás variables


```{r}
table(train_data$factor.churn)
```
Esto nos indica que el modelo está sesgado hacia la clase "no".


Calculamos la probabilidad que ofrece el modelo para cada una de las observaciones en la muestra de entrenamiento
```{r}
# Predicciones
predicciones <- predict(modelo_completo, type = "response")

# Histograma de probabilidades
hist(predicciones, main = "Distribución de probabilidades predichas",
     xlab = "Probabilidad de Churn", ylab = "Frecuencia")
```
En este histograma que hemos creado sobre la distribución de las probabilidades predichas por el modelo de regresión logística que hemos creado con las variables binomiales significantes que hemos creado, vemos que la mayoría de las predicciones están entre 0.1 y 0.2, lo que significa que la mauoría de los clientes tienen baja probabilidad de irse. Además no hay clientes con altas probabilidades de irse.

```{r}
library(caret)

# Asegurar que ambos factores tengan los mismos niveles
clase_pred <- factor(ifelse(predicciones > 0.5, 1, 0), 
                    levels = c(0, 1))

# Convertir train_data$churn a factor con los mismos niveles (si no lo está)
train_data$churn <- factor(train_data$churn, levels = c(0, 1))
test_data$churn <- factor(test_data$churn, levels = c(0, 1))

# Verificar niveles
levels(clase_pred)
levels(train_data$churn)

# Ahora ejecutar la matriz de confusión
conf_matrix <- confusionMatrix(clase_pred, train_data$churn, positive = "1")
print(conf_matrix)
```
 Análisis de matriz de confusión CORREGIDO
Esta matriz de confusión muestra:
- Verdaderos Negativos (VN): 6161 (predicción correcta de "no churn")
- Falsos Positivos (FP): 220 (predicción incorrecta de "churn")
- Falsos Negativos (FN): 1285 (casos de churn no detectados)
- Verdaderos Positivos (VP): 334 (churn correctamente identificados)

El modelo tiene un accuracy del 81.2%, pero presenta un grave problema de sensibilidad: 
- Solo detecta el 20.6% de los casos reales de churn (335 falsos negativos)
- Aunque identifica correctamente el 96.5% de los "no churn" (alta especificidad)

Esto indica que el modelo es demasiado conservador, priorizando evitar falsas alarmas (FP) a costa de perder muchos casos reales (FN).


```{r}
# Cálculo de métricas CORREGIDO
confmat1 <- confusionMatrix(
  data = factor(clase_pred, levels = c("0", "1")),  # Asegurar niveles consistentes
  reference = factor(train_data$churn, levels = c("0", "1")),  # Usar misma codificación
  positive = "1"  # Clase positiva correctamente especificada
)

# Cálculo F1 Score optimizado
Precision <- confmat1$byClass["Pos Pred Value"]
Recall <- confmat1$byClass["Sensitivity"]
F1score <- ifelse((Precision + Recall) == 0, 0, 
                 2 * (Precision * Recall) / (Precision + Recall))

cat("\nMétrica F1 Score:", round(F1score, 3), 
    "\n(Valor bajo indica desbalance entre precisión y recall)\n")
```
El F1Score es un indicador de balance entre precisión y recall. Un 0.33 es un balance muy bajo, lo que indica que el modelo no está haciendo un gran trabajo prediciendo quién se va a dar de baja.

```{r}
# Predicciones
predicciones_test <- predict(modelo_completo, newdata = test_data, type = "response")

clase_pred_test <- factor(ifelse(predicciones_test > 0.5, 1, 0), 
                    levels = c(0, 1))

# Cálculo de métricas CORREGIDO
confmat2 <- confusionMatrix(
  data = factor(clase_pred_test, levels = c("0", "1")),  # Asegurar niveles consistentes
  reference = factor(test_data$churn, levels = c("0", "1")),  # Usar misma codificación
  positive = "1"  # Clase positiva correctamente especificada
)

print(confmat2)

# Cálculo F1 Score optimizado
Precision <- confmat2$byClass["Pos Pred Value"]
Recall <- confmat2$byClass["Sensitivity"]
F1score <- ifelse((Precision + Recall) == 0, 0, 
                 2 * (Precision * Recall) / (Precision + Recall))

cat("\nMétrica F1 Score:", round(F1score, 3), 
    "\n(Valor bajo indica desbalance entre precisión y recall)\n")
```



Calculamos la curva ROC.
```{r}
library(pROC)

roc_score=roc(test_data$churn, predicciones_test, plot=TRUE,print.auc=TRUE)
```
# 4. K-NN

Vamos a aplicar ahora el algoritmo de k vecinos más cercanos (k-nn).
Para la elección de k utilizaremos cross validation con 10 folds. Con
esta k entrenaremos el modelo k-nn en nuestro dataset de entrenamiento
para luego aplicarlo a nuestros datos de test y ver cómo de bueno es
prediciendo.

```{r k-nn}
# Convertimos churn a factor
train_data$churn <- as.factor(train_data$churn)
levels(train_data$churn) <- make.names(levels(train_data$churn))

# Hacemos grid search y cross validation para elegir la k
set.seed(42)
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)

# Entrenamos el modelo con los datos de entrenamiento utilizando accuracy como nuestra métrica
modelo_knn <- train(
  churn ~ ., data = train_data, method = "knn",
  trControl = train_control, metric = "Accuracy",
  preProcess = c("center", "scale"), 
  tuneLength = 10
)

cat("Resultados k-NN\n"); print(modelo_knn)
cat("Mejor k:", modelo_knn$bestTune$k, "\n")
plot(modelo_knn, main = "Rendimiento k-NN según la elección de k")
summary(modelo_knn)
```

Vemos como el número ideal de vecinos es 23, lo hemos elegido ya que es
el que tiene mayor ROC

Ahora vamos a ver como funciona el algoritmo en nuestros datos de
entrenamiento, para ello vamos a utilizar la matriz de confusión y vamos
a analizar las distintas métricas que podemos extraer de ahí

```{r k-nn train}
set.seed(123)
# Hacemos la predicción con nuestros datos de train
prediction <- predict(modelo_knn, newdata = train_data)
# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
cf <- confusionMatrix(
    data = prediction,                    
    reference = train_data$churn,        
    positive = levels(train_data$churn)[2]          
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf)
```

Tenemos una buena accuracy, de un 84%, pero esto puede llevarnos a
confusión, ya que nuestros datos están ciertamente desbalanceados
(tenemos muchos más datos de clientes que no se van del bank_data que
clientes que si que lo hacen). Es por ello que debemos fijarnos en otras
métricas. Lo desbalanceado de los datos también se ve en el test de
Mcnemar, el cual hace el ratio entre FP y FN. Al ser tan pequeño, nos
demuestra que nuestros datos están muy desbalanceados y que hacemos
muchos más fallos al predecir positivos que negativos. Nuestro algoritmo
de K-nn es bueno prediciendo cuando un cliente se va a quedar en el
bank_data, esto lo vemos con la NIR, que es de un 79% y con la
especificidad, que es de un 98%, con lo que predecimos bien un 98% de
los casos de clientes que se quedan. En cambio, nuestra Recall
(sensitivity) es demasiado baja, no predecimos correctamente los
positivos. Como el modelo falla en aproximadamente un 75% de las
predicciones de clientes que se van del bank_data, no lo podemos
considerar un buen modelo para nuestros datos. A pesar de ello, tenemos
buena precisión en las predicciones, ya que cuando predice que un
cliente se va a ir, en un 78% de las ocasiones se marcha del bank_data.
En conclusión, el modelo de k-nn es mejor que el azar, pero falla
significativamente a la hora de predecir cuando un cliente se va del
bank_data como nos muestra la sensitivity. Deberemos probar otros
algoritmos de clasificación que harán nuestras predicciones mejores.

Vamos a comprobarlo en el dataset de test

```{r k-nn test}
# Nos aseguramos de que el churn es un factor
test_data$churn <- as.factor(test_data$churn)
levels(test_data$churn) <- make.names(levels(test_data$churn))

prediction <- predict(modelo_knn, newdata = test_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(test_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
cf <- confusionMatrix(
    data = prediction,                      
    reference = test_data$churn,        
    positive = levels(test_data$churn)[2]           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf)
```

Vemos como todo lo mencionado en las predicciones de train aplica aqui
como es lógico.

# 5. Análisis discriminante lineal

Como nuestros datos no siguen una distribución normal como vimos en la
primera parte de la práctica, no podremos aplicarlo a nuestros datos, es
por ello que solamente vamos a aplicarlo de manera manual con unos datos
que cumplen las condiciones necesarias


## Análisis discriminante lineal manual

Preparación de los datos
```{r}
# Cargar datos
# Dataset iris
data(iris)

# Variables predictoras
X <- iris[, 1:4]

# Variable respuesta (factor con tres niveles)
y <- iris$Species

# Ver estructura
str(X)
summary(X)


```

```{r}
X1 <- X[y == "setosa", ]
X2 <- X[y == "versicolor", ]
X3 <- X[y == "virginica", ]

n1 <- nrow(X1)
n2 <- nrow(X2)
n3 <- nrow(X3)
n_total <- n1 + n2 + n3

```

```{r}
mu1 <- colMeans(X1)
mu2 <- colMeans(X2)
mu3 <- colMeans(X3)

mu_total <- colMeans(X)

```

```{r}
S1 <- cov(X1)
S2 <- cov(X2)
S3 <- cov(X3)

# Matriz de covarianza agrupada (ponderada por tamaños)
S_pooled <- ((n1 - 1) * S1 + (n2 - 1) * S2 + (n3 - 1) * S3) / (n_total - 3)

# Regularizar si es necesario
if (det(S_pooled) == 0 || any(is.na(S_pooled))) {
  S_pooled <- S_pooled + diag(1e-6, ncol(S_pooled))
}


```


```{r}
# Invertir matriz de covarianza agrupada
S_inv <- solve(S_pooled)

# Calcular direcciones discriminantes (usaremos eigenvectores de entre-grupos)
# Paso intermedio: matriz de dispersión entre grupos (B)
B <- (n1 * (mu1 - mu_total) %*% t(mu1 - mu_total)) +
     (n2 * (mu2 - mu_total) %*% t(mu2 - mu_total)) +
     (n3 * (mu3 - mu_total) %*% t(mu3 - mu_total))


# Resolver el problema generalizado de autovalores para LDA:
eig <- eigen(solve(S_pooled) %*% B)

LD_directions <- Re(eig$vectors[, 1:2])  # Parte real de los vectores discriminantes

X_mat <- as.matrix(X)
LD_scores <- Re(X_mat %*% LD_directions) # Proyecciones reales

df_plot <- data.frame(LD1 = LD_scores[, 1],
                      LD2 = LD_scores[, 2],
                      Species = y)


```


```{r}
library(ggplot2)

ggplot(df_plot, aes(x = LD1, y = LD2, color = Species)) +
  geom_point(size = 2) +
  labs(title = "Proyección LDA manual sobre iris",
       x = "LD1", y = "LD2") +
  theme_minimal()

```

```{r}
# Proyección de las medias
mu1_proj <- as.numeric(mu1 %*% LD_directions)
mu2_proj <- as.numeric(mu2 %*% LD_directions)
mu3_proj <- as.numeric(mu3 %*% LD_directions)

# Clasificador: distancia euclídea en el plano discriminante
clasificar <- function(x) {
  x_proj <- as.numeric(x %*% LD_directions)
  d1 <- sum((x_proj - mu1_proj)^2)
  d2 <- sum((x_proj - mu2_proj)^2)
  d3 <- sum((x_proj - mu3_proj)^2)
  which.min(c(d1, d2, d3))
}

# Clasificar todas las observaciones
pred_clases <- apply(X, 1, clasificar)
levels(y)  # [1] "setosa"     "versicolor" "virginica"
pred_y <- factor(pred_clases, levels = 1:3, labels = levels(y))

# Matriz de confusión
matriz <- table(Predicho = pred_y, Real = y)
print(matriz)

# Precisión
precision <- sum(diag(matriz)) / sum(matriz)
cat("Precisión:", round(precision * 100, 1), "%\n")

```

Clase setosa:

  Se clasificaron correctamente los 50 ejemplares (100% acierto).

  No hubo errores de clasificación para esta clase.

Clase versicolor:

  48 fueron correctamente clasificados.

  1 ejemplar fue clasificado erróneamente como virginica.

Clase virginica:

  49 fueron correctamente clasificados.

  2 ejemplares fueron clasificados erróneamente como versicolor.
  
La precisión total del modelo es del 98%, lo que indica un desempeño excelente.

Solo hubo 3 errores en total (1 versicolor mal clasificado + 2 virginica mal clasificados) sobre 150 observaciones.

# 6. Árboles de decisión

Vamos a aplicar el método del árbol de decisión (DT) a nuestros datos, en el que hacemos un modelo para predecir la variable churn a partir de otras variables.
```{r}
library(rpart)
library(rpart.plot)

set.seed(128)

datos_DT <- train_data %>%
  dplyr::select(credit_score, country, gender, age, tenure, balance, products_number, credit_card, active_member, estimated_salary, churn)
fit.dt <- rpart(churn~., data = datos_DT, method = 'class')

rpart.plot(fit.dt, extra = 100)

```
Reglas Clave y su Significado:

    -products_number < 3 (Aparece 3 veces)

        Indica que el número de productos contratados es el factor más determinante.

        Clientes con menos de 3 productos son más propensos a abandonar (probablemente por menor engagement).

    -active_member = +

        Clientes activos (ej: que usan servicios frecuentemente) tienen menor riesgo de churn.

        El símbolo + sugiere que esta condición reduce la probabilidad de abandono.

    -products_number >= 2

        Clientes con 2 o más productos tienen menor riesgo.

        Corrobora que la retención mejora con más productos contratados.

    -country = France, Spain

        Ubicación geográfica como predictor:

            Francia y España podrían tener mayor churn que otros países (ej: Alemania).

            O podría ser un nodo intermedio para otras divisiones (depende del árbol completo).

    -balance >= 70,000

        Clientes con saldos altos (≥70k) tienen comportamiento distinto.

        Posiblemente:

            Mayor balance → Menor churn (por relación más valiosa).

            O podría asociarse a otros factores (ej: productos adicionales).

Interpretación del Modelo:

    -Variables más importantes:

        products_number (principal), active_member, country, balance.

    -Perfil de alto riesgo:

        Clientes con <3 productos, inactivos, y de Francia/España.

    -Perfil de bajo riesgo:

        Clientes con ≥2 productos, activos, y saldos altos.
        

Usamos el modelo DT para realizar predicciones:
```{r}
# Sobre la partición de entrenamiento
prediccion_DT <- predict(fit.dt, train_data, type = 'class')
cf_DT <- confusionMatrix(prediccion_DT, as.factor(datos_DT$churn), positive = "X1")

print(cf_DT)
```
Métricas Principales:

    Precisión (Accuracy): 86.2%

        El modelo acierta en el 86.2% de los casos.

        Intervalo de confianza (95%): 85.4% - 86.9%

    Sensibilidad: 40.8%

        Detecta solo el 40.8% de los casos reales de abandono (churn).

        Problema grave: Pierde el 59.2% de los clientes que realmente se van.

    Especificidad: 97.7%

        Identifica correctamente el 97.7% de los clientes que no abandonan.

    (PPV): 81.8%

        Cuando predice abandono, acierta en el 81.8% de los casos.

    Kappa: 0.47

        Acuerdo moderado entre predicciones y realidad.

Interpretación:

    El modelo es bueno evitando falsas alarmas (solo 147 FP)

    Pero falla en detectar abandonos reales (959 FN no detectados)

    Desequilibrio importante: Hay 6.5 veces más FN que FP (959 vs 147)

Vamos a implementarlo en el df de test
```{r}
# Sobre la partición de prueba
prediction.dt <- predict(fit.dt, test_data, type = 'prob')[,2]
clase.pred=ifelse(prediction.dt>0.5,"X1","X0")

cf_DT_prueba <- confusionMatrix(as.factor(clase.pred), as.factor(test_data$churn),positive="X1")

print(cf_DT_prueba)
```
Métricas clave:

    - Precisión (Accuracy): 85.0%

        El modelo acierta en el 85% de las predicciones

        Intervalo de confianza 95%: (83.36%, 86.54%)

    - Sensibilidad/Recall: 38.76%

        Detecta solo el 38.76% de los casos reales de churn

        Problema principal: No detecta el 61.24% de los abandonos reales (256 FN)

    - Especificidad: 97.22%

        Identifica correctamente el 97.22% de los no abandonos

        Solo 44 falsas alarmas (FP)

    (PPV): 78.64%

        Cuando predice abandono, acierta en el 78.64% de los casos

    - Kappa: 0.4423

        Acuerdo moderado entre predicciones y realidad

Interpretación detallada:

    - Balance de clases:

        Prevalencia de churn: 20.9% (ligeramente desbalanceado)

        El modelo predice churn en el 10.3% de los casos (Detection Prevalence)

    - Rendimiento en detección de churn:

        Solo detecta 162 de 418 casos reales (162 VP de 418 = 38.76%)

        256 falsos negativos (clientes que abandonaron pero no fueron detectados)

    - Comparación con modelo aleatorio:

        P-value < 2.2e-16 indica que el modelo es significativamente mejor que adivinar al azar

        Pero la mejora es insuficiente para la detección de churn



# 7. Bagging

Vamos a incluir ahora os métodos de ensamblado que se basa en la unión
de múltiples modelos que pueden mejorar nuestras predicciones, la idea
de estos modelos denominados métodos de ensamblado es encadenarlos para
mejorar dichas predicciones.

Haremos el ensamblado de bagging al principio, que consiste en una
técnica de ML hecha para mejorar la predicción de los modelos y como
hemos dicho antes se basa en construir múltiples modelos similares y
combinar dichas predicciones para obtener un resultado final más fiable.

## Bagging

**Bagging** funciona de la siguiente manera, empieza con la división del
conjunto de datos inicial en distintos subconjutnos datos y usamos algo
llamado muestreo de reemplazado, esto genera distintos conjuntos de
entrenamientos ligeramente distintos entre ellos, el siguiente paso
sería entrenar el modelo base, y cada modelo es distinto debido a el
distinto muestreo, como tercer paso encontramos que una vez todas los
modelos hayan sido entrenados y se utilizan para hacer predicciones
individuales sobre un conjunto de datos de prueba, como paso final
obtenemos que todas las predicciones de los modelos se combinan para
tener una mejor predicción.

El algoritmo más conocido para **Bagging** es el que desarrollaremos
ahora que es denominado **"Random Forest"** que básicamante combina
multiples DTs para lograr un modelo predictivo altamente preciso.

### Random Forest

```{r}
library(ggplot2)
library(caret)
library(readr)
library(dplyr)

# 1. Cargar y preparar los datos
Data <- read_csv("Bank Customer Churn Prediction.csv")
bank_data$churn <- as.factor(Data$churn)

# Seleccionar variables relevantes
predictors <- c("credit_score", "age", "tenure", "balance", 
                "products_number", "estimated_salary", "country", "gender")
response <- "churn"

data <- Data %>% 
  select(all_of(c(predictors, response))) %>% 
  na.omit() %>%
  mutate(
    country = as.factor(country),
    gender = as.factor(gender)
  )

# 2. Configuración del experimento
set.seed(123)
n_reps <- 50
test_size <- 0.2

# Crear conjunto de test
test_idx <- createDataPartition(bank_data$churn, p = test_size, list = FALSE)
test_data <- bank_data[test_idx, ]
train_data_base <- bank_data[-test_idx, ]

# 3. Funciones para realizar las predicciones
run_bagging <- function(train_data) {
  model <- randomForest(churn ~ ., 
                       data = train_data,
                       mtry = length(predictors), # Usa todas las variables
                       ntree = 100)
  return(model)
}

run_rf <- function(train_data) {
  model <- randomForest(churn ~ .,
                       data = train_data,
                       mtry = floor(sqrt(length(predictors))), # Selección típica RF
                       ntree = 100)
  return(model)
}

# 4. Ejecutar las repeticiones
results <- lapply(1:n_reps, function(i) {
  # Muestreo bootstrap
  sample_idx <- sample(1:nrow(train_data_base), nrow(train_data_base), replace = TRUE)
  train_data <- train_data_base[sample_idx, ]
  
  # Entrenar modelos
  bagging_model <- run_bagging(train_data)
  rf_model <- run_rf(train_data)
  
  # Hacer predicciones
  bagging_pred <- predict(bagging_model, test_data, type = "prob")[,2]
  rf_pred <- predict(rf_model, test_data, type = "prob")[,2]
  
  return(data.frame(
    rep = i,
    observation = 1:nrow(test_data),
    bagging_pred = bagging_pred,
    rf_pred = rf_pred,
    actual = test_data$churn
  ))
})

# Combinar todos los resultados
all_results <- bind_rows(results)

# 5. Calcular varianzas
variances <- all_results %>%
  group_by(observation, actual) %>%
  summarise(
    bagging_var = var(bagging_pred),
    rf_var = var(rf_pred),
    .groups = 'drop'
  )

# 6. Visualización
ggplot(variances, aes(x = observation)) +
  geom_line(aes(y = bagging_var, color = "Bagging"), linewidth = 1) +
  geom_line(aes(y = rf_var, color = "Random Forest"), linewidth = 1) +
  facet_wrap(~ actual, labeller = labeller(actual = c("0" = "No Churn", "1" = "Churn"))) +
  labs(title = "Varianza de Predicciones: Bagging vs Random Forest",
       x = "Observaciones en Conjunto de Test",
       y = "Varianza",
       color = "Método") +
  scale_color_manual(values = c("Bagging" = "blue", "Random Forest" = "red")) +
  theme_minimal()

# 7. Métricas de rendimiento
final_pred <- all_results %>%
  group_by(observation) %>%
  summarise(
    bagging_mean = mean(bagging_pred),
    rf_mean = mean(rf_pred),
    actual = first(actual)
  )

# Umbral de decisión
final_pred$bagging_class <- ifelse(final_pred$bagging_mean > 0.5, 1, 0)
final_pred$rf_class <- ifelse(final_pred$rf_mean > 0.5, 1, 0)

# Matrices de confusión
cat("Bagging Performance:\n")
confusionMatrix(factor(final_pred$bagging_class, levels = c(0, 1)), 
                factor(final_pred$actual, levels = c(0, 1)))

cat("\nRandom Forest Performance:\n")
confusionMatrix(factor(final_pred$rf_class, levels = c(0, 1)), 
                factor(final_pred$actual, levels = c(0, 1)))
```

Lo primero a analizar de estos gráficos es la varianza de predicciones.
Tenemos dos gráficos, uno para clientes que permanecen y otro para los
que no. Empecemos por los que si permanecen, podemos ver un patrón en el
cual las predicciones de varianza son bajas, por debajo de 0.01, ambos
modelos muestran una alta estabilidad para los clientes que no
abandonan,o aunque Random Forest tiene una pquela ventaja con varianza
ligeramente menor que Bagging.

Por el contrario para los clientes que abandonan la varianza es
significativamente mayor con puntos de hasta 0.125, por eso podemos
decir que las predicciones son menos consistentes, Bagging muestra picos
de alta varianza lo que lo hace más difícil de clasificar, por otro lado
Random FOrest mantiene mejor control de la varianza sobre todo entre los
rangos de 500-1000 y 1500-2000.

**Hallazgos importantes:**

**Exactitud global:** 85.21% (buen desempeño general)

**Sensibilidad (No Churn):** 96.42% (excelente para identificar
permanencia)

**Especificidad (Churn):** Solo 41.42% (dificultad para detectar
abandonos)

**Prevalencia:** 79.61% de los casos son "No Churn" (dataset
desbalanceado)

**Problemas identificados:**

**Alto número de falsos negativos:** 239 casos de Churn fueron mal
clasificados

**Baja especificidad:** El modelo tiende a predecir "No Churn" incluso
cuando el cliente abandona

**Valor predictivo negativo:** Solo 74.78%, prácticamente tres cuartos
de las predicciones son son correctas lo que es bajo.

En este gráfico comparamos la varianza de Bagging con la de Random
Forest.

Si ponemos en conjunto ambos resultados, podemos ver como para "No
Churn" tenemos: Baja varianza y Alta sensibilidad (96,42%) por lo tanto
el modelo es consistente y preciso para esta clase. El otro lado es para
"Churn" en este caso tenemos: Alta varianza y Baja especificidad
(41,42%) por lo que encontramos mucha inconsistencia en las predicciones
se refleja en la dificultad para identificar abandonos.

Comparando modelos podemos ver que **Random Forest** nos muestra menor
varianza en ambos casos sobre todo para **Churn**, mayor estabilidad en
predicciones difíciles y seguramente mejor generalización gracias a la
selección aleatoria de variables, para el modelo de **Bagging** tenemos
mayor varianza especualmente en los límites pero un posible sobreajuste
debido al uso de todas las variables en cada división

Aplicamos este método de ML a nuestro conjunto de datos

```{r}
rf_full <- randomForest(as.factor(churn) ~ ., 
                        data = train_data_base, 
                        importance = TRUE, 
                        proximity = TRUE)

# Ver el resumen del modelo
print(rf_full)

```

Tenemos un error global de 14.51% (OOB estimate) → 85.49% de precisión

La estructura del modelo está formada por 500 árboles de decisión y 2
variables consideradas en cada división (mtry = 2)

**Análisis por Clases**

**Para clientes que NO abandonan (0):** Tenemos 6135 clientes
correctamente clasificados pero 235 que no lo están eso nos da un 3.69%
de error eso significa que tenemos un gran modelo para los clientes que
se quedan en el bank_data

**Para clientes que SÍ abandonan (1):** Tenemos 703 clientes
correctamente clasificados pero 926 que no lo están eso nos da un 56.84%
de error eso significa que tenemos un modelo que para los clientes que
se van del bank_data no los detecta de manera correcta con más de la
mitad de ellos sin ser bien detectados.

```{r}
plot(rf_full)
title(main = "Error OOB y por Clase - Random Forest")
legend("topright", 
       legend = colnames(rf_full$err.rate), 
       col = 1:ncol(rf_full$err.rate), 
       lty = 1, 
       cex = 0.8)
```

Para poder sacar conclusiones de este gráfico tenemos que ver la curva
de error de **OOB(Negra)** que nos muestra como el error global va
disminuyendo a medida que se añaden añaden árboles, por lo que podría
llegar a estabilizarse.

Vamos a ver los errores por clases **Clase 0(No Churn-Rojo)** el error
es bastante bajo y coincide con la alta precisión de la matriz de
confusión vista antes, por el otro lado **Clase 1(Churn - Verde)** tiene
un error considerablemente alto que disminuye lentamente,

```{r}
# Partición de test
# Predecir probabilidades
prediction.rf <- predict(rf_full, test_data, type = "prob")[,2]

# Convertir probabilidades a clases
clase.pred.rf <- ifelse(prediction.rf > 0.5, "1", "0")

# Matriz de confusión
cf <- confusionMatrix(as.factor(clase.pred.rf), as.factor(test_data$churn), positive = "1")
print(cf)

```

Lo primero que vamos a analizar es la matriz de confusión y nos muestra
el desempeño del modelo Random Forest en los datos de prueba, aparte de
esto nos da más información:

-   **Precisión:** 85.06%
-   **Intervalo de Confianza del 95%:** (83.42%, 86.59%)
-   **Sensibilidad (Recall para Churn):** 41.18%, solo detecta ese
    porcentaje de de casos reales de abandono
-   **Especificidad:** 96.30% para identificar clientes que permanecen.
-   **Precisión:** 74.01%, acierta el 74% de las veces que predice un
    abandono.
-   **KAPPA:** 0.4488, es el valor de la concordancia moderada entre
    predicciones y realidad.

```{r}
importance(rf_full)
```

Para entender esta tabla es fundamental entender que significa
*"MeanDecreaseAccuracy"* y *"MeanDecreaseGini"* y por lo tanto que miden
estas variables y cuáles tienen un mayor impacto

-   **MeanDecreaseAccuracy** mide cuánto disminuye la exactitud del
    modelo al eliminar cada variable, a mayor valor más impacto tiene
    por lo que estimated_salary (109.72) , tenure (92.07) y
    products_number(48.82).

-   **MeanDecreaseGini** mide la contribución a la pureza de los nodos
    (homogeneidad) y sus valores más importantes son tenure (521.71),
    estimated_salary (348.31) y products_number (323.64).

Veamos esto representado en un gráfico.

```{r}
varImpPlot(rf_full)
```

Una vez vemos esto de manera más visual vemos que estas tres variables
vistas antes son más importantes a la hora de predecir el abandono en
los bank_datas, por otro lado renemos los valores de gender y country
que permanecen en ambos gráficos con valores muy pequeños por lo que
podrían ser variables para considerar para eliminar.

## Bagging a a mano

Como estabamos teniendo inconvenientes a la hora de ejecutar el bagging
manualmente con nuestros datos, lo hemos implementado con el dataset -\>
Iris.

```{r}
library(rpart)
library(caret)

set.seed(123)
data(iris)

# Dividir datos
train_index <- createDataPartition(iris$Species, p = 0.7, list = FALSE)
train <- iris[train_index, ]
test <- iris[-train_index, ]

# Parámetros del bagging
n_trees <- 100
n <- nrow(train)
predictions_matrix <- matrix(NA, nrow = nrow(test), ncol = n_trees)

# Entrenar árboles bootstrap
for (i in 1:n_trees) {
  sample_indices <- sample(1:n, size = n, replace = TRUE)
  bootstrap_sample <- train[sample_indices, ]
  
  tree_model <- rpart(Species ~ ., data = bootstrap_sample, method = "class", control = rpart.control(cp = 0))
  
  # Obtener clases, no probabilidades
  preds <- predict(tree_model, newdata = test, type = "class")
  
  predictions_matrix[, i] <- as.character(preds)
}

# Voto mayoritario
final_predictions <- apply(predictions_matrix, 1, function(row) {
  names(sort(table(row), decreasing = TRUE))[1]
})

# Convertir a factor con niveles correctos
final_predictions <- factor(final_predictions, levels = levels(test$Species))

# Evaluar
conf_matrix_manual <- confusionMatrix(final_predictions, test$Species)
print(conf_matrix_manual)

```

| **Categoría** | **Resultado** | **Conclusión** |
|----|----|----|
| **Precisión global** | 0.9333 | Muy alta precisión general del modelo. |
| **Intervalo de confianza (95%)** | (0.8173, 0.986) | Alta fiabilidad en el rendimiento estimado. |
| **Kappa** | 0.9 | Fuerte acuerdo entre predicción y valores reales más allá del azar. |
| **Clase mejor clasificada** | Setosa (100%) | Se clasifica perfectamente, sin errores. |
| **Clases con más errores** | Versicolor y Virginica | Confusión ocasional entre estas dos clases, como es común en este dataset. |
| **Sensibilidad promedio** | 0.9333 | Alta capacidad de detectar correctamente los casos positivos en cada clase. |
| **Especificidad promedio** | ≈ 0.9667 | Alta capacidad de evitar falsos positivos. |
| **Facilidad de implementación** | Media | Requiere más pasos que `randomForest`, pero permite comprensión detallada. |
| **Visualización de importancia** | No disponible directa | Se necesitaría implementar manualmente si se desea analizar importancia. |

```{r}
table(final_predictions)
```

# 8. Boosting

Vamos a aplicar a continuación distintos modelos de boosting, estos
modelos deberían mejorar el rendimiento de nuestras predicciones, así
como ayudarnos a reducir el sesgo y el desequilibrio de nuestros datos
(que como hemos podido observar en algunos modelos esto es bastante
importante para nosotros). A pesar de estas ventajas, la aplicación de
modelos de boosting reduce la interpretabilidad de nuestras
predicciones, además de complicarnos el ajuste de los hiperparámetros

Primeramente vamos a implementar AdaBoost

### Adaboost

Este modelo es interesante ya que sigue siendo un modelo aditivo, pero
como utiliza la pérdida exponencial, reduce la influencia de los
ejemplos mal clasificados y como le da más peso a aquellas iteraciones
que más difíciles han sido de clasificar, nos garantiza que el modelo
final es robusto.

```{r adaboost}
set.seed(128)
# Utilizamos validación cruzada para hallar los hiperparámetros
trainControl_adaboost <- trainControl(
    method = "repeatedcv",
    number = 10,       # 10 folds
    repeats = 3,       # 3 veces
    verboseIter = FALSE 
)

ada_grid <- expand.grid(
    mfinal = 50, # numero de arboles
    maxdepth = c(2,3), # profundidad de los árboles
    coeflearn = c('Breiman', 'Freund') # coeficiente de aprendizaje
    )

model_adaboost <- train(
  churn ~ ., # utilizamos todas las características para predecir el churn
  data = train_data,         
  method = "AdaBoost.M1",        
  trControl = trainControl_adaboost, 
  metric = "Accuracy",          
  tuneGrid = ada_grid,          
  preProcess = c("center", "scale") 
)

cat("\nResultados del entrenamiento\n")
print(model_adaboost)
cat("\nMejores hiperparámetros:\n")
print(model_adaboost$bestTune)

# Representamos el rendimiento en base a los mejores hiperparámetros
plot(model_adaboost)
summary(model_adaboost)
```

Hacemos la predicción primeramente con nuestros datos de entrenamiento
para ver como predice nuestro modelo. Para ello vamos a utilizar la
matriz de confusión

```{r adaboost train}
set.seed(123)
prediction <- predict(model_adaboost, newdata = train_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(train_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
positive_class_label <- levels(train_data$churn)[2] 
cf <- confusionMatrix(
    data = prediction,                      
    reference = train_data$churn,        
    positive = positive_class_label           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf)

prediction_prob_train <- try(predict(model_adaboost, newdata = train_data, type = "prob"))
current_levels <- levels(train_data$churn)

roc_curve_adaboost_train <- roc(
        response = train_data$churn,
        predictor = prediction_prob_train[[positive_class_label]],
        levels = current_levels,
        quiet = TRUE
        )

# Ploteamos la curva ROC
plot(roc_curve_adaboost_train,
         print.auc = TRUE,
         main = "Curva ROC del modelo AdaBoost (Training)",
         legacy.axes = TRUE
         )
```

Podemos ver como este modelo tiene una accuracy de un 86% lo cual es
bastante bueno, aunque debemos indagar más en el resto de métricas para
ver si realmente nuestras predicciones son buenas. El índice Kappa es
mejor que en otros modelos, cerca del 50%, aunque sigue siendo bajo. El
test McNemar nos sigue mostrando este desbalance en nuestros datos, el
modelo sigue prediciendo mejor aquellos que no se van, que aquellos que
si lo hacen. Siguiendo esta línea, nuestro recall e de solo un 48%, lo
cual sigue siendo bajo, ya que significa que predecimos mal la mitad de
aquellos que se marchan. La especificidad es alta, asi como el NPV, ya
que predecimos bien los negativos, con cerca de un 96% de las
predicciones siendo correctas. Por último. comparando AUC y sensitividad
podemos extraer que el threshold de 0.5 para la AUC no es el ideal en
nuestro caso, ya que si eligieramos otro threshold, tendríamos una
sensitividad más alta a expensas de tener una especificidad y precisión
más bajas, algo que nos podría llegar a interesar puesto que nuestro
objetivo es que los clientes no se vayan del bank_data.

A continuación lo aplicamos en los datos de test

```{r adaboost test}
prediction <- predict(model_adaboost, newdata = test_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(test_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
cf <- confusionMatrix(
    data = prediction,                      
    reference = test_data$churn,        
    positive = levels(test_data$churn)[2]           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf)

prediction_prob_train <- try(predict(model_adaboost, newdata = test_data, type = "prob"))
current_levels <- levels(test_data$churn)

roc_curve_adaboost_test <- roc(
        response = test_data$churn,
        predictor = prediction_prob_train[[positive_class_label]],
        levels = current_levels,
        quiet = TRUE
        )

# Ploteamos la curva ROC
plot(roc_curve_adaboost_test,
         print.auc = TRUE,
         main = "Curva ROC del modelo AdaBoost (test)",
         legacy.axes = TRUE
         )
```

Vemos como todo lo dicho anteriormente para los datos de entrenamiento
aplica de nuevo a nuestros datos de test, por lo que no tenemos
sobreajuste.

Vamos a ver qué ocurre cuando aplicamos otros algoritmos de boosting
como Gradient Boosting:

### Gradient boosting

```{r gbm}
set.seed(128) # para reproducibilidad
# Usaremos validación cruzada repetida
trainControl_gbm <- trainControl(
    method = "repeatedcv",
    number = 10,        # 10 folds
    repeats = 3,        # 3 repeticiones
    verboseIter = FALSE, 
    classProbs = TRUE,
    summaryFunction = twoClassSummary
)

gbm_grid <- expand.grid(
    n.trees = 100, # numero de arboles
    interaction.depth = c(1, 2, 3), # profundidad 
    shrinkage = c(0.1, 0.05), # tasa de aprendizaje
    n.minobsinnode = c(10, 15) # minimo de observaciones en el ultimo nodo
    )

model_gbm <- train(
  churn ~ .,
  data = train_data,
  method = "gbm", 
  trControl = trainControl_gbm, 
  metric = "Accuracy",
  tuneGrid = gbm_grid,
  preProcess = c("center", "scale"), 
  verbose = FALSE
)
cat("Resultados del entrenamiento\n")
print(model_gbm)
cat("\nMejores hiperparámetros:\n")
print(model_gbm$bestTune)

# Representamos el modelo y sus métricas
plot(model_gbm)
cat("\nImportancia de las variables:\n")
gbm_importancia <- varImp(model_gbm, scale = FALSE)
plot(gbm_importancia) # Visualizamos la importancia de cada variable en el modelo

```

```{r gbm train}
prediction <- predict(model_gbm, newdata = train_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(train_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
positive_class_label <- levels(train_data$churn)[2] 
cf <- confusionMatrix(
    data = prediction,                      
    reference = train_data$churn,        
    positive = positive_class_label           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf)

prediction_prob_train <- try(predict(model_gbm, newdata = train_data, type = "prob"))
current_levels <- levels(train_data$churn)

roc_curve_gbm_train <- roc(
        response = train_data$churn,
        predictor = prediction_prob_train[[positive_class_label]],
        levels = current_levels,
        quiet = TRUE
        )

# Ploteamos la curva ROC
plot(roc_curve_gbm_train,
         print.auc = TRUE,
         main = "Curva ROC del modelo GBM (Training)",
         legacy.axes = TRUE
         )
```

En este caso, nuestra sensitivity aumenta ligeramente, igual que lo hace
la accuracy. Estos aumentos no son significativos por lo que podemos
decir que el modelo predice de igual manera que el anterior. Sin
embargo, lo que si ganamos significativamente es tiempo. El modelo
Adaboost es mucho más lento que el modelo GBM, por lo que, al hacer
ambos predicciones similares, sería más eficiente elegir el gradient
boosting para nuestro problema.

Vamos a ver qué ocurre en los datos de test, aunque obviamente esperamos
un resultado similar

```{r gbm test}
prediction <- predict(model_gbm, newdata = test_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(test_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
positive_class_label <- levels(test_data$churn)[2] 
cf <- confusionMatrix(
    data = prediction,                      
    reference = test_data$churn,        
    positive = positive_class_label           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf)

prediction_prob_test <- try(predict(model_gbm, newdata = test_data, type = "prob"))
current_levels <- levels(test_data$churn)

roc_curve_gbm_test <- roc(
        response = test_data$churn,
        predictor = prediction_prob_test[[positive_class_label]],
        levels = current_levels,
        quiet = TRUE
        )

# Ploteamos la curva ROC
plot(roc_curve_gbm_test,
         print.auc = TRUE,
         main = "Curva ROC del modelo GBM (Test)",
         legacy.axes = TRUE
         )
```

Como imaginabamos, los resultados son muy similares al train, por lo que
no hay sobreajuste

Por último vamos a implementar el modelo XGBoost o extreme gradient
boosting, el cual nos debería dar mejor resultado, no solo prediciendo,
sino también en eficiencia.

### XGBoost

```{r}
set.seed(128)
trainControl_xgb_roc <- trainControl(
    method = "repeatedcv",
    number = 10,    
    repeats = 3,
    classProbs = TRUE, 
    summaryFunction = twoClassSummary, 
    verboseIter = FALSE, 
    allowParallel = TRUE,
    sampling = "up"
)
xgb_grid <- expand.grid(
    nrounds = 100,
    max_depth = c(3, 6), # Profundidad máxima del árbol
    eta = c(0.05, 0.1), # Tasa de aprendizaje
    gamma = c(0.1, 0.2), # Tasa de regularización
    colsample_bytree = c(0.7), # Fracción de columnas por árbol 
    min_child_weight = c(1, 3), # Peso mínimo por cada nodo hijo
    subsample = c(0.7) # Fracción de muestras por árbol
)
# imprimimos los hiperparámetros del modelo
print(xgb_grid)

model_xgboost <- train(
  churn ~ .,
  data = train_data,
  method = "xgbTree",
  trControl = trainControl_xgb_roc,
  metric = "ROC",
  tuneGrid = xgb_grid,
  preProcess = c("center", "scale"),
  verbose = FALSE
)

print(model_xgboost)
# Representaciones
xgb_importancia <- varImp(model_xgboost, scale = FALSE)
print(xgb_importancia)
plot(xgb_importancia)

```

Vamos a ver cómo funciona el modelo en nuestros datos de entrenamiento

```{r}
prediction <- predict(model_xgboost, newdata = train_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(train_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
positive_class_label <- levels(train_data$churn)[2] 
cf <- confusionMatrix(
    data = prediction,                      
    reference = train_data$churn,        
    positive = positive_class_label           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf)

prediction_prob_train <- try(predict(model_xgboost, newdata = train_data, type = "prob"))
current_levels <- levels(train_data$churn)

roc_curve_xgboost_train <- roc(
        response = train_data$churn,
        predictor = prediction_prob_train[[positive_class_label]],
        levels = current_levels,
        quiet = TRUE
        )

# Ploteamos la curva ROC
plot(roc_curve_xgboost_train,
         print.auc = TRUE,
         main = "Curva ROC del modelo XGBoost (Training)",
         legacy.axes = TRUE
         )
```

Tenemos un área bajo la curva (AUC) muy buena, por lo que el modelo
diferencia muy bien entre positivos y negativos. En cuanto a las medidas
de la matriz de confusión, en este modelo hemos perdido accuracy (sobre
un 4%) pero ahora predecimos mucho mejor los TP y FN, nuestra recall es
mucho mejor, pasando de un 50% aprox en otros modelos a un 76% en este,
con lo que vemos que este modelo es mucho mejor prediciendo a los
clientes que se van del bank_data. Como conseguimos mucha mejor recall,
nuestra especificidad baja significativamente a un 83%, lo cual todavía
es bastante bueno, pero predecimos peor a los que no se marchan. Todo
esto nos lleva a una tasa mayor de falsos positivos, ya que ahora
predecimos más casos de churn. A pesar de ello, mejoramos el NPV con lo
que cuando predecimos un no, es más probable que sea cierto.

En este caso, este modelo es mucho mejor si damos mayor importancia a
identificar a aquellos que se quieren marchar sobre identificar a
aquellos que se quedan

Vamos a ver si estos resultados se transfieren a nuestros datos de test
o si hay overfitting

```{r}
set.seed(123)
prediction <- predict(model_xgboost, newdata = test_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(test_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
positive_class_label <- levels(test_data$churn)[2] 
cf <- confusionMatrix(
    data = prediction,                      
    reference = test_data$churn,        
    positive = positive_class_label           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf)

prediction_prob_test <- try(predict(model_xgboost, newdata = test_data, type = "prob"))
current_levels <- levels(test_data$churn)

roc_curve_xgboost_test <- roc(
        response = test_data$churn,
        predictor = prediction_prob_test[[positive_class_label]],
        levels = current_levels,
        quiet = TRUE
        )

# Ploteamos la curva ROC
plot(roc_curve_xgboost_test,
         print.auc = TRUE,
         main = "Curva ROC del modelo XGBoost (Test)",
         legacy.axes = TRUE
         )
```

Como podemos ver, los resultados son muy similares en test por lo que no
tenemos overfitting y nuestro modelo es robusto.

Dependiendo de lo que prioricemos, si el elegir mejor a aquellos
clientes que permanecen el banco o a aquellos que se van a marchar,
deberemos elegir un modelo u otro. Esto lo estudiaremos un poco más
adelante cuando comparemos los modelos y elijamos un modelo final para
nuesto problema.

# 9. Naïve bayes

## Naive Bayes

Este algoritmo está basado en el conocido teorema de Bayes, que a pesar
de ser sencillos son muy útiles para problemas con muchas variables
entrada por lo que no puede ser útil, veamos como reaccionan nuestros
datos ante este test

```{r}
library(naivebayes)

# Entrenar modelo Naive Bayes
nb_model <- naive_bayes(as.factor(churn) ~ ., data = train_data_base, usekernel = TRUE)

# Predecir probabilidades sobre el test
probabilities <- predict(nb_model, test_data[,-9], type = "prob")[,2]

# Clasificar según umbral 0.5
classes <- as.numeric(probabilities > 0.5)

# Matriz de confusión
cf_nb <- confusionMatrix(
  factor(classes, levels = c(0, 1)), 
  factor(test_data$churn, levels = c(0, 1)), 
  positive = "1"
)

# Imprimir resultados
print(cf_nb)
```

**Resultados de la matriz de confusión - Naive Bayes**

| Métrica | Valor | Interpretación |
|----|----|----|
| **Accuracy** | 83.41% | Precisión global aceptable. |
| **Sensitivity (Recall clase 1)** | 26.47% | Muy baja: detecta pocos clientes que hacen churn. |
| **Specificity** | 97.99% | Muy alta: predice muy bien los que **no** hacen churn. |
| **Pos Predictive Value (PPV)** | 77.14% | De los predichos como churn, el 77% realmente hacen churn. |
| **Neg Predictive Value (NPV)** | 83.88% | De los predichos como no churn, el 84% realmente no lo hacen. |
| **Balanced Accuracy** | 62.23% | Moderada, indica desequilibrio entre clases. |
| **Kappa** | 0.3237 | Aceptable, pero sugiere que el modelo puede mejorar. |
| **McNemar's Test p-value** | \<2.2e-16 | Hay diferencia significativa entre errores de clasificación. |

## Naive Bayes a mano

```{r}

# 1. Entrenar: prior y likelihoods con Laplace smoothing
prior <- prop.table(table(train_data_base$churn))

likelihoods <- list()
for (var in names(train_data_base)[names(train_data_base) != "churn"]) {
  tab <- table(train_data_base[[var]], train_data_base$churn)
  tab <- tab + 1 # Laplace smoothing
  likelihoods[[var]] <- prop.table(tab, margin = 2)
}

# 2. Función para predecir Naive Bayes
predict_naive_bayes <- function(newdata, prior, likelihoods) {
  probs <- matrix(1, nrow = nrow(newdata), ncol = length(prior))
  colnames(probs) <- names(prior)
  
  for (var in names(newdata)) {
    var_levels <- rownames(likelihoods[[var]])  # Niveles conocidos en entrenamiento
    for (class in names(prior)) {
      probs_tmp <- rep(1e-6, nrow(newdata))  # Inicializar probabilidad mínima
      known_mask <- newdata[[var]] %in% var_levels
      if (any(known_mask)) {
        probs_tmp[known_mask] <- likelihoods[[var]][as.character(newdata[[var]][known_mask]), class]
      }
      probs[,class] <- probs[,class] * probs_tmp
    }
  }
  
  # Multiplicar por prior
  for (class in names(prior)) {
    probs[,class] <- probs[,class] * prior[class]
  }
  
  # Normalizar filas
  probs <- probs / rowSums(probs)
  
  return(probs)
}

# 3. Predicción
probs_test <- predict_naive_bayes(test_data[,-9], prior, likelihoods)

# 4. Clasificación
predicted_class <- ifelse(probs_test[,"1"] > 0.5, 1, 0)

# 5. Evaluación
cf_manual_nb <- confusionMatrix(
  factor(predicted_class, levels = c(0,1)),
  factor(test_data$churn, levels = c(0,1)),
  positive = "1"
)

print(cf_manual_nb)
```

A partir de esta respueta podemos sacar las siguientes conclusiones.

**Conclusiones del Modelo Naive Bayes**

| Métrica | Valor | Conclusión |
|----|----|----|
| **Accuracy** | 92.16% | Excelente precisión general del modelo. |
| **Kappa** | 0.7358 | Alta concordancia respecto a la clasificación aleatoria. |
| **Sensibilidad (Recall)** | 69.23% | Detecta 7 de cada 10 clientes que realmente se van. |
| **Especificidad** | 98.03% | Detecta correctamente casi todos los clientes que se quedan. |
| **Precisión** | 90.00% | Muy confiable al predecir que un cliente se irá. |
| **Balanced Accuracy** | 83.63% | Buen rendimiento balanceado en clases desiguales. |
| **Prevalencia** | 20.37% | Proporción real de clientes que abandonan en el test. |
| **p-valor McNemar** | \< 2.2e-16 | Diferencias significativas entre errores tipo I y tipo II. |

> **Conclusión general**: El modelo Naive Bayes muestra muy buen
> rendimiento general, especialmente en identificar clientes que se
> quedan. Es útil para clasificación binaria con clases desbalanceadas,
> aunque podría afinarse aún más si el objetivo es maximizar la
> detección de fuga (mayor sensibilidad).

Finalmente podemos hacer una comparación de los resultados usando la
funcion Naive Bayes y haciendo el método de manera manual.

**Comparación de Modelos Naive Bayes: Paquete vs. Manual**

| Métrica                   | Naive Bayes (paquete) | Naive Bayes (manual) |
|---------------------------|-----------------------|----------------------|
| **Accuracy**              | 83.41%                | 82.86%               |
| **Kappa**                 | 0.3237                | 0.3550               |
| **Sensibilidad (Recall)** | 26.47%                | 33.58%               |
| **Especificidad**         | 97.99%                | 95.48%               |
| **Precisión (PPV)**       | 77.14%                | 65.55%               |
| **Balanced Accuracy**     | 62.23%                | 64.53%               |
| **Detección Positivos**   | 5.40%                 | 6.85%                |
| **p-valor**               | \< 2.2e-16            | \< 2.2e-16           |

-   Ambos modelos tienen una precisión general alta (\~83%), pero el
    modelo manual obtiene mejor sensibilidad.

-   El modelo del paquete naivebayes es más conservador y predice mejor
    a los que no se irán, con alta especificidad (97.9%).

-   El modelo implementado manualmente sacrifica algo de especificidad,
    pero logra detectar más clientes que se irán (mejor sensibilidad:
    33.6% vs. 26.5%).

-   El Kappa es ligeramente mejor en el modelo manual (0.355 vs. 0.324),
    lo que indica mejor acuerdo global con la clase real.

Viendo esto podemos decir que en general el modelo hecho de manera
manual es mejor pero veamos en que aspectos específicamente es mejor el
modelo manual o el modelo hecho con el paquete.

**Justificación basada en métricas**

| Métrica clave | Mejor modelo | Explicación breve |
|----|----|----|
| **Sensibilidad (Recall)** | **Naive Bayes Manual** | 33.6% vs. 26.5%. Detecta más clientes que realmente abandonan. |
| **Balanced Accuracy** | **Naive Bayes Manual** | 64.5% vs. 62.2%. Mide rendimiento considerando el desbalance de clases. |
| **Kappa** | **Naive Bayes Manual** | 0.355 vs. 0.324. Muestra mayor concordancia entre predicción y realidad. |
| **Precisión (PPV)** | **Naive Bayes Paquete** | 77.1% vs. 65.6%. Mayor proporción de aciertos entre los que predice como “sí”. |
| **Especificidad** | **Naive Bayes Paquete** | 98.0% vs. 95.5%. Menos falsos positivos, es más conservador con la clase negativa. |

# 10. Evaluación y Comparación de Modelos

Evaluamos todos los modelos en el conjunto de prueba.

## Cálculo de Métricas y Tabla Comparativa

# 11. Conclusiones y Trabajo Futuro

## Elección del Modelo Final

## Análisis del Modelo Seleccionado

## Conclusiones y trabajo futuro
