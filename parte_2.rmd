---
title: "Proyecto_parte2"
author: "Grupo 6: Gonzalo Cruz Gómez, Samuel Martínez Lorente, Jorge Tordesillas García"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = 'center')

# Cargamos las librerías 
library(tidyverse)
library(data.table)
library(knitr)
library(caret)
library(pROC)
library(gbm)
library(xgboost)
library(adabag)
library(rpart)
library(ggplot2)
library(randomForest)
```
# Indice
-   Introducción (Gonzalo)
-   Carga de datos 
-   Regresión logística (Jorge)
-   K-nn (Gonzalo)
-   Análisis discriminante lineal (Jorge)
-   Árboles de decisión (Jorge)
-   Bagging (Samuel)
-   Boosting (Gonzalo)
-   Naive Bayes (Samuel)
-   Evaluación y comparación (Gonzalo)
-   Conclusiones (Gonzalo)

# Introducción

El objetivo principal de este conjunto de datos es predecir la fuga de clientes (churn). En otras palabras, queremos construir un modelo que pueda determinar qué clientes tienen más probabilidades de dejar el banco en el futuro. Esto es importante para los bancos, ya que retener a los clientes existentes es normalemente más rentable que conseguir clientes nuevos. En esta segunda parte de la práctica, utiizaremos modelos más complejos que nos ayudarán a realizar una clasificación más precisa acerca de si un cliente se va a marchar o no del banco. Además, elegiremos un modelo final con el que se harán las predicciones.

# Carga de datos

Cargamos los datos y hacemos la division train-test

```{r cargar datos}
# Cargar datos
bank_data <- read.csv('Bank Customer Churn Prediction.csv')
head(bank_data)
```

```{r split train-test}
set.seed(42)
train_index <- createDataPartition(bank_data$churn, p = 0.8, list = FALSE, times = 1)
train_data <- bank_data[train_index, ]
test_data <- bank_data[-train_index, ]

cat("Dimensiones - Train Data:", dim(train_data), "\n")
cat("Dimensiones - Test Data:", dim(test_data), "\n")
```

# Regresión logística

Primeramente hemos decidido incluir la regresión logística para clasificación. 

Estudiamos la influencia de active_member en churn
```{r}
tabla1 <- table(train_data$active_member, train_data$churn)
tabla1
```

```{r}
chisq.test(tabla1)
```
Podemos ver que existe una relación relevante entre ambas variables ya que es p-valor es claramente menor a 0.05. 

Usamos la regresión logística para cuantificar la relación:
```{r}
# Modelo con active_member
logit1 <- glm(churn ~ active_member, 
              data = train_data, 
              family = binomial(link = "logit"))

summary(logit1)

# Odds Ratios
exp(cbind(OR = coef(logit1), confint.default(logit1)))
```
Como vemos la variable active_members tiene alta significatividad en la variable respuesta churn. La estimación es negativa, lo que indica que ser miembro activo disminuye la probabilidad de churn(irse del bank_data).

Como el OR de active_member es menor que 1, es un factor protector contra el churn. Ser miembro activo del bank_data reduce de forma significativa la probabilidad de que un cliente se dé de baja del bank_data. La odds ratioo de 0.43... indica que las probabilidades de churn se reducen casi a la mitad cuando el cliente es activo.

```{r}
confint.default(logit1)
```


Ahora vamos a hacer un modelo con cada variable con la variable objetivo churn para ver como se comportan.
```{r}
# Convertir variables categóricas
convertir_variables <- function(data) {
  data %>%
    mutate(
      active_member = factor(active_member, levels = c(0, 1)),
      credit_card = factor(credit_card, levels = c(0, 1)),
      country = factor(country),
      gender = factor(gender),
    )
}

train_data <- convertir_variables(train_data)
test_data <- convertir_variables(test_data)

# Lista de variables predictoras
variables <- c("age", "tenure", "credit_score", "country", 
               "gender", "balance", "products_number", 
               "credit_card", "active_member", "estimated_salary")

# Análisis univariante
resultados_univariante <- list()

for (var in variables) {
  formula <- as.formula(paste("churn ~", var))
  modelo <- glm(formula, data = train_data, family = binomial)
  
  # Almacenar resultados
  resultados_univariante[[var]] <- list(
    summary = summary(modelo),
    OR = exp(cbind(OR = coef(modelo), confint.default(modelo))))
}

# Mostrar resultado
for (var in variables) {
  cat("\nVariable:", var, "\n")
  print(resultados_univariante[[var]]$summary)
  cat("\nOdds Ratio e IC 95%:\n")
  print(resultados_univariante[[var]]$OR)
}

```
Interpretación de resultados:
  
  Variable: age
p-value < 2e-16 → altamente significativa
OR = 1.072 → Por cada año adicional, las odds de abandono (churn) aumentan un 7.2%
Significativa

Variable: tenure
p-value = 0.126 → no significativa
OR = 0.98 → ligera disminución del riesgo con más años, pero no concluyente
No significativa

Variable: credit_score
p-value = 0.417 → no significativa
OR ≈ 1 → efecto mínimo/neutro
No significativa

Variable: country
countryGermany: p-value < 2e-16, OR = 2.49 → los clientes alemanes tienen 149% más odds de churn que los de referencia
countrySpain: p-value = 0.246, OR = 1.11 → no significativa
Solo countryGermany es significativa

Variable: gender
p-value < 2e-16, OR = 0.577 → ser hombre reduce las odds de churn en un 42.3%
Significativa

Variable: balance
p-value < 2e-16, OR muy cercana a 1 pero significativa → valores altos de balance se asocian positivamente con churn
Significativa

Variable: products_number
p-value = 0.006, OR = 0.846 → cada producto extra reduce las odds de churn en un 15.4%
Significativa

Variable: credit_card
p-value = 0.206, OR = 0.91 → no significativa
No significativa

Variable: active_member
p-value < 2e-16, OR = 0.47 → ser miembro activo reduce las odds de churn un 53%
Significativa

Variable: estimated_salary
p-value = 0.493, OR ≈ 1 → sin efecto aparente
No significativa

Conclusión general:
  Variables significativas individualmente:
  
  age

countryGermany

gender

balance

products_number

active_member

Variables no significativas:
  
  tenure

credit_score

countrySpain

credit_card

estimated_salary

A continuación. ajustamos un modelo de regresión logística con todas las carecterísticas significativas.
```{r}
modelo_completo <- glm(churn ~ age + country + gender + balance + products_number + active_member, data = train_data, family = "binomial")
summary(modelo_completo)
```

En un modelo con todas las variables deja de ser significante la variable de countrySpain al producirse la interacción con las demás variables


```{r}
table(train_data$factor.churn)
```
Esto nos indica que el modelo está sesgado hacia la clase "no".


Calculamos la probabilidad que ofrece el modelo para cada una de las observaciones en la muestra de entrenamiento
```{r}
# Predicciones
predicciones <- predict(modelo_completo, type = "response")

# Histograma de probabilidades
hist(predicciones, main = "Distribución de probabilidades predichas",
     xlab = "Probabilidad de Churn", ylab = "Frecuencia")
```
En este histograma que hemos creado sobre la distribución de las probabilidades predichas por el modelo de regresión logística que hemos creado con las variables binomiales significantes que hemos creado, vemos que la mayoría de las predicciones están entre 0.1 y 0.2, lo que significa que la mauoría de los clientes tienen baja probabilidad de irse. Además no hay clientes con altas probabilidades de irse.

```{r}
library(caret)

# Asegurar que ambos factores tengan los mismos niveles
clase_pred <- factor(ifelse(predicciones > 0.5, 1, 0), 
                    levels = c(0, 1))

# Convertir train_data$churn a factor con los mismos niveles (si no lo está)
train_data$churn <- factor(train_data$churn, levels = c(0, 1))
test_data$churn <- factor(test_data$churn, levels = c(0, 1))

# Verificar niveles
levels(clase_pred)
levels(train_data$churn)

# Ahora ejecutar la matriz de confusión
conf_matrix <- confusionMatrix(clase_pred, train_data$churn, positive = "1")
print(conf_matrix)
```
 Análisis de matriz de confusión 
Esta matriz de confusión muestra:
- Verdaderos Negativos (VN): 6161 (predicción correcta de "no churn")
- Falsos Positivos (FP): 220 (predicción incorrecta de "churn")
- Falsos Negativos (FN): 1285 (casos de churn no detectados)
- Verdaderos Positivos (VP): 334 (churn correctamente identificados)

El modelo tiene un accuracy del 81.2%, pero presenta un grave problema de sensibilidad: 
- Solo detecta el 20.6% de los casos reales de churn (335 falsos negativos)
- Aunque identifica correctamente el 96.5% de los "no churn" (alta especificidad)

Esto indica que el modelo es demasiado conservador, priorizando evitar falsas alarmas (FP) a costa de perder muchos casos reales (FN).


```{r}
# Cálculo de métricas 
confmat1 <- confusionMatrix(
  data = factor(clase_pred, levels = c("0", "1")),  # Asegurar niveles consistentes
  reference = factor(train_data$churn, levels = c("0", "1")),  # Usar misma codificación
  positive = "1"  # Clase positiva correctamente especificada
)

# Cálculo F1 Score optimizado
Precision <- confmat1$byClass["Pos Pred Value"]
Recall <- confmat1$byClass["Sensitivity"]
F1score <- ifelse((Precision + Recall) == 0, 0, 
                 2 * (Precision * Recall) / (Precision + Recall))

cat("\nMétrica F1 Score:", round(F1score, 3), 
    "\n(Valor bajo indica desbalance entre precisión y recall)\n")
```
El F1Score es un indicador de balance entre precisión y recall. Un 0.33 es un balance muy bajo, lo que indica que el modelo no está haciendo un gran trabajo prediciendo quién se va a dar de baja.

```{r}
# Predicciones

predicciones_test <- predict(modelo_completo, newdata = test_data, type = "response")

clase_pred_test <- factor(ifelse(predicciones_test > 0.5, 1, 0), 
                    levels = c(0, 1))

# Cálculo de métricas CORREGIDO
confmat2 <- confusionMatrix(
  data = factor(clase_pred_test, levels = c("0", "1")),  # Asegurar niveles consistentes
  reference = factor(test_data$churn, levels = c("0", "1")),  # Usar misma codificación
  positive = "1"  # Clase positiva correctamente especificada
)

print(confmat2)

# Cálculo F1 Score optimizado
Precision <- confmat2$byClass["Pos Pred Value"]
Recall <- confmat2$byClass["Sensitivity"]
F1score <- ifelse((Precision + Recall) == 0, 0, 
                 2 * (Precision * Recall) / (Precision + Recall))

cat("\nMétrica F1 Score:", round(F1score, 3), 
    "\n(Valor bajo indica desbalance entre precisión y recall)\n")
```
Comprobamos que hay un rendimiento muy similar al anterior, por lo que el modelo no presenta sobreajuste. 


Calculamos la curva ROC.
```{r}
library(pROC)

roc_score<-roc(test_data$churn, predicciones_test, plot=TRUE,print.auc=TRUE)
```
Podemos ver como la AUC no es particularmente grande debido a la baja sensibilidad del modelo

# K-NN

Vamos a aplicar ahora el algoritmo de k vecinos más cercanos (k-nn). Para la elección de k utilizaremos cross validation con 10 folds. Con esta k entrenaremos el modelo k-nn en nuestro dataset de entrenamiento para luego aplicarlo a nuestros datos de test y ver cómo de bueno es prediciendo. 

```{r k-nn}
# Convertimos churn a factor
train_data$churn <- as.factor(train_data$churn)
levels(train_data$churn) <- make.names(levels(train_data$churn))

# Hacemos grid search y cross validation para elegir la k
set.seed(42)
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)

# Entrenamos el modelo con los datos de entrenamiento utilizando accuracy como nuestra métrica
modelo_knn <- train(
  churn ~ ., data = train_data, method = "knn",
  trControl = train_control, metric = "Accuracy",
  preProcess = c("center", "scale"), 
  tuneLength = 10
)

cat("Resultados k-NN\n"); print(modelo_knn)
cat("Mejor k:", modelo_knn$bestTune$k, "\n")
plot(modelo_knn, main = "Rendimiento k-NN según la elección de k")
summary(modelo_knn)
```
Vemos como el número ideal de vecinos es 23, lo hemos elegido ya que es el que tiene mayor ROC 

Ahora vamos a ver como funciona el algoritmo en nuestros datos de entrenamiento, para ello vamos a utilizar la matriz de confusión y vamos a analizar las distintas métricas que podemos extraer de ahí
```{r k-nn train}
set.seed(123)
# Hacemos la predicción con nuestros datos de train
prediction <- predict(modelo_knn, newdata = train_data)
# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
cf <- confusionMatrix(
    data = prediction,                    
    reference = train_data$churn,        
    positive = levels(train_data$churn)[2]          
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf)

# imprimimos tmabien la curva roc
clase_positiva <- levels(train_data$churn)[2]
probabilidades_predichas <- predict(modelo_knn, newdata = train_data, type = "prob")


predicciones_numericas <- probabilidades_predichas[, clase_positiva]


roc_curve_knn_train <- roc(response = train_data$churn,
                           predictor = predicciones_numericas, 
                           levels = levels(train_data$churn), 
                           positive = clase_positiva,
                           plot = TRUE,
                           print.auc = TRUE,
                           main = "Curva ROC - k-NN (train)"
                           )
```
Tenemos una buena accuracy, de un 84%, pero esto puede llevarnos a confusión, ya que nuestros datos están ciertamente desbalanceados (tenemos muchos más datos de clientes que no se van del bank_data que clientes que si que lo hacen). Es por ello que debemos fijarnos en otras métricas. 

La naturaleza desbalanceada de los datos también se ve en el test de Mcnemar, el cual hace el ratio entre FP y FN. Al ser tan pequeño, nos demuestra que nuestros datos están muy desbalanceados y que hacemos muchos más fallos al predecir positivos que negativos. 

Nuestro algoritmo de K-nn es bueno prediciendo cuando un cliente se va a quedar en el bank_data, esto lo vemos con la NIR, que es de un 79% y con la especificidad, que es de un 98%, con lo que predecimos bien un 98% de los casos de clientes que se quedan. En cambio, nuestra Recall (sensitivity) es demasiado baja, no predecimos correctamente los positivos. Como el modelo falla en aproximadamente un 75% de las predicciones de clientes que se van del bank_data, no lo podemos considerar un buen modelo para nuestros datos.

A pesar de ello, tenemos buena precisión en las predicciones, ya que cuando predice que un cliente se va a ir, en un 78% de las ocasiones se marcha del bank_data. 
En conclusión, el modelo de k-nn es mejor que el azar, pero falla significativamente a la hora de predecir cuando un cliente se va del bank_data como nos muestra la sensitivity. Deberemos probar otros algoritmos de clasificación que harán nuestras predicciones mejores.

Vamos a comprobarlo en el dataset de test
```{r k-nn test}
# Nos aseguramos de que el churn es un factor
test_data$churn <- as.factor(test_data$churn)
levels(test_data$churn) <- make.names(levels(test_data$churn))

prediction <- predict(modelo_knn, newdata = test_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(test_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
cf_knn_test <- confusionMatrix(
    data = prediction,                      
    reference = test_data$churn,        
    positive = levels(test_data$churn)[2]           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf_knn_test)

# imprimimos tmabien la curva roc
clase_positiva <- levels(test_data$churn)[2]
probabilidades_predichas <- predict(modelo_knn, newdata = test_data, type = "prob")


predicciones_numericas <- probabilidades_predichas[, clase_positiva]


roc_curve_knn_test <- roc(response = test_data$churn,
                           predictor = predicciones_numericas, 
                           levels = levels(test_data$churn), 
                           positive = clase_positiva,
                           plot = TRUE,
                           print.auc = TRUE,
                           main = "Curva ROC - k-NN (test)"
                           )
```
Vemos como todo lo mencionado en las predicciones de train aplica aqui, por lo que no tenemos sobreajuste.

# Análisis discriminante lineal

Como nuestros datos no siguen una distribución normal ni son lineales como vimos en la primera parte de la práctica, no podremos aplicarlo a nuestros datos, es por ello que solamente vamos a aplicarlo de manera manual con unos datos que cumplen las condiciones necesarias

# Árboles de decisión

Vamos a aplicar el método del árbol de decisión (DT) a nuestros datos, en el que hacemos un modelo para predecir la variable churn a partir de otras variables.
```{r}
library(rpart)
library(rpart.plot)

set.seed(128)

datos_DT <- train_data %>%
  dplyr::select(credit_score, country, gender, age, tenure, balance, products_number, credit_card, active_member, estimated_salary, churn)
fit.dt <- rpart(churn~., data = datos_DT, method = 'class')

rpart.plot(fit.dt, extra = 100)

```
Reglas Clave y su Significado:

    -products_number < 3 (Aparece 3 veces)

        Indica que el número de productos contratados es el factor más determinante.

        Clientes con menos de 3 productos son más propensos a abandonar (probablemente por menor engagement).

    -active_member = +

        Clientes activos (ej: que usan servicios frecuentemente) tienen menor riesgo de churn.

        El símbolo + sugiere que esta condición reduce la probabilidad de abandono.

    -products_number >= 2

        Clientes con 2 o más productos tienen menor riesgo.

        Corrobora que la retención mejora con más productos contratados.

    -country = France, Spain

        Ubicación geográfica como predictor:

            Francia y España podrían tener mayor churn que otros países (ej: Alemania).

            O podría ser un nodo intermedio para otras divisiones (depende del árbol completo).

    -balance >= 70,000

        Clientes con saldos altos (≥70k) tienen comportamiento distinto.

        Posiblemente:

            Mayor balance → Menor churn (por relación más valiosa).

            O podría asociarse a otros factores (ej: productos adicionales).

Interpretación del Modelo:

    -Variables más importantes:

        products_number (principal), active_member, country, balance.

    -Perfil de alto riesgo:

        Clientes con <3 productos, inactivos, y de Francia/España.

    -Perfil de bajo riesgo:

        Clientes con ≥2 productos, activos, y saldos altos.
        

Usamos el modelo DT para realizar predicciones:
```{r}
# Sobre la partición de entrenamiento
prediccion_DT <- predict(fit.dt, train_data, type = 'class')
cf_DT <- confusionMatrix(prediccion_DT, as.factor(datos_DT$churn), positive = "X1")

print(cf_DT)

clase_positiva <- levels(train_data$churn)[2]
probabilidades_predichas <- predict(fit.dt, newdata = train_data, type = "prob")


predicciones_numericas <- probabilidades_predichas[, clase_positiva]


roc_curve_dt_train <- roc(response = train_data$churn,
                           predictor = predicciones_numericas, 
                           levels = levels(train_data$churn), 
                           positive = clase_positiva,
                           plot = TRUE,
                           print.auc = TRUE,
                           main = "Curva ROC - DT (train)"
                           )
```
Métricas Principales:

    Precisión (Accuracy): 86.2%

        El modelo acierta en el 86.2% de los casos.

        Intervalo de confianza (95%): 85.4% - 86.9%

    Sensibilidad: 40.8%

        Detecta solo el 40.8% de los casos reales de abandono (churn).

        Problema grave: Pierde el 59.2% de los clientes que realmente se van.

    Especificidad: 97.7%

        Identifica correctamente el 97.7% de los clientes que no abandonan.

    (PPV): 81.8%

        Cuando predice abandono, acierta en el 81.8% de los casos.

    Kappa: 0.47

        Acuerdo moderado entre predicciones y realidad.

Interpretación:

    El modelo es bueno evitando falsas alarmas (solo 147 FP)

    Pero falla en detectar abandonos reales (959 FN no detectados)

    Desequilibrio importante: Hay 6.5 veces más FN que FP (959 vs 147)

Vamos a implementarlo en el df de test
```{r}
# Sobre la partición de prueba
prediction.dt <- predict(fit.dt, test_data, type = 'prob')[,2]
clase.pred=ifelse(prediction.dt>0.5,"X1","X0")

cf_DT_prueba <- confusionMatrix(as.factor(clase.pred), as.factor(test_data$churn),positive="X1")

print(cf_DT_prueba)

clase_positiva <- levels(test_data$churn)[2]
probabilidades_predichas <- predict(fit.dt, newdata = test_data, type = "prob")


predicciones_numericas <- probabilidades_predichas[, clase_positiva]


roc_curve_dt_test <- roc(response = test_data$churn,
                           predictor = predicciones_numericas, 
                           levels = levels(test_data$churn), 
                           positive = clase_positiva,
                           plot = TRUE,
                           print.auc = TRUE,
                           main = "Curva ROC - DT (test)"
                           )

```
Métricas clave:

    - Precisión (Accuracy): 85.0%

        El modelo acierta en el 85% de las predicciones

        Intervalo de confianza 95%: (83.36%, 86.54%)

    - Sensibilidad/Recall: 38.76%

        Detecta solo el 38.76% de los casos reales de churn

        Problema principal: No detecta el 61.24% de los abandonos reales (256 FN)

    - Especificidad: 97.22%

        Identifica correctamente el 97.22% de los no abandonos

        Solo 44 falsas alarmas (FP)

    (PPV): 78.64%

        Cuando predice abandono, acierta en el 78.64% de los casos

    - Kappa: 0.4423

        Acuerdo moderado entre predicciones y realidad

Interpretación detallada:

    - Balance de clases:

        Prevalencia de churn: 20.9% (ligeramente desbalanceado)

        El modelo predice churn en el 10.3% de los casos (Detection Prevalence)

    - Rendimiento en detección de churn:

        Solo detecta 162 de 418 casos reales (162 VP de 418 = 38.76%)

        256 falsos negativos (clientes que abandonaron pero no fueron detectados)

    - Comparación con modelo aleatorio:

        P-value < 2.2e-16 indica que el modelo es significativamente mejor que adivinar al azar

        Pero la mejora es insuficiente para la detección de churn

# Bagging

Vamos a incluir ahora os métodos de ensamblado que se basa en la unión
de múltiples modelos que pueden mejorar nuestras predicciones, la idea
de estos modelos denominados métodos de ensamblado es encadenarlos para
mejorar dichas predicciones.

Haremos el ensamblado de bagging al principio, que consiste en una
técnica de ML hecha para mejorar la predicción de los modelos y como
hemos dicho antes se basa en construir múltiples modelos similares y
combinar dichas predicciones para obtener un resultado final más fiable.

## Bagging

**Bagging** funciona de la siguiente manera, empieza con la división del
conjunto de datos inicial en distintos subconjutnos datos y usamos algo
llamado muestreo de reemplazado, esto genera distintos conjuntos de
entrenamientos ligeramente distintos entre ellos, el siguiente paso
sería entrenar el modelo base, y cada modelo es distinto debido a el
distinto muestreo, como tercer paso encontramos que una vez todas los
modelos hayan sido entrenados y se utilizan para hacer predicciones
individuales sobre un conjunto de datos de prueba, como paso final
obtenemos que todas las predicciones de los modelos se combinan para
tener una mejor predicción.

El algoritmo más conocido para **Bagging** es el que desarrollaremos
ahora que es denominado **"Random Forest"** que básicamante combina
multiples DTs para lograr un modelo predictivo altamente preciso.

## Random Forest

```{r}
library(ggplot2)
library(caret)
library(readr)
library(dplyr)

# 1. Cargar y preparar los datos
Data <- read_csv("Bank Customer Churn Prediction.csv")
bank_data$churn <- as.factor(Data$churn)

# Seleccionar variables relevantes
predictors <- c("credit_score", "age", "tenure", "balance", 
                "products_number", "estimated_salary", "country", "gender", "active_member")
response <- "churn"

data <- Data %>% 
  select(all_of(c(predictors, response))) %>% 
  na.omit() %>%
  mutate(
    country = as.factor(country),
    gender = as.factor(gender),
    active_member = as.factor(active_member),
    churn = as.factor(churn)
  )

# 2. Configuración del experimento
set.seed(123)
n_reps <- 50
test_size <- 0.2

# Crear conjunto de test
test_idx <- createDataPartition(bank_data$churn, p = test_size, list = FALSE)
test_data <- bank_data[test_idx, ]
train_data_base <- bank_data[-test_idx, ]

# 3. Funciones para realizar las predicciones
run_bagging <- function(train_data) {
  model <- randomForest(churn ~ ., 
                       data = train_data,
                       mtry = length(predictors), # Usa todas las variables
                       ntree = 100)
  return(model)
}

run_rf <- function(train_data) {
  model <- randomForest(churn ~ .,
                       data = train_data,
                       mtry = floor(sqrt(length(predictors))), # Selección típica RF
                       ntree = 100)
  return(model)
}

# 4. Ejecutar las repeticiones
results <- lapply(1:n_reps, function(i) {
  # Muestreo bootstrap
  sample_idx <- sample(1:nrow(train_data_base), nrow(train_data_base), replace = TRUE)
  train_data <- train_data_base[sample_idx, ]
  
  # Entrenar modelos
  bagging_model <- run_bagging(train_data)
  rf_model <- run_rf(train_data)
  
  # Hacer predicciones
  bagging_pred <- predict(bagging_model, test_data, type = "prob")[,2]
  rf_pred <- predict(rf_model, test_data, type = "prob")[,2]
  
  return(data.frame(
    rep = i,
    observation = 1:nrow(test_data),
    bagging_pred = bagging_pred,
    rf_pred = rf_pred,
    actual = test_data$churn
  ))
})

# Combinar todos los resultados
all_results <- bind_rows(results)

# 5. Calcular varianzas
variances <- all_results %>%
  group_by(observation, actual) %>%
  summarise(
    bagging_var = var(bagging_pred),
    rf_var = var(rf_pred),
    .groups = 'drop'
  )

# 6. Visualización
ggplot(variances, aes(x = observation)) +
  geom_line(aes(y = bagging_var, color = "Bagging"), linewidth = 1) +
  geom_line(aes(y = rf_var, color = "Random Forest"), linewidth = 1) +
  facet_wrap(~ actual, labeller = labeller(actual = c("0" = "No Churn", "1" = "Churn"))) +
  labs(title = "Varianza de Predicciones: Bagging vs Random Forest",
       x = "Observaciones en Conjunto de Test",
       y = "Varianza",
       color = "Método") +
  scale_color_manual(values = c("Bagging" = "blue", "Random Forest" = "red")) +
  theme_minimal()

# 7. Métricas de rendimiento
final_pred <- all_results %>%
  group_by(observation) %>%
  summarise(
    bagging_mean = mean(bagging_pred),
    rf_mean = mean(rf_pred),
    actual = first(actual)
  )

# Umbral de decisión
final_pred$bagging_class <- ifelse(final_pred$bagging_mean > 0.5, 1, 0)
final_pred$rf_class <- ifelse(final_pred$rf_mean > 0.5, 1, 0)

# Matrices de confusión
cat("Bagging Performance:\n")
cf_bagging<-confusionMatrix(factor(final_pred$bagging_class, levels = c(0, 1)), 
                factor(final_pred$actual, levels = c(0, 1)))
print(cf_bagging)

cat("\nRandom Forest Performance:\n")
cf_rf <- confusionMatrix(factor(final_pred$rf_class, levels = c(0, 1)), 
                factor(final_pred$actual, levels = c(0, 1)))
print(cf_rf)
```

Lo primero a analizar de estos gráficos es la varianza de predicciones.
Tenemos dos gráficos, uno para clientes que permanecen y otro para los
que no. Empecemos por los que si permanecen, podemos ver un patrón en el
cual las predicciones de varianza son bajas, por debajo de 0.01, ambos
modelos muestran una alta estabilidad para los clientes que no
abandonan,o aunque Random Forest tiene una pquela ventaja con varianza
ligeramente menor que Bagging.

Por el contrario para los clientes que abandonan la varianza es
significativamente mayor con puntos de hasta 0.125, por eso podemos
decir que las predicciones son menos consistentes, Bagging muestra picos
de alta varianza lo que lo hace más difícil de clasificar, por otro lado
Random FOrest mantiene mejor control de la varianza sobre todo entre los
rangos de 500-1000 y 1500-2000.

**Hallazgos importantes:**

**Exactitud global:** 86.21% (buen desempeño general)

**Sensibilidad (No Churn):** 96.80% (excelente para identificar
permanencia)

**Especificidad (Churn):** Solo 44.85% (dificultad para detectar
abandonos)

**Prevalencia:** 79.61% de los casos son "No Churn" (dataset
desbalanceado)

**Problemas identificados:**

**Alto número de falsos negativos:** 225 casos de Churn fueron mal
clasificados

**Baja especificidad:** El modelo tiende a predecir "No Churn" incluso
cuando el cliente abandona

**Valor predictivo negativo:** Solo 78.21%, prácticamente tres cuartos
de las predicciones son son correctas lo que es bajo.

En este gráfico comparamos la varianza de Bagging con la de Random
Forest.

Si ponemos en conjunto ambos resultados, podemos ver como para "No
Churn" tenemos: Baja varianza y Alta sensibilidad (96,80%) por lo tanto
el modelo es consistente y preciso para esta clase. El otro lado es para
"Churn" en este caso tenemos: Alta varianza y Baja especificidad
(44,85%) por lo que encontramos mucha inconsistencia en las predicciones
se refleja en la dificultad para identificar abandonos.

Comparando modelos podemos ver que **Random Forest** nos muestra menor
varianza en ambos casos sobre todo para **Churn**, mayor estabilidad en
predicciones difíciles y seguramente mejor generalización gracias a la
selección aleatoria de variables, para el modelo de **Bagging** tenemos
mayor varianza especualmente en los límites pero un posible sobreajuste
debido al uso de todas las variables en cada división

Aplicamos este método de ML a nuestro conjunto de datos

```{r}
rf_full <- randomForest(as.factor(churn) ~ ., 
                        data = train_data_base, 
                        importance = TRUE, 
                        proximity = TRUE)

# Ver el resumen del modelo
print(rf_full)

```

Tenemos un error global de 14.03% (OOB estimate) → 85.97% de precisión

La estructura del modelo está formada por 500 árboles de decisión y 2
variables consideradas en cada división (mtry = 2)

**Análisis por Clases**

**Para clientes que NO abandonan (0):** Tenemos 6135 clientes
correctamente clasificados pero 235 que no lo están eso nos da un 3.69%
de error eso significa que tenemos un gran modelo para los clientes que
se quedan en el bank_data

**Para clientes que SÍ abandonan (1):** Tenemos 703 clientes
correctamente clasificados pero 926 que no lo están eso nos da un 56.84%
de error eso significa que tenemos un modelo que para los clientes que
se van del bank_data no los detecta de manera correcta con más de la mitad
de ellos sin ser bien detectados.

```{r}
plot(rf_full)
title(main = "Error OOB y por Clase - Random Forest")
legend("topright", 
       legend = colnames(rf_full$err.rate), 
       col = 1:ncol(rf_full$err.rate), 
       lty = 1, 
       cex = 0.8)
```

Para poder sacar conclusiones de este gráfico tenemos que ver la curva
de error de **OOB(Negra)** que nos muestra como el error global va
disminuyendo a medida que se añaden añaden árboles, por lo que podría
llegar a estabilizarse.

Vamos a ver los errores por clases **Clase 0(No Churn-Rojo)** el error
es bastante bajo y coincide con la alta precisión de la matriz de
confusión vista antes, por el otro lado **Clase 1(Churn - Verde)** tiene
un error considerablemente alto que disminuye lentamente,

```{r}
# Partición de test
# Predecir probabilidades
prediction.rf <- predict(rf_full, test_data, type = "prob")[,2]

# Convertir probabilidades a clases
clase.pred.rf <- ifelse(prediction.rf > 0.5, "1", "0")

# Matriz de confusión
cf_rf_test <- confusionMatrix(as.factor(clase.pred.rf), as.factor(test_data$churn), positive = "1")
print(cf_rf_test)

clase_positiva <- levels(test_data$churn)[2]
probabilidades_predichas <- predict(rf_full, newdata = test_data, type = "prob")


predicciones_numericas <- probabilidades_predichas[, clase_positiva]


roc_curve_rf_test <- roc(response = test_data$churn,
                           predictor = predicciones_numericas, 
                           levels = levels(test_data$churn), 
                           positive = clase_positiva,
                           plot = TRUE,
                           print.auc = TRUE,
                           main = "Curva ROC - random forest (test)"
                           )

```

Lo primero que vamos a analizar es la matriz de confusión y nos muestra
el desempeño del modelo Random Forest en los datos de prueba, aparte de
esto nos da más información:

-   **Precisión:** 86.41%
-   **Intervalo de Confianza del 95%:** (84%, 87.89%)
-   **Sensibilidad (Recall para Churn):** 46,07%, solo detecta ese
    porcentaje de de casos reales de abandono
-   **Especificidad:** 96,70% para identificar clientes que permanecen.
-   **Precisión:** 74.01%, acierta el 74% de las veces que predice un
    abandono.
-   **KAPPA:** 0.4488, es el valor de la concordancia moderada entre
    predicciones y realidad.

```{r}
importance(rf_full)
```

Para entender esta tabla es fundamental entender que significa
*"MeanDecreaseAccuracy"* y *"MeanDecreaseGini"* y por lo tanto que miden
estas variables y cuáles tienen un mayor impacto

-   **MeanDecreaseAccuracy** mide cuánto disminuye la exactitud del
    modelo al eliminar cada variable, a mayor valor más impacto tiene
    por lo que estimated_salary (109.72) , tenure (92.07) y
    products_number(48.82).

-   **MeanDecreaseGini** mide la contribución a la pureza de los nodos
    (homogeneidad) y sus valores más importantes son tenure (521.71),
    estimated_salary (348.31) y products_number (323.64).

Veamos esto representado en un gráfico.

```{r}
varImpPlot(rf_full)
```

Una vez vemos esto de manera más visual vemos que estas tres variables
vistas antes son más importantes a la hora de predecir el abandono en
los bank_datas, por otro lado renemos los valores de gender y country que
permanecen en ambos gráficos con valores muy pequeños por lo que podrían
ser variables para considerar para eliminar.

# Boosting

Vamos a aplicar a continuación distintos modelos de boosting, estos modelos deberían mejorar el rendimiento de nuestras predicciones, así como ayudarnos a reducir el sesgo y el desequilibrio de nuestros datos (que como hemos podido observar en algunos modelos esto es bastante importante para nosotros). A pesar de estas ventajas, la aplicación de modelos de boosting reduce la interpretabilidad de nuestras predicciones, además de complicarnos el ajuste de los hiperparámetros

Primeramente vamos a implementar AdaBoost

### Adaboost
Este modelo es interesante ya que sigue siendo un modelo aditivo, pero como utiliza la pérdida exponencial, reduce la influencia de los ejemplos mal clasificados y como le da más peso a aquellas iteraciones que más difíciles han sido de clasificar, nos garantiza que el modelo final es robusto. 
```{r adaboost}
set.seed(42)
bank_data <- bank_data <- read.csv('Bank Customer Churn Prediction.csv')
train_index <- createDataPartition(bank_data$churn, p = 0.8, list = FALSE, times = 1)
train_data <- bank_data[train_index, ]
test_data <- bank_data[-train_index, ]

train_data$churn <- as.factor(train_data$churn)
levels(train_data$churn) <- make.names(levels(train_data$churn))
test_data$churn <- as.factor(test_data$churn)
levels(test_data$churn) <- make.names(levels(test_data$churn))

set.seed(128)
# Utilizamos validación cruzada para hallar los hiperparámetros
trainControl_adaboost <- trainControl(
    method = "repeatedcv",
    number = 10,       # 10 folds
    repeats = 3,       # 3 veces
    verboseIter = FALSE 
)

ada_grid <- expand.grid(
    mfinal = 50, # numero de arboles
    maxdepth = c(2,3), # profundidad de los árboles
    coeflearn = c('Breiman', 'Freund') # coeficiente de aprendizaje
    )

model_adaboost <- train(
  churn ~ ., # utilizamos todas las características para predecir el churn
  data = train_data,         
  method = "AdaBoost.M1",        
  trControl = trainControl_adaboost, 
  metric = "Accuracy",          
  tuneGrid = ada_grid,          
  preProcess = c("center", "scale") 
)

cat("\nResultados del entrenamiento\n")
print(model_adaboost)
cat("\nMejores hiperparámetros:\n")
print(model_adaboost$bestTune)

# Representamos el rendimiento en base a los mejores hiperparámetros
plot(model_adaboost)
summary(model_adaboost)
```

Hacemos la predicción primeramente con nuestros datos de entrenamiento para ver como predice nuestro modelo. Para ello vamos a utilizar la matriz de confusión
```{r adaboost train}
set.seed(123)
prediction <- predict(model_adaboost, newdata = train_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(train_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
positive_class_label <- levels(train_data$churn)[2] 
cf <- confusionMatrix(
    data = prediction,                      
    reference = train_data$churn,        
    positive = positive_class_label           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf)

prediction_prob_train <- try(predict(model_adaboost, newdata = train_data, type = "prob"))
current_levels <- levels(train_data$churn)

roc_curve_adaboost_train <- roc(
        response = train_data$churn,
        predictor = prediction_prob_train[[positive_class_label]],
        levels = current_levels,
        quiet = TRUE
        )

# Ploteamos la curva ROC
plot(roc_curve_adaboost_train,
         print.auc = TRUE,
         main = "Curva ROC del modelo AdaBoost (Training)",
         legacy.axes = TRUE
         )
```
Podemos ver como este modelo tiene una accuracy de un 86% lo cual es bastante bueno, aunque debemos indagar más en el resto de métricas para ver si realmente nuestras predicciones son buenas. 
El índice Kappa es mejor que en otros modelos, cerca del 50%, aunque sigue siendo bajo. El test McNemar nos sigue mostrando este desbalance en nuestros datos, el modelo sigue prediciendo mejor aquellos que no se van, que aquellos que si lo hacen. 
Siguiendo esta línea, nuestro recall e de solo un 48%, lo cual sigue siendo bajo, ya que significa que predecimos mal la mitad de aquellos que se marchan. 
La especificidad es alta, asi como el NPV, ya que predecimos bien los negativos, con cerca de un 96% de las predicciones siendo correctas. 
Por último. comparando AUC y sensitividad podemos extraer que el threshold de 0.5 para la AUC no es el ideal en nuestro caso, ya que si eligieramos otro threshold, tendríamos una sensitividad más alta a expensas de tener una especificidad y precisión más bajas, algo que nos podría llegar a interesar puesto que nuestro objetivo es que los clientes no se vayan del bank_data.

A continuación lo aplicamos en los datos de test
```{r adaboost test}
prediction <- predict(model_adaboost, newdata = test_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(test_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
cf_ada_test <- confusionMatrix(
    data = prediction,                      
    reference = test_data$churn,        
    positive = levels(test_data$churn)[2]           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf_ada_test)

prediction_prob_train <- try(predict(model_adaboost, newdata = test_data, type = "prob"))
current_levels <- levels(test_data$churn)

roc_curve_adaboost_test <- roc(
        response = test_data$churn,
        predictor = prediction_prob_train[[positive_class_label]],
        levels = current_levels,
        quiet = TRUE
        )

# Ploteamos la curva ROC
plot(roc_curve_adaboost_test,
         print.auc = TRUE,
         main = "Curva ROC del modelo AdaBoost (test)",
         legacy.axes = TRUE
         )
```
Vemos como todo lo dicho anteriormente para los datos de entrenamiento aplica de nuevo a nuestros datos de test, por lo que no tenemos sobreajuste.

Vamos a ver qué ocurre cuando aplicamos otros algoritmos de boosting como Gradient Boosting: 

### Gradient boosting

```{r gbm}
set.seed(128) # para reproducibilidad
# Usaremos validación cruzada repetida
trainControl_gbm <- trainControl(
    method = "repeatedcv",
    number = 10,        # 10 folds
    repeats = 3,        # 3 repeticiones
    verboseIter = FALSE, 
    classProbs = TRUE,
    summaryFunction = twoClassSummary
)

gbm_grid <- expand.grid(
    n.trees = 100, # numero de arboles
    interaction.depth = c(1, 2, 3), # profundidad 
    shrinkage = c(0.1, 0.05), # tasa de aprendizaje
    n.minobsinnode = c(10, 15) # minimo de observaciones en el ultimo nodo
    )

model_gbm <- train(
  churn ~ .,
  data = train_data,
  method = "gbm", 
  trControl = trainControl_gbm, 
  metric = "Accuracy",
  tuneGrid = gbm_grid,
  preProcess = c("center", "scale"), 
  verbose = FALSE
)
cat("Resultados del entrenamiento\n")
print(model_gbm)

# Representamos el modelo y sus métricas
plot(model_gbm)
cat("\nImportancia de las variables:\n")
gbm_importancia <- varImp(model_gbm, scale = FALSE)
plot(gbm_importancia) # Visualizamos la importancia de cada variable en el modelo

```

```{r gbm train}
prediction <- predict(model_gbm, newdata = train_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(train_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
positive_class_label <- levels(train_data$churn)[2] 
cf <- confusionMatrix(
    data = prediction,                      
    reference = train_data$churn,        
    positive = positive_class_label           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf)

prediction_prob_train <- try(predict(model_gbm, newdata = train_data, type = "prob"))
current_levels <- levels(train_data$churn)

roc_curve_gbm_train <- roc(
        response = train_data$churn,
        predictor = prediction_prob_train[[positive_class_label]],
        levels = current_levels,
        quiet = TRUE
        )

# Ploteamos la curva ROC
plot(roc_curve_gbm_train,
         print.auc = TRUE,
         main = "Curva ROC del modelo GBM (Training)",
         legacy.axes = TRUE
         )
```
En este caso, nuestra sensitivity aumenta ligeramente, igual que lo hace la accuracy. Estos aumentos no son significativos por lo que podemos decir que el modelo predice de igual manera que el anterior. Sin embargo, lo que si ganamos significativamente es tiempo. El modelo Adaboost es mucho más lento que el modelo GBM, por lo que, al hacer ambos predicciones similares, sería más eficiente elegir el gradient boosting para nuestro problema. 

Vamos a ver qué ocurre en los datos de test, aunque obviamente esperamos un resultado similar

```{r gbm test}
prediction <- predict(model_gbm, newdata = test_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(test_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
positive_class_label <- levels(test_data$churn)[2] 
cf_gbm_test <- confusionMatrix(
    data = prediction,                      
    reference = test_data$churn,        
    positive = positive_class_label           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf_gbm_test)

prediction_prob_test <- try(predict(model_gbm, newdata = test_data, type = "prob"))
current_levels <- levels(test_data$churn)

roc_curve_gbm_test <- roc(
        response = test_data$churn,
        predictor = prediction_prob_test[[positive_class_label]],
        levels = current_levels,
        quiet = TRUE
        )

# Ploteamos la curva ROC
plot(roc_curve_gbm_test,
         print.auc = TRUE,
         main = "Curva ROC del modelo GBM (Test)",
         legacy.axes = TRUE
         )
```
Como imaginabamos, los resultados son muy similares al train, por lo que no hay sobreajuste

Por último vamos a implementar el modelo XGBoost o extreme gradient boosting, el cual nos debería dar mejor resultado, no solo prediciendo, sino también en eficiencia.

### XGBoost

```{r}
set.seed(128)
trainControl_xgb_roc <- trainControl(
    method = "repeatedcv",
    number = 10,    
    repeats = 3,
    classProbs = TRUE, 
    summaryFunction = twoClassSummary, 
    verboseIter = FALSE, 
    allowParallel = TRUE,
    sampling = "up"
)
xgb_grid <- expand.grid(
    nrounds = 100,
    max_depth = c(3, 6), # Profundidad máxima del árbol
    eta = c(0.05, 0.1), # Tasa de aprendizaje
    gamma = c(0.1, 0.2), # Tasa de regularización
    colsample_bytree = c(0.7), # Fracción de columnas por árbol 
    min_child_weight = c(1, 3), # Peso mínimo por cada nodo hijo
    subsample = c(0.7) # Fracción de muestras por árbol
)
# imprimimos los hiperparámetros del modelo
print(xgb_grid)

model_xgboost <- train(
  churn ~ .,
  data = train_data,
  method = "xgbTree",
  trControl = trainControl_xgb_roc,
  metric = "ROC",
  tuneGrid = xgb_grid,
  preProcess = c("center", "scale"),
  verbose = FALSE
)

print(model_xgboost)
# Representaciones
xgb_importancia <- varImp(model_xgboost, scale = FALSE)
print(xgb_importancia)
plot(xgb_importancia)

```

Vamos a ver cómo funciona el modelo en nuestros datos de entrenamiento
```{r}
prediction <- predict(model_xgboost, newdata = train_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(train_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
positive_class_label <- levels(train_data$churn)[2] 
cf <- confusionMatrix(
    data = prediction,                      
    reference = train_data$churn,        
    positive = positive_class_label           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf)

prediction_prob_train <- try(predict(model_xgboost, newdata = train_data, type = "prob"))
current_levels <- levels(train_data$churn)

roc_curve_xgboost_train <- roc(
        response = train_data$churn,
        predictor = prediction_prob_train[[positive_class_label]],
        levels = current_levels,
        quiet = TRUE
        )

# Ploteamos la curva ROC
plot(roc_curve_xgboost_train,
         print.auc = TRUE,
         main = "Curva ROC del modelo XGBoost (Training)",
         legacy.axes = TRUE
         )
```
Tenemos un área bajo la curva (AUC) muy buena, por lo que el modelo diferencia muy bien entre positivos y negativos. En cuanto a las medidas de la matriz de confusión, en este modelo hemos perdido accuracy (sobre un 4%) pero ahora predecimos mucho mejor los TP y FN, nuestra recall es mucho mejor, pasando de un 50% aprox en otros modelos a un 75% en este, con lo que vemos que este modelo es mucho mejor prediciendo a los clientes que se van del banco. 

Como conseguimos mucha mejor recall, nuestra especificidad baja significativamente a un 83%, lo cual todavía es bastante bueno, pero predecimos peor a los que no se marchan. Todo esto nos lleva a una tasa mayor de falsos positivos, ya que ahora predecimos más casos de churn. A pesar de ello, mejoramos el NPV con lo que cuando predecimos un no, es más probable que sea cierto. 

En este caso, este modelo es mucho mejor si damos mayor importancia a identificar a aquellos que se quieren marchar sobre identificar a aquellos que se quedan

Vamos a ver si estos resultados se transfieren a nuestros datos de test o si hay overfitting
```{r}
set.seed(123)
prediction <- predict(model_xgboost, newdata = test_data)

cat("Primeras predicciones:", paste(head(prediction), collapse=", "), "\n")
cat("Primeros valores reales:", paste(head(test_data$churn), collapse=", "), "\n")

# Calculamos la matriz de confusión
# Como nosotros queremos predecir si un cliente se va o no del bank_data, elegimos aquellos que sí se van como nuestro positivo
positive_class_label <- levels(test_data$churn)[2] 
cf_xgb_test <- confusionMatrix(
    data = prediction,                      
    reference = test_data$churn,        
    positive = positive_class_label           
    )

# Imprimimos la matriz de confusión y sus métricas
print(cf_xgb_test)

prediction_prob_test <- try(predict(model_xgboost, newdata = test_data, type = "prob"))
current_levels <- levels(test_data$churn)

roc_curve_xgboost_test <- roc(
        response = test_data$churn,
        predictor = prediction_prob_test[[positive_class_label]],
        levels = current_levels,
        quiet = TRUE
        )

# Ploteamos la curva ROC
plot(roc_curve_xgboost_test,
         print.auc = TRUE,
         main = "Curva ROC del modelo XGBoost (Test)",
         legacy.axes = TRUE
         )
```
Como podemos ver, los resultados son similares en test por lo que no tenemos overfitting y nuestro modelo es robusto. 

Dependiendo de lo que prioricemos, si el elegir mejor a aquellos clientes que permanecen el banco o a aquellos que se van a marchar, deberemos elegir un modelo u otro. Esto lo estudiaremos un poco más adelante cuando comparemos los modelos y elijamos un modelo final para nuesto problema.

# Naïve bayes

Este algoritmo está basado en el conocido teorema de Bayes, que a pesar
de ser sencillos son muy útiles para problemas con muchas variables
entrada por lo que no puede ser útil, veamos como reaccionan nuestros
datos ante este test

```{r}
library(naivebayes)
# Seleccionar variables relevantes
predictors <- c("credit_score", "age", "tenure", "balance", 
                "products_number", "estimated_salary", "country", "gender", "active_member")
response <- "churn"

bank_data <- Data %>% 
  select(all_of(c(predictors, response))) %>% 
  na.omit() %>%
  mutate(
    country = as.factor(country),
    gender = as.factor(gender),
    active_member = as.factor(active_member),
    churn = as.factor(churn)
  )

# 2. Configuración del experimento
set.seed(123)
n_reps <- 50
test_size <- 0.2

# Crear conjunto de test
test_idx <- createDataPartition(bank_data$churn, p = test_size, list = FALSE)
test_data <- bank_data[test_idx, ]
train_data_base <- bank_data[-test_idx, ]

# Entrenar modelo Naive Bayes
nb_model <- naive_bayes(as.factor(churn) ~ ., data = train_data_base, usekernel = TRUE)

# Predecir probabilidades sobre el test
probabilities <- predict(nb_model, test_data[,-9], type = "prob")[,2]

# Clasificar según umbral 0.5
classes <- as.numeric(probabilities > 0.5)

# Matriz de confusión
cf_nb <- confusionMatrix(
  factor(classes, levels = c(0, 1)), 
  factor(test_data$churn, levels = c(0, 1)), 
  positive = "1"
)

# Imprimir resultados
print(cf_nb)

clase_positiva <- levels(test_data$churn)[2]
probabilidades_predichas <- predict(nb_model, newdata = test_data, type = "prob")


predicciones_numericas <- probabilidades_predichas[, clase_positiva]


roc_curve_naivebayes_test <- roc(response = test_data$churn,
                           predictor = predicciones_numericas, 
                           levels = levels(test_data$churn), 
                           positive = clase_positiva,
                           plot = TRUE,
                           print.auc = TRUE,
                           main = "Curva ROC - naive bayes (test)"
                           )
```

**Resultados de la matriz de confusión - Naive Bayes**

| Métrica | Valor | Interpretación |
|----|----|----|
| **Accuracy** | 83.76% | Precisión global aceptable. |
| **Sensitivity (Recall clase 1)** | 23.28% | Muy baja: detecta pocos clientes que hacen churn. |
| **Specificity** | 99.24% | Muy alta: predice muy bien los que **no** hacen churn. |
| **Pos Predictive Value (PPV)** | 88.78% | De los predichos como churn, el 77% realmente hacen churn. |
| **Neg Predictive Value (NPV)** | 83.48% | De los predichos como no churn, el 84% realmente no lo hacen. |
| **Balanced Accuracy** | 61,27% | Moderada, indica desequilibrio entre clases. |
| **Kappa** | 0.3105 | Aceptable, pero sugiere que el modelo puede mejorar. |
| **McNemar's Test p-value** | \<2.2e-16 | Hay diferencia significativa entre errores de clasificación. |


# Evaluación y Comparación de Modelos

Evaluamos todos los modelos en el conjunto de prueba y comparamos sus matrices de confusion y estadísticas asociadas

```{r}
#regresion logistica
print(confmat2)
# knn
print(cf_knn_test)
# decision tree
print(cf_DT_prueba)
# random forest
print(cf_rf_test)
# adaboost
print(cf_ada_test)
# gradient boosting
print(cf_gbm_test)
# xgboost
print(cf_xgb_test)
# naive bayes
print(cf_nb)

```
Vemos como todos los modelos tienen una especificidad muy alta, es decir, predicen muy bien a los clientes que se van a quedar. En cambio, el problema viene cuando nos fijamos en la sensitivity o recall, la cual es muy baja en la mayoría de modelos, casi siempre por debajo del 50% y muchas veces por debajo incluso del 30%. Teniendo en cuenta que el problema se trata de predecir el churn de los clientes de un banco, creemos que es más importante detectar a aquellos que se van a marchar, para tratar de evitar que lo hagan, frente a detectar a los que se quedan y fallar en los que se van a ir. En este sentido, claramente el mejor modelo es el xgboost, ya que es el único que predice con precisión estos verdaderos positivos, con un 75% de precisión. Esto se podría mejorar aún más haciendo el modelo más complejo, entrenando sobre más iteraciones y ajustando aun más los hiperparámetros. Por tanto, en este sentido el mejor modelo es claramente el xgboost

A continuación vamos a comparar las curvas ROC de los modelos 

```{r}
library(gridExtra)
create_roc_plot <- function(roc_obj, title) {
  roc_df <- data.frame(
    fpr = 1 - roc_obj$specificities,
    tpr = roc_obj$sensitivities
  )
  auc_val <- auc(roc_obj)

  ggplot(data = roc_df, aes(x = fpr, y = tpr)) +
    geom_line(color = "blue") +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
    labs(title = title,
         x = "FP rate",
         y = "TP rate",
         caption = paste("AUC =", round(auc_val, 3))) +
    theme_bw() +
    theme(plot.caption = element_text(hjust = 0.5))
}

# Crea una lista de los objetos roc y sus títulos
roc_objects <- list(
  list(roc_score, "Logística"),
  list(roc_curve_knn_test, "KNN"),
  list(roc_curve_dt_test, "DT"),
  list(roc_curve_rf_test, "RF"),
  list(roc_curve_adaboost_test, "Adaboost"),
  list(roc_curve_gbm_test, "GBM"),
  list(roc_curve_xgboost_test, "XGBoost"),
  list(roc_curve_naivebayes_test, "Naive Bayes")
)

# Crea una lista para almacenar los gráficos
plot_list <- list()

# Genera cada gráfico y lo añade a la lista
for (i in seq_along(roc_objects)) {
  roc_obj <- roc_objects[[i]][[1]]
  title <- roc_objects[[i]][[2]]
  plot_list[[i]] <- create_roc_plot(roc_obj, title)
}

# Organiza los gráficos en un grid de 2 filas y 4 columnas
grid.arrange(grobs = plot_list, nrow = 2, ncol = 4)
```


```{r}
# Define una paleta de colores para cada curva
colores <- c("blue", "red", "green", "purple", "orange", "brown", "darkgrey", "cyan")

plot(roc_score,
     col = colores[1],
     main = "Curvas ROC de los Modelos",
     legacy.axes = TRUE,
     xlab = "Tasa de Falsos Positivos",
     ylab = "Tasa de Verdaderos Positivos",
     print.auc = TRUE,
     print.auc.cex = 0.8,
     print.auc.y = 0.1
)

plot(roc_curve_knn_test,
     col = colores[2],
     add = TRUE,
     print.auc = TRUE,
     print.auc.cex = 0.8,
     print.auc.y = 0.2
)

plot(roc_curve_dt_test,
     col = colores[3],
     add = TRUE,
     print.auc = TRUE,
     print.auc.cex = 0.8,
     print.auc.y = 0.3
)

plot(roc_curve_rf_test,
     col = colores[4],
     add = TRUE,
     print.auc = TRUE,
     print.auc.y = 0.4,
     print.auc.cex = 0.8
)

plot(roc_curve_adaboost_test,
     col = colores[5],
     add = TRUE,
     print.auc = TRUE,
     print.auc.y = 0.5,
     print.auc.cex = 0.8
)

plot(roc_curve_gbm_test,
     col = colores[6],
     add = TRUE,
     print.auc = TRUE,
     print.auc.y = 0.6,
     print.auc.cex = 0.8
)

plot(roc_curve_xgboost_test,
     col = colores[7],
     add = TRUE,
     print.auc = TRUE,
     print.auc.y = 0.7,
     print.auc.cex = 0.8
)

plot(roc_curve_naivebayes_test,
     col = colores[8],
     add = TRUE,
     print.auc = TRUE,
     print.auc.y = 0.8,
     print.auc.cex = 0.8
)

# Añade una leyenda
legend("bottomright",
       legend = c("Regresión Logística", "KNN", "Árbol de Decisión", "Random Forest",
                  "Adaboost", "Gradient Boosting", "XGBoost", "Naive Bayes"),
       col = colores,
       lty = 1,
       cex = 0.7
)
```
Vemos como claramente los modelos de boosting son los que mejor AUC tienen en el conjunto de test, por lo que lo ideal sería escoger uno de ellos como nuestro modelo final. También podemos ver como el modelo de regresión logística y el modelo de k-nn son los dos "peores" modelos, aunque sean los que peor AUC tienen, siguen siendo modelos decentes que sobre todo se caracterizan por su simplicidad frente al resto de modelos. Siguiendo en la línea de las conclusiones que hemos sacado de la comparativa de las métricas extraidas de la matriz de confusión, el xgboost es el mejor modelo, ya que es el que mejor recall tiene además de tener una de las mejores AUC. 

# Conclusiones y Trabajo Futuro

## Elección del Modelo Final

Como modelo final hemos elegido el modelo xgboost. ¿Por qué este modelo? Como hemos dicho anteriormente, creemos que para un banco es más importante detectar a aquellos clientes que se quieren marchar que detectar a aquellos que se quieren quedar. Esto se debe a que es mejor ofrecerle algo a alquien que crees que se va a marchar aunque finalmente no se fuera a marchar frente a no ofrecerle nada a un cliente porque creamos que no se va a marchar y que finalmente se marche. Traducido a nuestras métricas de la matriz de confusión, es mejor tener falsos positivos que falsos negativos en nuestro caso, por lo que necesitamos un modelo con una tasa de falsos positivos baja y de verdaderos positivos alta. Además, necesitamos una recall alta, ya que debemos ser capaces de predecir los positivos con precisión y, como vimos en la comparativa de modelos, el que mejor se ajusta a estas necesidades es el modelo XGBoost. A pesar de ello, se trata de un modelo complejo y poco explicable, por lo que dependiendo del caso podría no ser el mejor modelo, si fueramos capaces de ajustar otros modelos más sencillos para que sean mejores prediciendo los clientes que se marchan, aunque dichos modelos tuvieran parámetros algo peores, podríamos elegirlos por el principio de parsimonia. 

```{r}
print(cf_xgb_test)
plot(roc_curve_xgboost_test,
         print.auc = TRUE,
         main = "Curva ROC del modelo XGBoost (Test)",
         legacy.axes = TRUE
         )
```
Aqui podemos ver la evaluación del modelo elegido en el conjunto de test, con un AUC de un 87%, lo cual es bastante bueno, una recall de un 75% y una especificidad del 82%. Todos estos parámetros son los que nos hacen elegir este modelo frente a los demás, es el mejor modelo a la hora de predecir el churn.

## Conclusiones y trabajo futuro

En la primera parte de la práctica hicimos un extenso análisis exploratorio de los datos para entenderlos bien con el objetivo de emplear modelos de aprendizaje automático para poder predecir el churn de los clientes de un banco. Además empleamos técnicas de aprendizaje no supervisado como k-means, así como métodos de selección de variables. A pesar de que obtuvimos buenos resultados, estos no fueron todo lo buenos que nos gustaría, por lo que el empleo de modelos supervisados era algo que sabíamos que mejoraría con creces nuestras predicciones. 

En esta segunda parte empleamos múltiples modelos de clasificación con los que obtuvimos resultados diversos. Claramente obtuvimos peores resultados con la regresión logística, con el k-nn y con el modelo de naive bayes, esto se debe a que nuestros datos están muy desbalanceados y estos modelos, al ser mucho más sencillos, no son capaces de lidiar con el desbalance de daros de manera efectiva, aunque hemos sido capaces de predecir muy bien a aquellos que no se marchaban del banco, lo cual es destacable tratándose de modelos sencillos. 

Después aplicamos otros modelos, como los árboles de decisión o los métodos de bagging. Con estos modelos obtuvimos resultados considerablemente mejores, siendo capaces de predecir muy bien y con mucha precisión a aquellos clientes que se iban a quedar en el banco, a pesar de ello, estos modelos no son capaces de predecir con precisión a aquellos clientes que se marchan del banco, por lo que no se ajustan del todo bien a nuestro problema, cuyo objetivo principal como hemos mencionado anteriormente es detectar el mayor número de clientes que se marchan del banco posible.

Por último tenemos los modelos de boosting, con los cuales hemos obtenido los mejores resultados. Adaboost y gradient boosting tuvieron resultados muy similares, prediciendo de nuevo muy bien a aquelos que no se marchan pero fallando más en los que si lo hacen. A pesar de ello, aqui obtuvimos una mejora significativa en la recall, llegando a superar el 50% de los clientes que se marchan de manera correcta.Es posible que con un reajuste de los hiperparámetros seríamos capaces de realizar estas predicciones de clientes que se marchan mejor. El XGBoost, nuestro modelo elegido, es el que predice mejor los clientes que se marchan, sin dejar de predecir bien a aquellos que se quedan en el banco. Es por ello que es el modelo que hemos elegido, como explicamos en el apartado anterior de elección del modelo.

Como trabajo a futuro, se debería trabajar en afinar el ajuste de los hiperparámetros de algunos modelos más sencillos, ya que creemos que se pueden ajustar para que estos modelos predigan mejor los clientes que se van a marchar frente a los que se quedan, logrando el objetivo de nuestra práctica con modelos mucho más sencillos y explicables, algo que es deseable ya que el modelo que hemos escogido es bastante complejo y debemos también intentar utilizar modelos más sencillos. También, en base a esto mismo, se podría trabajar en ampliar el dataset con más datos de clientes que se han marchado, ya que el desbalance de nuestro dataset es la principal razón para que estas predicciones no sean tan buenas con modelos sencillos y haga falta recurrir a modelos más complejos. 

