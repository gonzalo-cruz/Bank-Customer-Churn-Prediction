---
title: "Bank Customer Churn"
author: "Grupo 6: Gonzalo Cruz Gómez, Samuel Martínez Lorente, Jorge Tordesillas García"
output: 
  html_document:
    theme: flatly
    toc: yes
    toc_float:
      collapsed: true
---

## Indice:

-   Comprensión del problema. Explicación. Lectura de datos. Particiones (Gonzalo)

-   Preparación de datos y EDA (Gonzalo)

-   Técnicas de reducción de la dimensionalidad (Samuel)

    -   PCA con prcomp

    -   PCA manual

-   Aprendizaje no supervisado

    -   Matriz de distancias (Gonzalo y Jorge)

    -   Clustering no jerárquico (Gonzalo y Jorge)

    -   Clustering jerárquico (Gonzalo)

-   Conclusiones (Gonzalo, Samuel, Jorge)

## Comprensión del problema. Explicación. Lectura de datos. Particiones

Objetivo:

El objetivo principal de este conjunto de datos es predecir la fuga de clientes (churn). En otras palabras, queremos construir un modelo que pueda determinar qué clientes tienen más probabilidades de dejar el banco en el futuro. Esto es importante para los bancos, ya que retener a los clientes existentes es normalemente más rentable que conseguir clientes nuevos.

Descripción de Datos (Variables):

```         
CustomerId: Identificador único para cada cliente. (Tipo: Numérico)
Credit_score: Puntaje de crédito del cliente. (Tipo: Numérico)
Country: País donde reside el cliente (por ejemplo, Francia, España, Alemania). (Tipo: Categórico)
Gender: Género del cliente (Masculino o Femenino). (Tipo: Categórico)
Age: Edad del cliente. (Tipo: Numérico)
Tenure: Número de años que el cliente ha sido cliente del banco. (Tipo: Numérico)
Balance: Saldo en la cuenta bancaria del cliente. (Tipo: Numérico)
Products_number: Número de productos bancarios que el cliente utiliza. (Tipo: Numérico)
Credit_card: Indica si el cliente tiene tarjeta de crédito (1 = Sí, 0 = No). (Tipo: Binario)
Active_member: Indica si el cliente es un miembro activo (1 = Sí, 0 = No). (Tipo: Binario)
EstimatedSalary: Salario estimado del cliente. (Tipo: Numérico)
Churn: Variable objetivo. Indica si el cliente dejó el banco (1 = Sí, 0 = No). (Tipo: Binario)
```

Tipo de Problema:

Este es un problema de clasificación binaria. El objetivo es clasificar a los clientes en dos categorías: aquellos que se irán (1) y aquellos que se quedarán (0).

### Business understanding

Planteamos preguntas sobre nuestros datos:

-   ¿Influye el salario en si el cliente deja el banco?

-   ¿Influye el tiempo que lleva el cliente en el banco en si este deja este banco?

-   ¿Influye la edad?

-   ¿Influye el género?

-   ¿Influye el país?

### Data understanding y división de los datos

Importamos las librerías

```{r librerias}
library(ggplot2)
library(readr)
library(tidyr)
library(dplyr)
library(gridExtra)
library(GGally)
library(factoextra)
library(cluster)
library("NbClust")
library(parameters)
library(tidyverse)
library(corrplot)
```

Leemos los datos y vemos sus dimensiones

```{r lectura de datos}
Data <- read_csv("Bank Customer Churn Prediction.csv")
ntotal <- dim(Data)[1]
ptotal <- dim(Data)[2]
```

```{r visualizar primeros datos de la tabla}
head(Data)
```

Podemos ver que tenemos n = 10000 observaciones y 12 variables en el dataset

Dividimos los datos en train-test-validate

```{r division train-test-val}
set.seed(123)

# creamos los indices
indices <- 1:ntotal
ntrain <- ntotal *.6
ntest <- ntotal* .2
nval <- ntotal-(ntrain+ntest)
indices.train <- sample(indices, ntrain, replace= FALSE)
indices.test <- sample(indices[-indices.train], ntest, replace= FALSE)
indices.val <- indices[-c(indices.train, indices.test)]

# 60% para train, 20% para test y 20% para validate

train <- Data[indices.train,]
test <- Data[indices.test,]
validate <- Data[indices.val,]
```

Veamos las variables:

```{r visualizar variables}
str(train)
```

Podemos observar que la mayoria de nuestras variables son numéricas, a excepción de aquellas que son char como country y gender, que son variables categóricas. En el caso de credit card, active_member, products number, tenure y churn se trata de variables discretas.

Antes de nada vamos a transformar nuestras variables categóricas de char a factor.

```{r}
train$country <- as.factor(train$country)
train$gender <- as.factor(train$gender)
```

## Exploratory Data Analysis

Veamos un resumen de algunos de los estadísticos principales de cada columna

```{r estadísticos resumen}
summary(train)
```

Comprobamos si hay valores faltantes:

```{r valores faltantes}
colSums(is.na(Data))
```

Vemos que no tenemos valores faltantes en ninguna columna.

Visualizamos nuestros datos para ver cómo son y cómo están distribuidos

```{r visualizacion variables continuas}

train_long <- train %>%
  dplyr::select(estimated_salary, balance, credit_score) %>%  
  tidyr::gather(key = "Variable", value = "Value")

ggplot(train_long, aes(x = Value, fill = Variable)) +
  geom_histogram(bins = 20, color = "black", alpha = 0.5, aes(y =..density..)) + 
  geom_density(aes(y =..density.., color = Variable), linewidth = 0.5, alpha = 0.2) + 
  facet_wrap(~ Variable, scales = "free") +
  labs(title = "Continuous variables", x = "Value", y = "Density") +
  scale_fill_manual(values = c("skyblue", "lightgreen", "lightcoral")) +
  scale_color_manual(values = c("darkblue", "darkgreen", "darkred")) + 
  theme_bw() +
  theme(legend.position = "none")
```

Nuestras variables balance y credit score siguen distribuciones bastante similares a la normal (exceptuando el gran número de ceros en balance), mientras que el salario estimado sigue una distribución más uniforme.

```{r visualizacion variables discretas}

  train_long <- train %>%
    tidyr::pivot_longer(cols = c(products_number, active_member, churn),
                        names_to = "variable", values_to = "value")

  combined_plot <- ggplot(train_long, aes(x = value)) +
    geom_bar(fill = "lightblue", color = "black") +
    facet_wrap(~ variable, scales = "free_x") +
    labs(title = "Discrete variables",
         x = "Value",
         y = "Count") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  
  print(combined_plot)
  
```

Podemos ver como hay más gente que se queda en el banco respecto a la que se va, el numero de miembros activos y pasados es basicamente igual y que el número de productos más habitual es o 0 o 1, con muy pocos clientes teniendo más de 2.

```{r visualizacion tenure}
plot_tenure_hist <- ggplot(train, aes(x = tenure)) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +  
  labs(title = "Distribution of Tenure",
       x = "Tenure",
       y = "Count") +
  theme_bw()

print(plot_tenure_hist)
```

Tenemos una distribución bastante uniforme en el tiempo que está cada cliente en el banco.

Ahora miramos las variables categóricas

```{r}
# Gráfico de barras para 'Gender'
ggplot(train, aes(x = gender)) +
  geom_bar(fill = c("skyblue", "salmon")) +
  labs(title = "Distribución de Género",
       x = "Género",
       y = "Número de Clientes") +
  theme_minimal()

# Gráfico de barras para 'Geography'
ggplot(train, aes(x = country)) +
  geom_bar(fill = c("lightgreen", "lightcoral", "lightblue")) +
  labs(title = "Distribución por Países",
       x = "País",
       y = "Número de Clientes") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotar etiquetas

```

Podemos ver que hay más clientes hombres que mujeres, aunque la diferencia no es grande, y también podemos observar que la mayor parte de los clientes del banco son franceses, con un número similar de clientes de este país a la combinación de España y Alemania.

Vamos a buscar si hay algún tipo de relación entre nuestras variables, para ello vamos a utilizar scatter plots:

```{r}
# Estimated salary vs Balance
ggplot(train, aes(x = estimated_salary, y = balance)) +
  geom_point() +
  labs(x = "Estimated Salary", y = "Balance")

# Balance vs Credit Score
ggplot(train, aes(x = credit_score, y = balance)) +
  geom_point() +
  labs(x = "Credit Score", y = "Balance")

# Credit Score vs Estimated Salary
ggplot(train, aes(x = estimated_salary, y = credit_score)) +
  geom_point() +
  labs(x = "Estimated Salary", y = "Credit Score")


```

Podemos ver como siguen distribuciones bastante normales, con la excepción de los datos con valor 0 del balance, que como hay muchos, tienen mucho peso.

Hacemos una matriz de correlación para ver como están relacionadas nuestras variables entre ellas:

```{r}
train_filtered <- train %>% select(-country, -gender)
correlation_matrix <- cor(train_filtered)
corrplot(correlation_matrix, method = "color", type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45) 
```

Podemos ver como el balance y el número de productos están bastante relacionados, de manera inversa, cuanto menos dinero, más productos, además la edad está bastante relacionada con que un cliente abandone el banco.El resto de variables no tienen demasiada relación entre ellas.

Vamos a ver qué nos dice nuestra variable objetivo

```{r visualización churn}
ggplot(data = train, aes(x = churn, fill = factor(churn))) + 
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "none") +
  ylab("Relative Frequency") +
  xlab("Churn") +
  theme_minimal() +  
  scale_fill_manual(values = c("skyblue", "tomato"), 
                    labels = c("No", "Yes")) + 
  labs(title = "Churn Distribution")
```

Podemos ver como la mayoría de los clientes no se van del banco pero hay un porcentaje significativo que si lo hace

Ahora veamos como estan distribuidos por género y país:

```{r churn vs genero}
train$churn <- factor(train$churn, levels = c(0, 1), labels = c("No", "Yes"))

train %>%
  count(gender, churn) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ggplot(aes(x = gender, y = Percentage, fill = churn)) +
  geom_col(position = "dodge") +
  labs(title = "Churn by Gender", x = "Gender", y = "Percentage") +
  theme_bw() +
  scale_fill_manual(values = c("skyblue", "tomato"), name = "Churn Status")

```

Aquí se observa que el porcentaje de hombres que no se marchan del banco es mayor que el de las mujeres

```{r}
train %>%
  count(country, churn) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ggplot(aes(x = country, y = Percentage, fill = churn)) +
  geom_col(position = "dodge") +
  labs(title = "Churn by country", x = "country", y = "Percentage") +
  theme_bw() +
  scale_fill_manual(values = c("skyblue", "tomato"), name = "Churn Status")
```

Vemos como los clientes franceses y los españoles son menos propensos que los alemanes a marcharse del banco

Vamos a intentar contestar las preguntas que nos hicimos al inicio:

- ¿Influye el salario en si el cliente deja el banco?

```{r churn vs salario}
ggplot(train, aes(x = factor(churn), y = estimated_salary, fill = factor(churn))) +
  geom_boxplot() +
  labs(title = "Salary vs. Churn", x = "Churn", y = "Estimated Salary") +
  scale_x_discrete(labels = c("No", "Yes")) +
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) 

```

Podemos ver como, a priori, el salario anual no influye en la decisión de abandonar el banco. Vamos a comprobarlo con un contraste de hipótesis.

Primero vemos si sigue una distribución normal para saber qué test aplicar después.. Para ello utilizamos el test de shapiro:

```{r distribucion salary}
train$churn <- as.factor(train$churn)
alpha <- 0.05
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(estimated_salary)$p.value)

```

Como no sigue una distribución normal debemos utilizar un test no paramétrico coomo el de wilcoxon para comprobar si existe una diferencia significativa.

```{r contraste churn-salario}

wilcox_result <- wilcox.test(estimated_salary ~ churn, data = train)
print(wilcox_result)
alpha <-  0.05
p_value <- wilcox_result$p.value

```

Como ya podíamos intuir por la gráfica no hay una diferencia significativa respecto al salario

\- ¿Influye el tiempo que lleva el cliente en el banco en si este deja el banco?

```{r churn vs tenure}
ggplot(train, aes(x = factor(churn), y = tenure, fill = factor(churn))) + 
  geom_boxplot() +
  labs(title = "Tenure vs. Churn", x = "Churn", y = "Tenure (years)") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))  
```

Vamos a ver si hay una diferencia significativa entre los dos grupos. Para ello vamos a plantear un contraste de hipótesis:

Primero comprobamos si tenure sigue una distribución normal

```{r distribucion tenure}
train$churn <- as.factor(train$churn)
alpha <- 0.05
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(tenure)$p.value)

```

Sigue una distribución normal. Utilizamos un test t para comprobar si existe alguna diferencia significativa.

```{r contraste tenure-churn}
t_test_result <- t.test(tenure ~ churn, data = train)
print(t_test_result)

p_value <- t_test_result$p.value


```

Por tanto podemos decir que el hecho de abandonar o no el banco sí depende del tiempo que llevan los clientes en el mismo.

\- ¿Influye la edad?

```{r age vs churn}
ggplot(train, aes(x = factor(churn), y = age, fill = factor(churn))) + 
  geom_boxplot() +
  labs(title = "Age vs. Churn", x = "Churn", y = "Age (years)") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
```

Vamos a comprobar si es normal para ver qué test utilizamos:

```{r distribucion age}
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(age)$p.value)
```

Vemos que age no sigue una distribución normal, por lo que aplico un test de wilcoxon

```{r }
wilcox_result <- wilcox.test(age ~ churn, data = train)
print(wilcox_result)
p_value <- wilcox_result$p.value
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa en el tiempo que son clientes entre aquellos que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```
```{r contraste age-churn}
wilcox_result <- wilcox.test(age ~ churn, data = train)
print(wilcox_result)
p_value <- wilcox_result$p.value
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa con la edad de los clientes y los que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```

Podemos ver como la edad importa a la hora de que un cliente abandone el banco o no, usualmente siendo los clientes mayores los que tienden a abandonar el banco

-   ¿Influye el balance?

```{r balance vs churn}
ggplot(train, aes(x = factor(churn), y = balance, fill = factor(churn))) + 
  geom_boxplot() +
  labs(title = "Balance vs. Churn", x = "Churn", y = "Balance") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
```

Vamos a comprobar si es normal para ver qué test utilizamos:

```{r distribucion balance}
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(age)$p.value)
if(p_value<alpha){
  print("No sigue una distribución normal")
}else{
  print("sigue una distribución normal")
}
```

```{r contraste balance-churn}
wilcox_result <- wilcox.test(balance ~ churn, data = train)
print(wilcox_result)
p_value <- wilcox_result$p.value
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa en el balance de los clientes entre aquellos que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```

Se observa que cuanto mayor es el balance, mayor es la probabilidad de que el cliente se cambie de banco

## Técnicas de reducción a la dimensión (PCA).

### PCA con prcomp

Con el siguiente código veremos cómo afecta el uso del PCA a nuestros datos. Para utilizar PCA únicamente utilizamos los datos continuos de nuestro dataset

```{r}
set.seed(1234)

data_cont <- train[, c("credit_score", "age", "tenure", "balance", "products_number", "estimated_salary")]
data_cont <- na.omit(data_cont)

pca <- prcomp(data_cont, center = TRUE, scale. = TRUE)

# Guardar nuevas varialbes
data_pca <- as.data.frame(pca$x)
colnames(data_pca) <- paste0("PC", 1:ncol(data_pca))  # Renombrar dinámicamente

# Gráfico de datos originales (matriz de dispersión)
p1 <- ggpairs(data_cont) + 
  ggtitle("Datos originales (variables continuas)")

# Gráfico de componentes principales
p2 <- ggplot(data_pca, aes(x = PC1, y = PC2)) +
  geom_point(color = "red", alpha = 0.6) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_fixed() +
  ggtitle("Datos transformados (PCA)")

# Mostrar gráficos lado a lado
# Mostrar gráficos de forma separada
print(p1)  # Matriz de dispersión creada con ggpairs
print(p2)  # Gráfico de componentes principales
```

Ahora los resultados de aplicar el PCA usando el summary para ver el resumen:

```{r}
# Resumen del PCA 
pca_summary <- summary(pca)
print(pca$sdev)
print(pca$rotation)
print(pca$center)
print(pca$scale)
print(pca_summary)
```

Podemos ver que nuestras componentes principales muestran lo siguiente:

-   PC1 es la componente principal más importante, nos muestra un 21% de la información del dataset, explica la mayor parte de la varianza y está relacionada principalmente con el credit score y balance de forma negativa y con el número de productos de forma positiva. Esto quiere decir que los clientes con un mayor número de productos van a ir por un lado, mientras que los que tengan mayor credit score y balance ìrán por el otro.

-   PC2 es la siguiente más importante, nos muestra un 17% de la información y está asociada con la edad de forma negativa y con el salario estimado de forma positiva, por lo que muestra una relación entre la edad y el salario estimado. Esto quiere decir que los clientes mayores irían por un lado, mientras que los que tienen salarios altos irían por el otro.

```{r}
prcomp(Data$credit_score)
```

### PCA manual

Sin usar ninguna función de R para reducir la dimensionalidad lo haremos de la siguiente manera: 

1er paso: Realizaremos la normal de nuestro conjunto de datos, los escalamos y los centramos:

```{r}
set.seed(1234)

# tomamos los datos continuos
data_cont <- train[, c("credit_score", "age", "tenure", "balance", "products_number", "estimated_salary")]
data_cont <- na.omit(data_cont)

X_data <- as.matrix(data_cont)

# Calculamos las medias de cada columna.
medias_data <- colMeans(X_data)

# Calculamos la desviación típica de cada columna.
desv_data <- apply(X_data,2,sd)

# Una vez tenemos los datos podemos normalizar nuestra variable.
# Primero restamos la media y después dividimos por la norma.
X_data <- sweep(X_data, 2, medias_data, "-")
X_norm <- sweep(X_data, 2, desv_data, "/")
```

2º paso: Calculamos la matriz de covarianza.

```{r}
C <- cov(X_norm)
print(C)
```

3er paso: Calculamos los autovalores y autovectores, usamos "eigen"

```{r}
# Descomponemos
eig <- eigen(C, TRUE, only.values = FALSE, EISPACK = FALSE)

# Accedemos a los autovalores y autovectores
autovalores <- eig$values
autovectores <- eig$vectors
print(autovectores)
print(autovalores)
```

Podemos ver como el resultado es el mismo que aplicando la función prcomp

## Aprendizaje no supervisado

Vamos a ver cómo se agrupan los clientes en base a una serie de parámetros, para ello vamos a utilizar clustering, tanto jerárquico como no jerárquico.

### Matriz de distancias

En primer lugar vamos a calcular la distancia Euclídea entre las observaciones de la base de datos. También mostraremos la matriz de distancias.

```{r}
# Hacemos un escalado de las variables continuas
datos_escalados <- scale(data_cont)
```

```{r}
head(datos_escalados)
```

Matriz de distancias

```{r}
distance <- get_dist(datos_escalados, method = "euclidean")
print(as.matrix(distance)[1:5, 1:5])

# para visualizarlo, vamos a utilizar una muestra para optimizar el rendimiento
tamaño_muestra <- floor(0.1 * nrow(datos_escalados))
sampled_data <- datos_escalados[sample(seq_len(nrow(datos_escalados)), size = tamaño_muestra), ]

distance_matrix_sampled <- get_dist(sampled_data, method = "euclidean")

fviz_dist(distance_matrix_sampled) 

```

A partir de esto, podemos observar como se cumplen las condiciones para ser una métrica o medida de desemejanza:

-   Coincidencia: podemos ver que cuando x=y el valor es 0

-   No negatividad: el valor más bajo de nuestra matriz de distancias es 0

-   Simetría: con el gráfico podemos ver como la matriz es simétrica respecto al eje formado por x = y

### Clustering no jerárquico

Para empezar, podemos aplicar el algoritmo de las k-medias con k = 2, luego iremos ajustando el número de clusters para llegar al óptimo

```{r}
k_medias <- kmeans(datos_escalados, centers = 2, nstart = 25)
str(k_medias)
```

Si imprimimos los resultados vemos que la técnica de agrupaciones dio lugar a 2 conglomerados o medias para los dos grupos en las variables que haya. También obtenemos la asignación de conglomerados para cada observación.

```{r}
set.seed(595)
k_medias
```

Visualización de los clusters

```{r}
fviz_cluster(k_medias, data = datos_escalados, geom = "point")
```

Vamos a realizar los 3 métodos más populares para determinar el número óptimo de clústeres:

MÉTODO DEL CODO

```{r}
set.seed(123)

fviz_nbclust(datos_escalados, kmeans, method = "wss")
```

En este análisis, a partir de 4 clusters, la reducción en la suma total de cuadrados internos parece estabilizarse, indicando que k = 4 es una buena opción.

```{r}
set.seed(595)
k4 <- kmeans(datos_escalados, centers = 4, nstart = 25)

fviz_cluster(k4, data = datos_escalados, geom = "point")
k4

```

Del plot de los clusters podemos ver como este numero de clusters no es ideal, ya que se mezclan demasiado entre ellos, sin haber una diferenciación clara entre clusters.

```{r}
set.seed(595)
table(k_medias$cluster, k4$cluster)

```

MÉTODO DE LA SILUETA

```{r}

fviz_nbclust(datos_escalados, kmeans, method = "silhouette")

```

Con esta técnica, en nuestros datos, parece que la mejor elección es k igual a 2.

```{r}
sil <-  silhouette(k_medias$cluster, dist(datos_escalados))

fviz_silhouette(sil)
```

La interpretación del coeficiente de la silueta la entendemos como que un valor positivo significa que la observación está bien agrupada. Cuanto más se acerque al coeficiente a 1, mejor agrupada estará la observacíon. En cambio, un valor negativo significa que la obervación está mal agrupada. Finalmente, un valor igual a 0 significa que la observación se encuentra entre dos conglomerados.

El gráfico anterior y el coeficiente de silueta medio ayudan a determinar si la agrupación es buena o no.

MÉTODO GAP

El proceso es el siguiente: se aplica el algoritmo de clustering a los datos con diferentes valores de k, se generan conjuntos de datos de referencia aleatorios, se calcula el estadístico Gap para cada valor de k y la selección del número óptimo de clusters.

```{r}
set.seed(123)
# como el dataset es muy grande, vamos a utilizar una muestra de nuestro dataset
gap <-  clusGap(sampled_data, FUN = kmeans, K.max = 10, nstart = 25, B = 50)

print(gap, method = "firstmax")
```

```{r}
fviz_gap_stat(gap)
```

Parece ser que el número óptimo de clusters según este método es 1, algo que no tiene demasiado sentido, ya que nos interesa clasificar los datos en distintos grupos, no solo en 1.

```{r}
k1 <-  kmeans(datos_escalados, centers = 1, iter.max = 10, nstart = 25)

fviz_cluster(k1, data = datos_escalados, geom = "point")
```

Esquema de agrupación a partir de los diferentes resultados obtenidos variando todas las combinaciones de clústeres, medias de distancia y métodos de agrupación

```{r}
# de nuevo para facilitar el procesamiento utilizamos una muestra del dataset
 nb <-  NbClust(sampled_data, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans")
```

Las dos primeras gráficas me indican que, a más clusters, mejor es la estadística de Hubert y mejor es el ajuste de los clusters, además nos dice que el mejor número de clusters respecto a dicha estadística es 3 y que a partir de 4 ya deja de tener sentido hacer más clusters. A pesar de esto, como hemos podido ver con los 4 clusters, perdemos mucha explicabilidad, ya que se empiezan a mezclar los clusters, por lo que, aunque respecto a la estadística de Hubert es lo más apropiado, puede no ser el número óptimo de clusters. Las siguientes dos gráficas nos dicen lo siguiente:

-   El primero nos dice que según la métrica de la silueta, a más clusters, peor, como hemos podido observar anteriormente.

-   La última nos dice que el mejor número de clusters en este sentido es 2 y que a partir de ahí, el clustering empieza a ser peor.

```{r}
n_clust <-  n_clusters(as.data.frame(sampled_data), 
                       package = c("easystarts", "NbClust", "mclust"),
                       standardize = FALSE)
n_clust
```

```{r}
plot(n_clust)
```

Esta función, que compara el número de clusters óptimo obtenido mediante varios métodos, nos dice claramente que el mejor número es 3.

```{r}
set.seed(100)
k3 <-  kmeans(datos_escalados, centers = 3, iter.max = 10, nstart = 25)

fviz_cluster(k3, data = datos_escalados, geom = "point")
```

Podemos ver como hay tres conjuntos relativamente bien diferenciados. Hay solapamiento ya que nuestras dos primeras componentes principales no muestran demasiada información del dataset. Tendríamos que representarlo en más dimensiones para poder verlo mejor

```{r}
data_cont %>%
  mutate(clusters = k3$cluster) %>%
  group_by(clusters) %>%
  summarise_all("mean")
```

Podemos interpretar esta tabla de diferenciación de las medias de la siguiente manera:

-   El primer cluster es de la gente que lleva menos tiempo en el banco, pero que tiene mucho dinero metido en el y tienen el mejor credit score

-   El segundo es gente que lleva más tiempo que los anteriores en el banco, ganan más o menos lo mismo que los anteriores, pero su balance es mucho menor. También son los que más productos tienen del banco. Su credit score es intermedio

-   Por último, el tercer cluster se compone de la gente que más tiempo lleva en el banco, más dinero tiene y más gana, pero con el menor credit score.

```{r}
res_kmeans <-  cluster_analysis(datos_escalados, 
                                n = 3,
                                method = "kmeans")

plot(summary(res_kmeans))
```

Este análisis del cluster confirma nuestras conclusiones extraídas de la tabla de diferenciación.

A continuación vamos a probar con k-medioides en vez de con k-medias.

Primeramente utilizamos el metodo pam, con distancia manhattan en vez de la distancia euclídea

```{r}
set.seed(100)
pam_cluster <- pam(x = datos_escalados, k = 3, metric = "manhattan")
pam_cluster

fviz_cluster(object = pam_cluster, data = datos_escalados, ellipse = TRUE, 
             repel = TRUE) +
  theme_bw() +
  labs(title = "Resultados clustering PAM") + theme(legend.position = "none")
```

Podemos ver que no es un método idóneo, ya que hay mucho solapamiento entre clusters. Vamos a probar con el método de los clara clusters

```{r}
set.seed(100)
clara_clusters <- clara(x = datos_escalados, k = 3, metric = "manhattan", stand = TRUE,
                        samples = 50, pamLike = TRUE)
clara_clusters
```

Visualizamos:

```{r}
set.seed(100)
fviz_cluster(object = clara_clusters, ellipse.type = "t", geom = "point",
             pointsize = 2.5) +
  theme_bw() +
  labs(title = "Resultados clustering CLARA") +
  theme(legend.position = "none")
```

Podemos ver como es bastante más parecido al resultado obtenido con k-means, aunque aquí se puede ver como solamente toma una muestra del dataset completo.

### Clustering jerárquico

Después de haber hecho clustering no jerárquico con k-medias y k-medioides, obteniendo resultados similares, vamos a aplicar métodos de clustering jerárquico.

Vamos a empezar por el clustering jerárquico algomerativo, para ello vamos a evaluar todos los métodos para ver cual es el que mejor coeficiente de agrupamiento nos da.

```{r}
set.seed(100)
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# Calculamos el coeficiente de agrupamiento de todos los métodos
ac <- function(x) {
  agnes(sampled_data, method = x)$ac
}

map_dbl(m, ac)
```

Podemos ver como el método Ward nos dá el mayor coeficiente de agrupamiento. Dado el gran número de observaciones que tenemos, elaborar un dendograma no tiene ningún sentido, ya que no solo sería ilegible, sino que también su coste computacional sería muy alto.

```{r}
hc1 <- hclust(distance, method = "ward.D2")
hc1
```

Hacemos como con el clústering no jerárquico, primeramente visualizamos 2 clusters y luego veremos el número idóneo de clusters con diferentes métodos.

```{r}
set.seed(100)
clusters <- cutree(hc1, k = 2)
pca_df <- as.data.frame(pca$x[, 1:2]) 

pca_df$cluster <- as.factor(clusters)

# Hacemos un scatter plot para visualizar los clusters. 
ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster)) + 
  geom_point() +
  labs(title = "Scatter Plot of Clusters", x = "PC1", y = "PC2")

```

Vamos a hacer el clustering jerárquico divisivo para ver el número de clusters

```{r}
hc2 <- diana(sampled_data)
hc2$dc
```

Vamos a visualizar un dendrograma, aunque no nos será muy útil:

```{r}
pltree(hc2, cex = 0.6, hang = -1, main = "Dendrogram de DIANA")
```

Como ya habíamos dicho antes, hacer un dendograma no tiene mucho sentido ya que, al tener tantas observaciones, no se ve demasiado, lo que si podemos ver es que si cortamos a una altura de 6 nos quedan 4 clusters medianamente bien diferenciados. Vamos a aplicar mejor los métodos que aplicamos en clustering no jerárquico para ver el número óptimo de clusters.

Primeramente el método del codo:

```{r}
fviz_nbclust(datos_escalados, FUN = hcut, method = "wss")
```

Podemos ver como aquí el número óptimo de clusters parece ser 4

Vamos a aplicar el método gap, para ello vamos a utilizar una muestra del dataset:

```{r}
set.seed(100)
gap_stat <- clusGap(sampled_data, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

Como nos pasaba en el clustering no jerárquico, también nos dice que el número óptimo es 1 cluster, algo que no nos sirve, vamos a ver otros métodos como el NbClust:

```{r}
 nb2 <-  NbClust(sampled_data, distance = "euclidean", min.nc = 2, max.nc = 10, method = "ward.D2")
```

Según este método, nuestro número ideal de clusters es 2 o 3, como podemos ver en las gráficas.

A continuación vamos a utilizar n_clusters con NbClust únicamente, ya que es el paquete que más se centra en clustering jerárquico

```{r}
n_clust2 <-  n_clusters(as.data.frame(sampled_data), 
                       package = c("NbClust"),
                       standardize = FALSE)
n_clust2
plot(n_clust2)
```

Podemos ver como nos indica que el número idóneo de clusters es 3.

Por ello, vamos a cortar en 3 clusters nuestros datos

```{r}
set.seed(100)
hc3 <- hclust(distance, method = "ward.D2" )

sub_grp <- cutree(hc3, k = 3)
table(sub_grp)
```

De aqui podemos extraer que el segundo cluster contiene bastantes más puntos que los otros dos.

Vamos a visualizar nuestros clusters:

```{r}
set.seed(100)
fviz_cluster(list(data=datos_escalados,cluster=sub_grp), geom = "point")
```

A pesar de ser 3 clusters igual que en el clustering no jerárquico, el resultado es diferente al que obtuvimos en este, vamos a ver qué nos dicen estos clusters.

```{r}
cluster_means <- datos_escalados %>%
  as.data.frame() %>%  
  mutate(clusters = sub_grp) %>%
  group_by(clusters) %>%
  summarise_all(mean)

print(cluster_means)

```

Podemos ver como ahora los clusters se dividen de la siguiente manera:

-   El primer cluster está compuesto por la gente que tiene un credit score ligeramente más alto, son un poco más jovenes que la media, tienen muchos productos y bastante dinero en el banco.

-   El segundo contiene a gente algo más mayor, con un credit score promedio, un balance alto en el banco, pero utilizan menos productos.

-   Por último, el tercero está compuesto por aquellos que son ligeramente más jóvenes que la media, tienen un credit score más bajo, un balance considerablemente más bajo y utilizan menos productos del banco.

## Conclusiones

Primeramente hemos podido ver que nuestro dataset está muy completo, sin datos faltantes ni outliers que nos puedan dificultar nuestro análisis de los mismos.
Al observar las distribuciones de nuestro dataset, hemos visto que nuestras variables contínuas siguen en su mayoría distriones cercanas a la normal, además de tener bastantes variables binarias que nos pueden dar mucha información a futuro cuando apliquemos algoritmos de clasificación binaria para preedecir si un cliente va o no a abandonar el banco.

Podemos observar que la tasa de personas que se mantienen en el banco es mayor que la de la gente que se va, también vemos las relaciones de cada variable con el abandono del banco y podemos ver como algunas de las variables más relacionadas con el abandono son la edad o el balance, que muestra el dinero restante de cada cliente en el banco.

Podemos ver  hay cerca de un 80% de los clientes del banco que no lo abandonan y poco más del 20% que sí, de esa tasa de abandono cercana al 20% son más mujeres que hombres las que abandonan el banco, en cambio si filtramos por países los alemanes son más propensos a irse del banco.

La relación del dinero con el banco es interesante, no solo el dinero que tienen en el banco si no el salario que estimamos que tiene cada cliente con su permanencia en el banco, podemos ver que realmente el salario estimado no tiene relacion alguna ni con el abandono ni con la permanencia de los cliente, mientras que el balance sí que importa para que una persona se quede en el banco o no, ya que a mayor balance, podemos observar una mayor tendencia a abandonar el banco.
Respecto a la edad y el tiempo que llevan en el banco vemos que también influyen, a mayor edad la tasa de abandono es mayor.

En cuanto al PCA, nuestras dos primeras componentes principales nos muestran solamente un 40% aproximado de la información, por lo todos nuestros clusters, independientemente del método que utilicemos, se van a ver solapados, ya que no tienen mucha cantidad de información del dataset. Si visualizáramos en más dimensiones quedaría más clara la separación de los clusters. 

También en relación con el PCA, estamos utilizando solamente las variables continuas, por lo que como  nuestro dataset tiene bastantes variables binarias, asi como dos variables categóricas, estamos perdiendo información que podría ser valiosa a la hora de clasificar nuestros datos en diferentes clusters. Para ello deberíamos utlizar otros métodos como el escalado multidimensional o algún otro tipo de selección de variables, algo que sería interesante implementar de cara al aprendizaje supervisado, para poder predecir con toda la información de nuestro dataset si una persona se irá o no del banco.

También para reducir la dimensionalidad podríamos utilizar los mapas auto-organizados, que utilizando clustering nos podrían ayudar a hacer una selección de variables aún más precisa.

Asimismo, nuestra matriz de distancias está computada con las distancias euclídeas entre nuestros datos contínuos, ganaríamos más precisión si creáramos nosotros una matriz de distancias teniendo en cuenta estas variables binarias también, además de las variables categóricas. 

En cuanto a los distintos métodos de clustering, tanto para clustering jerárquico como no jerárquico, vemos que el número de clusters es igual, a pesar de eso, los clusters que obtenemos mediante los métodos jerárquicos y no jerárquicos. Con k-means y k-medioides con el método clara tenemos un resultado bastante similar y con los métodos jerárquicos y el método PAM tenemos resultados también similares, pero diferentes a los anteriores. Estos resultados se diferencian entre si por el criterio que utilizan a la hora de separar los valores. Mientras que el clustering jerárquico separa los puntos por edad, credit score, balance y productos, siendo bastante importantes a la hora de diferenciar los conjuntos el número de productos, el balance y el credit score; el clustering no jerárquico da mayor importancia a al tiempo que llevan los clientes en el banco (tenure), el balance de las cuentas y el credit score. Esto se asemeja a las primeras conclusiones que pudimos extraer del dataset al hacer el EDA, donde pudimos ver que el balance, la edad y el tiempo que llevan los clientes en el banco son factores importantes y diferenciales. 

Estos grupos pueden servir después para ver qué grupo es más probable que abandone el banco en algún momento en base al grupo al que pertenezca, esto lo podremos hacer con los métodos de aprendizaje supervisado que veremos más adelante en la asignatura, con los que podremos predecir esto

# Práctica 2

## Métodos de ensamblado.

Vamos a incluir ahora os métodos de ensamblado que se basa en la unión
de múltiples modelos que pueden mejorar nuestras predicciones, la idea
de estos modelos denominados métodos de ensamblado es encadenarlos para
mejorar dichas predicciones.

Haremos el ensamblado de bagging al principio, que consiste en una
técnica de ML hecha para mejorar la predicción de los modelos y como
hemos dicho antes se basa en construir múltiples modelos similares y
combinar dichas predicciones para obtener un resultado final más fiable.

### Bagging

**Bagging** funciona de la siguiente manera, empieza con la división del
conjunto de datos inicial en distintos subconjutnos datos y usamos algo
llamado muestreo de reemplazado, esto genera distintos conjuntos de
entrenamientos ligeramente distintos entre ellos, el siguiente paso
sería entrenar el modelo base, y cada modelo es distinto debido a el
distinto muestreo, como tercer paso encontramos que una vez todas los
modelos hayan sido entrenados y se utilizan para hacer predicciones
indivivuales sobre un conjunto de datos de prueba, como paso final
obtenemos que todas las predicciones de los modelos se combinan para
tener una mejor predicción.

El algoritmo más conocido para **Bagging** es el que desarrollaremos
ahora que es denominado **"Random Forest"** que básicamante combina
multiples DTs para lograr un modelo predictivo altamente preciso.

#### Random Forest.

```{r}
library(ggplot2)
library(randomForest)
library(caret)
library(readr)
library(dplyr)

# 1. Cargar y preparar los datos
Data <- read_csv("Bank Customer Churn Prediction.csv")
Data$churn <- as.factor(Data$churn)

# Seleccionar variables relevantes
predictors <- c("credit_score", "age", "tenure", "balance", 
                "products_number", "estimated_salary", "country", "gender")
response <- "churn"

data <- Data %>% 
  select(all_of(c(predictors, response))) %>% 
  na.omit() %>%
  mutate(
    country = as.factor(country),
    gender = as.factor(gender)
  )

# 2. Configuración del experimento
set.seed(123)
n_reps <- 50  # Reducido para mayor velocidad (aumentar para análisis final)
test_size <- 0.2

# Crear conjunto de test
test_idx <- createDataPartition(data$churn, p = test_size, list = FALSE)
test_data <- data[test_idx, ]
train_data_base <- data[-test_idx, ]

# 3. Funciones para realizar las predicciones
run_bagging <- function(train_data) {
  model <- randomForest(churn ~ ., 
                       data = train_data,
                       mtry = length(predictors), # Usa todas las variables
                       ntree = 100)
  return(model)
}

run_rf <- function(train_data) {
  model <- randomForest(churn ~ .,
                       data = train_data,
                       mtry = floor(sqrt(length(predictors))), # Selección típica RF
                       ntree = 100)
  return(model)
}

# 4. Ejecutar las repeticiones
results <- lapply(1:n_reps, function(i) {
  # Muestreo bootstrap
  sample_idx <- sample(1:nrow(train_data_base), nrow(train_data_base), replace = TRUE)
  train_data <- train_data_base[sample_idx, ]
  
  # Entrenar modelos
  bagging_model <- run_bagging(train_data)
  rf_model <- run_rf(train_data)
  
  # Hacer predicciones
  bagging_pred <- predict(bagging_model, test_data, type = "prob")[,2]
  rf_pred <- predict(rf_model, test_data, type = "prob")[,2]
  
  return(data.frame(
    rep = i,
    observation = 1:nrow(test_data),
    bagging_pred = bagging_pred,
    rf_pred = rf_pred,
    actual = test_data$churn
  ))
})

# Combinar todos los resultados
all_results <- bind_rows(results)

# 5. Calcular varianzas
variances <- all_results %>%
  group_by(observation, actual) %>%
  summarise(
    bagging_var = var(bagging_pred),
    rf_var = var(rf_pred),
    .groups = 'drop'
  )

# 6. Visualización
ggplot(variances, aes(x = observation)) +
  geom_line(aes(y = bagging_var, color = "Bagging"), linewidth = 1) +
  geom_line(aes(y = rf_var, color = "Random Forest"), linewidth = 1) +
  facet_wrap(~ actual, labeller = labeller(actual = c("0" = "No Churn", "1" = "Churn"))) +
  labs(title = "Varianza de Predicciones: Bagging vs Random Forest",
       x = "Observaciones en Conjunto de Test",
       y = "Varianza",
       color = "Método") +
  scale_color_manual(values = c("Bagging" = "blue", "Random Forest" = "red")) +
  theme_minimal()

# 7. Métricas de rendimiento
final_pred <- all_results %>%
  group_by(observation) %>%
  summarise(
    bagging_mean = mean(bagging_pred),
    rf_mean = mean(rf_pred),
    actual = first(actual)
  )

# Umbral de decisión
final_pred$bagging_class <- ifelse(final_pred$bagging_mean > 0.5, 1, 0)
final_pred$rf_class <- ifelse(final_pred$rf_mean > 0.5, 1, 0)

# Matrices de confusión
cat("Bagging Performance:\n")
confusionMatrix(factor(final_pred$bagging_class, levels = c(0, 1)), 
                factor(final_pred$actual, levels = c(0, 1)))

cat("\nRandom Forest Performance:\n")
confusionMatrix(factor(final_pred$rf_class, levels = c(0, 1)), 
                factor(final_pred$actual, levels = c(0, 1)))
```

Lo primero a analizar de estos gráficos es la varianza de predicciones.
Tenemos dos gráficos, uno para clientes que permanecen y otro para los
que no. Empecemos por los que si permanecen, podemos ver un patrón en el
cual las predicciones de varianza son bajas, por debajo de 0.01, ambos
modelos muestran una alta estabilidad para los clientes que no
abandonan,o aunque Random Forest tiene una pquela ventaja con varianza
ligeramente menor que Bagging.

Por el contrario para los clientes que abandonan la varianza es
significativamente mayor con puntos de hasta 0.125, por eso podemos
decir que las predicciones son menos consistentes, Bagging muestra picos
de alta varianza lo que lo hace más difícil de clasificar, por otro lado
Random FOrest mantiene mejor control de la varianza sobre todo entre los
rangos de 500-1000 y 1500-2000.

**Hallazgos importantes:**

**Exactitud global:** 85.21% (buen desempeño general)

**Sensibilidad (No Churn):** 96.42% (excelente para identificar
permanencia)

**Especificidad (Churn):** Solo 41.42% (dificultad para detectar
abandonos)

**Prevalencia:** 79.61% de los casos son "No Churn" (dataset
desbalanceado)

**Problemas identificados:**

**Alto número de falsos negativos:** 239 casos de Churn fueron mal
clasificados

**Baja especificidad:** El modelo tiende a predecir "No Churn" incluso
cuando el cliente abandona

**Valor predictivo negativo:** Solo 74.78%, prácticamente tres cuartos
de las predicciones son son correctas lo que es bajo.

En este gráfico comparamos la varianza de Bagging con la de Random
Forest.

Si ponemos en conjunto ambos resultados, podemos ver como para "No
Churn" tenemos: Baja varianza y Alta sensibilidad (96,42%) por lo tanto
el modelo es consistente y preciso para esta clase. El otro lado es para
"Churn" en este caso tenemos: Alta varianza y Baja especificidad
(41,42%) por lo que encontramos mucha inconsistencia en las predicciones
se refleja en la dificultad para identificar abandonos.

Comparando modelos podemos ver que **Random Forest** nos muestra menor
varianza en ambos casos sobre todo para **Churn**, mayor estabilidad en
predicciones difíciles y seguramente mejor generalización gracias a la
selección aleatoria de variables, para el modelo de **Bagging** tenemos
mayor varianza especualmente en los límites pero un posible sobreajuste
debido al uso de todas las variables en cada división

Aplicamos este método de ML a nuestro conjunto de datos

```{r}
rf_full <- randomForest(as.factor(churn) ~ ., 
                        data = train_data_base, 
                        importance = TRUE, 
                        proximity = TRUE)

# Ver el resumen del modelo
print(rf_full)

```

Tenemos un error global de 14.51% (OOB estimate) → 85.49% de precisión

La estructura del modelo está formada por 500 árboles de decisión y 2
variables consideradas en cada división (mtry = 2)

**Análisis por Clases**

**Para clientes que NO abandonan (0):** Tenemos 6135 clientes
correctamente clasificados pero 235 que no lo están eso nos da un 3.69%
de error eso significa que tenemos un gran modelo para los clientes que
se quedan en el banco

**Para clientes que SÍ abandonan (1):** Tenemos 703 clientes
correctamente clasificados pero 926 que no lo están eso nos da un 56.84%
de error eso significa que tenemos un modelo que para los clientes que
se van del banco no los detecta de manera correcta con más de la mitad
de ellos sin ser bien detectados.

```{r}
plot(rf_full)

# Añadir un título
title(main = "Error OOB y por Clase - Random Forest")

# Añadir una leyenda
legend("topright", 
       legend = colnames(rf_full$err.rate), 
       col = 1:ncol(rf_full$err.rate), 
       lty = 1, 
       cex = 0.8)
```

Para poder sacar conclusiones de este gráfico tenemos que ver la curva
de error de **OOB(Negra)** que nos muestra como el error global va
disminuyendo a medida que se añaden añaden árboles, por lo que podría
llegar a estabilizarse.

Vamos a ver los errores por clases **Clase 0(No Churn-Rojo)** el error
es bastante bajo y coincide con la alta precisión de la matriz de
confusión vista antes, por el otro lado **Clase 1(Churn - Verde)** tiene
un error considerablemente alto que disminuye lentamente,

```{r}
# Partición de prueba (ya tienes test_data)
df.test <- test_data %>%
  mutate(fgender = as.factor(gender)) %>%
  dplyr::select(credit_score, age, tenure, balance, 
                products_number, estimated_salary, 
                country, gender = fgender, churn)

# Asegurarnos que churn sea factor
df.test$churn <- as.factor(df.test$churn)

# Predecir probabilidades
prediction.rf <- predict(rf_full, df.test, type = "prob")[,2]

# Convertir probabilidades a clases
clase.pred.rf <- ifelse(prediction.rf > 0.5, "1", "0")

# Matriz de confusión
cf <- confusionMatrix(as.factor(clase.pred.rf), as.factor(df.test$churn), positive = "1")
print(cf)

```

Lo primero que vamos a analizar es la matriz de confusión y nos muestra
el desempeño del modelo Random Forest en los datos de prueba, aparte de
esto nos da más información:

-   **Precisión:** 85.06%
-   **Intervalo de Confianza del 95%:** (83.42%, 86.59%)
-   **Sensibilidad (Recall para Churn):** 41.18%, solo detecta ese
    porcentaje de de casos reales de abandono
-   **Especificidad:** 96.30% para identificar clientes que permanecen.
-   **Precisión:** 74.01%, acierta el 74% de las veces que predice un
    abandono.
-   **KAPPA:** 0.4488, es el valor de la concordancia moderada entre
    predicciones y realidad.

```{r}
importance(rf_full)
```

Para entender esta tabla es fundamental entender que significa
*"MeanDecreaseAccuracy"* y *"MeanDecreaseGini"* y por lo tanto que miden
estas variables y cuáles tienen un mayor impacto

-   **MeanDecreaseAccuracy** mide cuánto disminuye la exactitud del
    modelo al eliminar cada variable, a mayor valor más impacto tiene
    por lo que estimated_salary (109.72) , tenure (92.07) y
    products_number(48.82).

-   **MeanDecreaseGini** mide la contribución a la pureza de los nodos
    (homogeneidad) y sus valores más importantes son tenure (521.71),
    estimated_salary (348.31) y products_number (323.64).

Veamos esto representado en un gráfico.

```{r}
varImpPlot(rf_full)
```

Una vez vemos esto de manera más visual vemos que estas tres variables
vistas antes son más importantes a la hora de predecir el abandono en
los bancos, por otro lado renemos los valores de gender y country que
permanecen en ambos gráficos con valores muy pequeños por lo que podrían
ser variables para considerar para eliminar.

### Naive Bayes

Este algoritmo está basado en el conocido teorema de Bayes, que a pesar
de ser sencillos son muy útiles para problemas con muchas variables
entrada por lo que no puede ser útil, veamos como reaccionan nuestros
datos ante este test

```{r}
library(naivebayes)

# Entrenar modelo Naive Bayes
nb_model <- naive_bayes(as.factor(churn) ~ ., data = train_data_base, usekernel = TRUE)

# Predecir probabilidades sobre el test
probabilities <- predict(nb_model, test_data[,-9], type = "prob")[,2]

# Clasificar según umbral 0.5
classes <- as.numeric(probabilities > 0.5)

# Matriz de confusión
cf_nb <- confusionMatrix(
  factor(classes, levels = c(0, 1)), 
  factor(test_data$churn, levels = c(0, 1)), 
  positive = "1"
)

# Imprimir resultados
print(cf_nb)
```

**Resultados de la matriz de confusión - Naive Bayes**

| Métrica | Valor | Interpretación |
|-------------------|-------------------|----------------------------------|
| **Accuracy** | 83.41% | Precisión global aceptable. |
| **Sensitivity (Recall clase 1)** | 26.47% | Muy baja: detecta pocos clientes que hacen churn. |
| **Specificity** | 97.99% | Muy alta: predice muy bien los que **no** hacen churn. |
| **Pos Predictive Value (PPV)** | 77.14% | De los predichos como churn, el 77% realmente hacen churn. |
| **Neg Predictive Value (NPV)** | 83.88% | De los predichos como no churn, el 84% realmente no lo hacen. |
| **Balanced Accuracy** | 62.23% | Moderada, indica desequilibrio entre clases. |
| **Kappa** | 0.3237 | Aceptable, pero sugiere que el modelo puede mejorar. |
| **McNemar's Test p-value** | \<2.2e-16 | Hay diferencia significativa entre errores de clasificación. |

## Naive Bayes sin función.

```{r}

# 1. Entrenar: prior y likelihoods con Laplace smoothing
prior <- prop.table(table(train_data_base$churn))

likelihoods <- list()
for (var in names(train_data_base)[names(train_data_base) != "churn"]) {
  tab <- table(train_data_base[[var]], train_data_base$churn)
  tab <- tab + 1 # Laplace smoothing
  likelihoods[[var]] <- prop.table(tab, margin = 2)
}

# 2. Función para predecir Naive Bayes
predict_naive_bayes <- function(newdata, prior, likelihoods) {
  probs <- matrix(1, nrow = nrow(newdata), ncol = length(prior))
  colnames(probs) <- names(prior)
  
  for (var in names(newdata)) {
    var_levels <- rownames(likelihoods[[var]])  # Niveles conocidos en entrenamiento
    for (class in names(prior)) {
      probs_tmp <- rep(1e-6, nrow(newdata))  # Inicializar probabilidad mínima
      known_mask <- newdata[[var]] %in% var_levels
      if (any(known_mask)) {
        probs_tmp[known_mask] <- likelihoods[[var]][as.character(newdata[[var]][known_mask]), class]
      }
      probs[,class] <- probs[,class] * probs_tmp
    }
  }
  
  # Multiplicar por prior
  for (class in names(prior)) {
    probs[,class] <- probs[,class] * prior[class]
  }
  
  # Normalizar filas
  probs <- probs / rowSums(probs)
  
  return(probs)
}

# 3. Predicción
probs_test <- predict_naive_bayes(test_data[,-9], prior, likelihoods)

# 4. Clasificación
predicted_class <- ifelse(probs_test[,"1"] > 0.5, 1, 0)

# 5. Evaluación
cf_manual_nb <- confusionMatrix(
  factor(predicted_class, levels = c(0,1)),
  factor(test_data$churn, levels = c(0,1)),
  positive = "1"
)

print(cf_manual_nb)
```

A partir de esta respueta podemos sacar kas siguientes conclusiones.

**Conclusiones del Modelo Naive Bayes**

| Métrica | Valor | Conclusión |
|-------------------|-------------------|----------------------------------|
| **Accuracy** | 92.16% | Excelente precisión general del modelo. |
| **Kappa** | 0.7358 | Alta concordancia respecto a la clasificación aleatoria. |
| **Sensibilidad (Recall)** | 69.23% | Detecta 7 de cada 10 clientes que realmente se van. |
| **Especificidad** | 98.03% | Detecta correctamente casi todos los clientes que se quedan. |
| **Precisión** | 90.00% | Muy confiable al predecir que un cliente se irá. |
| **Balanced Accuracy** | 83.63% | Buen rendimiento balanceado en clases desiguales. |
| **Prevalencia** | 20.37% | Proporción real de clientes que abandonan en el test. |
| **p-valor McNemar** | \< 2.2e-16 | Diferencias significativas entre errores tipo I y tipo II. |

> **Conclusión general**: El modelo Naive Bayes muestra muy buen
> rendimiento general, especialmente en identificar clientes que se
> quedan. Es útil para clasificación binaria con clases desbalanceadas,
> aunque podría afinarse aún más si el objetivo es maximizar la
> detección de fuga (mayor sensibilidad).

Finalmente podemos hacer una comparación de los resultados usando la
funcion Naive Bayes y haciendo el método de manera manual.

**Comparación de Modelos Naive Bayes: Paquete vs. Manual**

| Métrica                   | Naive Bayes (paquete) | Naive Bayes (manual) |
|---------------------------|-----------------------|----------------------|
| **Accuracy**              | 83.41%                | 82.86%               |
| **Kappa**                 | 0.3237                | 0.3550               |
| **Sensibilidad (Recall)** | 26.47%                | 33.58%               |
| **Especificidad**         | 97.99%                | 95.48%               |
| **Precisión (PPV)**       | 77.14%                | 65.55%               |
| **Balanced Accuracy**     | 62.23%                | 64.53%               |
| **Detección Positivos**   | 5.40%                 | 6.85%                |
| **p-valor**               | \< 2.2e-16            | \< 2.2e-16           |

-   Ambos modelos tienen una precisión general alta (\~83%), pero el
    modelo manual obtiene mejor sensibilidad.

-   El modelo del paquete naivebayes es más conservador y predice mejor
    a los que no se irán, con alta especificidad (97.9%).

-   El modelo implementado manualmente sacrifica algo de especificidad,
    pero logra detectar más clientes que se irán (mejor sensibilidad:
    33.6% vs. 26.5%).

-   El Kappa es ligeramente mejor en el modelo manual (0.355 vs. 0.324),
    lo que indica mejor acuerdo global con la clase real.

Viendo esto podemos decir que en reneral el modelo hecho de manera
manual es mejor pero veamos en que aspectos específicamente es mejor el
modelo manual o el modelo hecho con el paquete.

**Justificación basada en métricas**

| Métrica clave | Mejor modelo | Explicación breve |
|-----------------|-----------------|--------------------------------------|
| **Sensibilidad (Recall)** | **Naive Bayes Manual** | 33.6% vs. 26.5%. Detecta más clientes que realmente abandonan. |
| **Balanced Accuracy** | **Naive Bayes Manual** | 64.5% vs. 62.2%. Mide rendimiento considerando el desbalance de clases. |
| **Kappa** | **Naive Bayes Manual** | 0.355 vs. 0.324. Muestra mayor concordancia entre predicción y realidad. |
| **Precisión (PPV)** | **Naive Bayes Paquete** | 77.1% vs. 65.6%. Mayor proporción de aciertos entre los que predice como “sí”. |
| **Especificidad** | **Naive Bayes Paquete** | 98.0% vs. 95.5%. Menos falsos positivos, es más conservador con la clase negativa. |


# JORGE
Entrenamos los datos

```{r}
library(tidyverse)
banco <- read.csv('Bank Customer Churn Prediction.csv')
head(banco)
```


```{r}
cat("tiene una dimension de", dim(banco))
str(banco)
```

Particionamos los datos
```{r}
set.seed(11234)
n <- dim(banco)[1]
indices <- seq(1:n)

indices.train <- sample(indices, size = n* .5, replace = FALSE)
indices.valid <- sample(indices[-indices.train], size = n* .25, replace = FALSE)
indices.test <- indices[-c(indices.train, indices.valid)]

banco.train <- banco[indices.train,]
banco.valid <- banco[indices.valid,]
banco.test <- banco[indices.test,]

dim(banco.train)
dim(banco.valid)
dim(banco.test)

```

Estudiamos la influencia de active_member en churn
```{r}
tabla1 <- xtabs(~active_member+churn, data = banco.train)
tabla1
```

```{r}
chisq.test(tabla1)
```
Podemos ver que existe una relación relevante entre ambas variables ya que es p-valor es claramente menor a 0.05. 

Usamos la regresión logística para cuantificar la relación:
```{r}
banco.train$factor.churn <- factor(banco.train$churn)
logit1 <- glm(factor.churn ~ active_member, data = banco.train, family = "binomial")
summary(logit1)
```
Como vemos la variable active_members tiene alta significatividad en la variable respuesta churn. La estimación es negativa, lo que indica que ser miembro activo disminuye la probabilidad de churn(irse del banco).

```{r}
exp(cbind(OR=coef(logit1), confint.default(logit1)))
```

Como el OR de active_member es menor que 1, es un factor protector contra el churn. Ser miembro activo del banco reduce de forma significativa la probabilidad de que un cliente se dé de baja del banco. La odds ratioo de 0.47... indica que las probabilidades de churn se reducen casi a la mitad cuando el cliente es activo.

```{r}
confint.default(logit1)
```


Ahora vamos a hacer un modelo con cada variable con la variable objetivo churn para ver como se comportan.
```{r}
banco$gender <- as.factor(banco$gender)
banco$country <- as.factor(banco$country)
banco$credit_card <- as.factor(banco$credit_card)
banco$active_member <- as.factor(banco$active_member)

# Lista de nombres como strings
variables_binomiales <- c("age", "tenure", "credit_score", "country", 
                          "gender", "balance", "products_number", 
                          "credit_card", "active_member", "estimated_salary")


factor.churn <- factor(banco$churn)

# Bucle
for (var in variables_binomiales) {
  formula <- as.formula(paste("factor.churn ~", var))
  modelo <- glm(formula, data = banco.train, family = "binomial")
  
  cat("\n---------------------------\n")
  cat("Variable:", var, "\n")
  print(summary(modelo))  # Resumen del modelo
  
  # Mostrar odds ratios e intervalos de confianza
  cat("Odds Ratio e IC 95%:\n")
  print(exp(cbind(OR = coef(modelo), confint.default(modelo))))
}


```
Interpretación de resultados:

Variable: age
p-value < 2e-16 → altamente significativa
OR = 1.072 → Por cada año adicional, las odds de abandono (churn) aumentan un 7.2%
Significativa

Variable: tenure
p-value = 0.126 → no significativa
OR = 0.98 → ligera disminución del riesgo con más años, pero no concluyente
No significativa

Variable: credit_score
p-value = 0.417 → no significativa
OR ≈ 1 → efecto mínimo/neutro
No significativa

Variable: country
countryGermany: p-value < 2e-16, OR = 2.49 → los clientes alemanes tienen 149% más odds de churn que los de referencia
countrySpain: p-value = 0.246, OR = 1.11 → no significativa
Solo countryGermany es significativa

Variable: gender
p-value < 2e-16, OR = 0.577 → ser hombre reduce las odds de churn en un 42.3%
Significativa

Variable: balance
p-value < 2e-16, OR muy cercana a 1 pero significativa → valores altos de balance se asocian positivamente con churn
Significativa

Variable: products_number
p-value = 0.006, OR = 0.846 → cada producto extra reduce las odds de churn en un 15.4%
Significativa

Variable: credit_card
p-value = 0.206, OR = 0.91 → no significativa
No significativa

Variable: active_member
p-value < 2e-16, OR = 0.47 → ser miembro activo reduce las odds de churn un 53%
Significativa

Variable: estimated_salary
p-value = 0.493, OR ≈ 1 → sin efecto aparente
No significativa

Conclusión general:
Variables significativas individualmente:

    age

    countryGermany

    gender

    balance

    products_number

    active_member

Variables no significativas:

    tenure

    credit_score

    countrySpain

    credit_card

    estimated_salary

A continuación. ajustamos un modelo de regresión logística con todas las carecterísticas significativas.
```{r}
modelo2 <- glm(factor.churn ~ age + country + gender + balance + products_number + active_member, data = banco.train, family = "binomial")
summary(modelo2)
```

En un modelo con todas las variables deja de ser significante la variable de products_number al producirse la interacción con las demás variables


```{r}
table(banco.train$factor.churn)
```
Esto nos indica que el modelo está sesgado hacia la clase "no".


Calculamos la probabilidad que ofrece el modelo para cada una de las observaciones en la muestra de entrenamiento
```{r}
predicciones <- predict(modelo2, type="response")
hist(predicciones)
```
En este histograma que hemos creado sobre la distribución de las probabilidades predichas por el modelo de regresión logística que hemos creado con las variables binomiales significantes que hemos creado, vemos que la mayoría de las predicciones están entre 0.1 y 0.2, lo que significa que la mauoría de los clientes tienen baja probabilidad de irse. Además no hay clientes con altas probabilidades de irse.
#TENGO QUE EXPLICAR BIENE ESTE GRÁFICO





```{r}
library(caret)

# Asegura que la variable objetivo sea un factor con los niveles correctos
banco.train$churn <- ifelse(banco.train$churn %in% c("yes", "1"), "yes", "no")
banco.train$churn <- factor(banco.train$churn, levels = c("no", "yes"))


clase.pred <- ifelse(predicciones > 0.5, "yes", "no")
clase.pred <- factor(clase.pred, levels = c("no", "yes"))

confusionMatrix(data= clase.pred ,reference=banco.train$churn ,positive="yes")
```
Esta matriz de confusión nos indica los falsos positivos y negativos, igual que los verdaderos positivos y negativos. Nuestro modelo acierta el 81.2% de las veces(Accuracy), aunque el modelo no está prediciendo bien la probabilidad de irse del banco(churn = yes) ya que hay más falsos positivos que verdaderos positivos. La sensibilidad nos dice que el modelo solo detecta el 22.6% de los que realmente hacen churn, mientras de los que no hacen churn un 96.5%. 


```{r}
confmat1 <- confusionMatrix(
  data = clase.pred,
  reference = factor(banco.train$churn, levels = c("no", "yes")),
  positive = "yes"
)

# F1 Score real
Precision <- confmat1$byClass["Pos Pred Value"]
Recall <- confmat1$byClass["Sensitivity"]
F1score <- 2 * Precision * Recall / (Precision + Recall)
names(F1score) <- "F1Score"
F1score

```
El F1Score es un indicador de balance entre precisión y recall. Un 0.33 es un balance muy bajo, lo que indica que el modelo no está haciendo un gran trabajo prediciendo quién se va a dar de baja.

Vamos a realizar un balanceo de las clases, lo que ayuda a mejorar la predicción, ya que tenemos un conjunto de datos desequillibrados, para ello hemos eliminado la columna customer_id ya que no aportaba nada de información, hemos convertido la variable gender a tipo factor.
```{r}
library(ROSE)

# 1. Eliminamos customer_id
banco.train$customer_id <- NULL

# 2. Convertimos gender a factor
banco.train$gender <- as.factor(banco.train$gender)

# 3. Aseguramos que todos los character se conviertan en factor, por si acaso
banco.train[sapply(banco.train, is.character)] <- 
  lapply(banco.train[sapply(banco.train, is.character)], as.factor)

# 4. Aplicamos ROSE (usamos churn como variable objetivo)
datos_balanceados <- ROSE(churn ~ ., data = banco.train, seed = 123)$data

# 5. Confirmamos balanceo
table(datos_balanceados$churn)

```

Después de aplicar el balanceo con ROSE, las clases ahora están equilibradas, con 2521 para no y 2479 para yes.

```{r}
library(caret)
set.seed(42)
trainIndex <- createDataPartition(datos_balanceados$factor.churn, p = 0.8, list = FALSE)
datos_train <- datos_balanceados[trainIndex, ]
datos_test <- datos_balanceados[-trainIndex, ]

modelo_log <- glm(factor.churn ~ credit_score + gender + age + tenure + balance +
                  products_number + credit_card + active_member + estimated_salary +
                  country,
                  data = datos_train, family = "binomial")

predicciones_prob <- predict(modelo_log, datos_test, type = "response")
predicciones <- ifelse(predicciones_prob > 0.5, "1", "0")

predicciones <- factor(predicciones, levels = c("0", "1"))
reales <- datos_test$factor.churn

matriz_confusion <- confusionMatrix(predicciones, reales, positive = "1")
matriz_confusion
f1_score <- matriz_confusion$byClass["F1"]
f1_score

```
Con los nuevos datos balanceados tenemos un F1Score de 0.71 lo que es mucho mayor y es casi el doble de rendimiento sobre la clase positiva de churn


Calculamos la curva ROC.
```{r}
library(pROC)
roc_score=roc(datos_test$churn, predicciones_prob,plot=TRUE,print.auc=TRUE)
```


### Análisis Discriminante Lineal

Es una técnica estadística de clasificación supervisada utilizada para encontrar una combinación lienal de variables que discrimine uno o más grupos.

El análisis discriminante lineal (LDA) sigue una filosofía similar al PCA, pero con un objetivo distinto que es maximinar la separabilidad entre clases. 

Vamos a hacer un ejemplo comprarando LDA y PCA
```{r}
library(MASS)
```

```{r}
set.seed(42)

datos_train$factor.churn <- factor(datos_train$factor.churn, levels = c("0", "1"))

modelo_lda <- lda(factor.churn ~ credit_score + gender + age + tenure + balance +
                  products_number + credit_card + active_member + estimated_salary +
                  country,
                  data = datos_train)

predicciones_lda <- predict(modelo_lda, newdata = datos_test)
```


```{r}
# Gráfico de LDA
plot(predicciones_lda$x[, 1], rep(0, length(predicciones_lda$x[, 1])), col = datos_test$churn, 
     xlab = "Componente discriminante 1", ylab = "", 
     main = "Proyección de LDA", pch = 19)
legend("topright", legend = levels(datos_test$churn), col = 1:length(levels(datos_test$class)), pch = 19)
```

```{r}
# Convertimos predicciones a factor con los mismos niveles que el real
pred_class <- factor(predicciones_lda$class, levels = c("0", "1"))
actuales <- factor(datos_test$factor.churn, levels = c("0", "1"))

conf_matrix <- confusionMatrix(pred_class, actuales)
f1_score <- conf_matrix$byClass["F1"]
f1_score

```

Con el análisis discriminante lineal ha mejorado levemente el modelo.

### Árboles de decisión.

Vamos a aplicar el método del árbol de decisión(DT) a nuestros datos, en el que hacemos un modelo para predecir la variable churn a partir de otras variables.
```{r}
library(rpart)
library(rpart.plot)

set.seed(128)

datos_DT <- datos_train %>%
  dplyr::select(credit_score, country, gender, age, tenure, balance, products_number, credit_card, active_member, estimated_salary, churn)
fit.dt <- rpart(churn~., data = datos_DT, method = 'class')

rpart.plot(fit.dt, extra = 100)

```
El nodo raíz es la condición de age < 41, si la condición es verdadera se sigue por la rama izquierda, si es falsa por la derecha. Cada nodo intermedio tiene su condición.
Los nodos hojas contienen una predicción "yes" o "no" y el porcentaje de datos en ese nodo que cumplen esa clase. Por ejemplo, el nodo "no" del 51% predice que el cliente no se dará de baja y el 51% de los casos están ahí.

ANÁLISIS DEL ÁRBOL DE DECISIÓN: PREDICCIÓN DE CHURN

    Raíz del árbol
    Condición: age < 41

    Si la edad es menor a 41: rama izquierda

    Si la edad es 41 o más: rama derecha
    
    Distribución:
    53% de los casos van hacia 'no churn'

    47% hacia 'sí churn'

    Rama izquierda (age < 41)
    Condición: products_number < 2.6

    Si products_number < 2.6:
    Resultado: 51% no churn, 3% sí churn
    Interpretación: Clientes jóvenes con pocos productos tienden a quedarse.

    Si products_number ≥ 2.6:
    Se analiza: balance < 63000

        Si balance < 63000:
        Resultado: 6% no churn, 3% sí churn
        Interpretación: Jóvenes con más productos y saldo bajo siguen siendo leales.

        Si balance ≥ 63000:
        Se analiza: age < 49

            Si age < 49:
            Resultado: 3% no churn, 3% sí churn
            Interpretación: Jóvenes con más productos y saldo alto muestran comportamiento mixto.

    Rama derecha (age ≥ 41)
    Condición: products_number ≥ 1.7

    Si products_number ≥ 1.7:
    Resultado: 29% sí churn
    Interpretación: Clientes mayores con muchos productos tienden a abandonar el servicio.

    Si products_number < 1.7:
    Se analiza: products_number < 2.4

        Si products_number < 2.4:
        Resultado: 5% sí churn

        Si products_number ≥ 2.4:
        Resultado: 6% sí churn
        Interpretación: Clientes mayores con menos productos tienen menor probabilidad de churn.
        

Usamos el mdoelo DT para realizar predicciones:
```{r}
# Sobre la partición de entrenamiento
prediccion_DT <- predict(fit.dt, datos_train, type = 'class')
cf_DT <- confusionMatrix(prediccion_DT, as.factor(datos_DT$churn), positive = "yes")

print(cf_DT)
```
Un rendimiento del 73.83%.


```{r}
# Sobre la partición de prueba
prediction.dt <- predict(fit.dt, datos_test, type = 'prob')[,2]
clase.pred=ifelse(prediction.dt>0.5,"yes","no")

cf_DT_prueba <- confusionMatrix(as.factor(clase.pred), as.factor(datos_test$churn),positive="yes")

print(cf_DT_prueba)
```
Un rendimiento casi igual que el de entrenamiento del 73.27%

### Grid Search

Es una técnica para encontrar los mejores hiperparámetros de nuestro modelo.

```{r}
library(caret)

set.seed(1271)

# Definir correctamente el control de entrenamiento
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)

# Crear la grilla de valores de k
grid <- expand.grid(.k = seq(1, 30, by = 1))

# Entrenar el modelo KNN
fit.knn <- train(churn ~ ., data=datos_train, method="knn",
                 metric="Accuracy", tuneGrid=grid, trControl=train_control)


# Obtener el mejor valor de k
knn.k2 <- fit.knn$bestTune

# Mostrar resultados
print(fit.knn)

```

El mejor valor de k fue 22, ya que tuvo la mejor precisión promedio: 0.5441.
Kappa = 0.0879 es un valor muy bajo, indica que el modelo no mejora mucho sobre una predicción aleatoria.
Además, el modelo tiene una precisión muy baja.


```{r}
plot(fit.knn)
```


```{r}
set.seed(128)

prediction.knn <- predict(fit.knn, newdata = datos_test, type = "prob")[,2]
clase.pred.knn <- ifelse(prediction.knn > 0.5, "yes", "no")

confusionMatrix(as.factor(clase.pred.knn), as.factor(datos_test$churn), positive = "yes")

```
Problemas en el modelo:
- Predice mejor los churn "no" que los "yes".
- Hay un alto número de falsos negativos(273 clientes que están mal clasificados).
- Precisión general muy baja(52.65%).


Lo que podemos hacer es un escalado y centrado de datos en KNN.
```{r}
set.seed(1271)

# Entrenar el modelo KNN
fit.knn.scaled <- train(churn ~ ., 
                        data = datos_train, 
                        method = "knn",
                        metric = "Accuracy", 
                        tuneGrid = grid, 
                        trControl = train_control,
                        preProcess = c("center", "scale"))


# Obtener el mejor valor de k
knn.k2 <- fit.knn.scaled$bestTune

# Mostrar resultados
print(fit.knn.scaled)

```


```{r}
plot(fit.knn.scaled)
```

Estos resultados son sospechosamente buenos, casi perfectos.

Es muy probable que haya un problema de data leakage (fuga de datos), donde alguna variable en datos_train permite predecir la clase con altísima precisión (por ejemplo, una variable redundante o derivada de la clase churn).


```{r}
# Predicción de probabilidades y clases
prediction.knn <- predict(fit.knn.scaled, newdata = datos_test, type = "prob")[,2]
clase.pred.knn <- ifelse(prediction.knn > 0.5, "yes", "no")

# Evaluación con matriz de confusión
confusionMatrix(as.factor(clase.pred.knn), as.factor(datos_test$churn), positive = "yes")

```
El modelo está prediciendo con 100% de precisión en los datos de test, lo cual nunca es realista en un problema real.


