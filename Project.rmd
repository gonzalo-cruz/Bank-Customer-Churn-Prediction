---
title: "Bank Customer Churn"
author: "Gonzalo Cruz Gómez, Samuel Martínez Lorente, Jorge Tordesillas García"
output: 
  html_document:
    theme: flatly
    toc: yes
    toc_float:
      collapsed: true
---

### Introducción

Objetivo:

El objetivo principal de este conjunto de datos es predecir la fuga de clientes (churn). En otras palabras, queremos construir un modelo que pueda determinar qué clientes tienen más probabilidades de dejar el banco en el futuro. Esto es importante para los bancos, ya que retener a los clientes existentes es normalemente más rentable que conseguir clientes nuevos.

Descripción de Datos (Variables):

```         
CustomerId: Identificador único para cada cliente. (Tipo: Numérico)
CreditScore: Puntaje de crédito del cliente. (Tipo: Numérico)
Country: País donde reside el cliente (por ejemplo, Francia, España, Alemania). (Tipo: Categórico)
Gender: Género del cliente (Masculino o Femenino). (Tipo: Categórico)
Age: Edad del cliente. (Tipo: Numérico)
Tenure: Número de años que el cliente ha sido cliente del banco. (Tipo: Numérico)
Balance: Saldo en la cuenta bancaria del cliente. (Tipo: Numérico)
Products_number: Número de productos bancarios que el cliente utiliza. (Tipo: Numérico)
Card: Indica si el cliente tiene tarjeta de crédito (1 = Sí, 0 = No). (Tipo: Binario)
Active_member: Indica si el cliente es un miembro activo (1 = Sí, 0 = No). (Tipo: Binario)
EstimatedSalary: Salario estimado del cliente. (Tipo: Numérico)
Churn: Variable objetivo. Indica si el cliente dejó el banco (1 = Sí, 0 = No). (Tipo: Binario)
```

Tipo de Problema:

Este es un problema de clasificación binaria. El objetivo es clasificar a los clientes en dos categorías: aquellos que se irán (1) y aquellos que se quedarán (0).

### Business understanding

Planteamos preguntas sobre nuestros datos:

-   ¿Influye el salario en si el cliente deja el banco?

-   ¿Influye el tiempo que lleva el cliente en el banco en si este deja este banco?

-   ¿Influye la edad?

-   ¿Influye el género?

-   ¿Influye el país?

### Data understanding

Importamos las librerías

```{r librerias}
library(ggplot2)
library(readr)
library(tidyr)
library(dplyr)
library(gridExtra)
library(GGally)
library(factoextra)
library(cluster)
library("NbClust")
library(parameters)
```

Leemos los datos

```{r lectura de datos}
Data <- read_csv("Bank Customer Churn Prediction.csv")
ntotal <- dim(Data)[1]
ptotal <- dim(Data)[2]
```

```{r visualizar primeros datos de la tabla}
head(Data)
```

Podemos ver que tenemos n = 10000 observaciones y 12 variables en el dataset

Dividimos los datos en train-test-validate

```{r division train-test-val}
set.seed(123)

# creamos los indices
indices <- 1:ntotal
ntrain <- ntotal *.6
ntest <- ntotal* .2
nval <- ntotal-(ntrain+ntest)
indices.train <- sample(indices, ntrain, replace= FALSE)
indices.test <- sample(indices[-indices.train], ntest, replace= FALSE)
indices.val <- indices[-c(indices.train, indices.test)]

# 60% para train, 20% para test y 20% para validate

train <- Data[indices.train,]
test <- Data[indices.test,]
validate <- Data[indices.val,]
```

Veamos las variables:

```{r visualizar variables}
str(train)
```

Podemos observar que la mayoria de nuestras variables son numéricas, a excepción de aquellas que son char como country y gender, que son variables categóricas. En el caso de credit card, active_member, products number, tenure y churn se trata de variables discretas.

Antes de nada vamos a transformar nuestras variables categóricas de char a factor.

```{r}
train$country <- as.factor(train$country)
train$gender <- as.factor(train$gender)
```

### Exploratory Data Analysis

Veamos un resumen de algunos de los estadísticos principales de cada columna

```{r estadísticos resumen}
summary(train)
```

Comprobamos si hay valores faltantes:

```{r valores faltantes}
colSums(is.na(Data))
```

Vemos que no tenemos valores faltantes en ninguna columna.

Visualizamos nuestros datos para ver cómo son y cómo están distribuidos

```{r visualizacion variables continuas}
train_long <- train %>%
  dplyr::select(estimated_salary, balance, credit_score) %>%  
  tidyr::gather(key = "Variable", value = "Value")

ggplot(train_long, aes(x = Value, fill = Variable)) +
  geom_histogram(bins = 20, color = "black", alpha = 0.5, aes(y =..density..)) + 
  geom_density(aes(y =..density.., color = Variable), linewidth = 0.5, alpha = 0.2) + 
  facet_wrap(~ Variable, scales = "free") +
  labs(title = "Continuous variables", x = "Value", y = "Density") +
  scale_fill_manual(values = c("skyblue", "lightgreen", "lightcoral")) +
  scale_color_manual(values = c("darkblue", "darkgreen", "darkred")) + 
  theme_bw() +
  theme(legend.position = "none")
```

Nuestras variables balance y credit score siguen distribuciones bastante similares a la normal (exceptuando el gran número de ceros en balance), mientras que el salario estimado sigue una distribución más uniforme.

```{r vsualizacion variables discretas}
  train_long <- train %>%
    tidyr::pivot_longer(cols = c(products_number, active_member, churn),
                        names_to = "variable", values_to = "value")

  combined_plot <- ggplot(train_long, aes(x = value)) +
    geom_bar(fill = "lightblue", color = "black") +
    facet_wrap(~ variable, scales = "free_x") +
    labs(title = "Discrete variables",
         x = "Value",
         y = "Count") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  
  print(combined_plot)
```

Podemos ver como hay más gente que se queda en el banco respecto a la que se va, el numero de miembros activos y pasados es basicamente igual y que el número de productos más habitual es o 0 o 1, con muy pocos clientes teniendo más de 2.

```{r visualizacion tenure}
plot_tenure_hist <- ggplot(train, aes(x = tenure)) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +  
  labs(title = "Distribution of Tenure",
       x = "Tenure",
       y = "Count") +
  theme_bw()

print(plot_tenure_hist)
```

Ahora miramos las variables categóricas

```{r}
# Gráfico de barras para 'Gender'
ggplot(train, aes(x = gender)) +
  geom_bar(fill = c("skyblue", "salmon")) +
  labs(title = "Distribución de Género",
       x = "Género",
       y = "Número de Clientes") +
  theme_minimal()

# Gráfico de barras para 'Geography'
ggplot(train, aes(x = country)) +
  geom_bar(fill = c("lightgreen", "lightcoral", "lightblue")) +
  labs(title = "Distribución por Países",
       x = "País",
       y = "Número de Clientes") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotar etiquetas

```

Podemos ver que hay más clientes hombres que mujeres, aunque la diferencia no es grande, y también podemos observar que la mayor parte de los clientes del banco son franceses, con un número similar de clientes de este país a la combinación de España y Alemania.

Vamos a buscar si hay algún tipo de relación entre nuestras variables, para ello vamos a utilizar scatter plots:

```{r}
# Estimated salary vs Balance
ggplot(train, aes(x = estimated_salary, y = balance)) +
  geom_point() +
  labs(x = "Estimated Salary", y = "Balance")

# Balance vs Credit Score
ggplot(train, aes(x = credit_score, y = balance)) +
  geom_point() +
  labs(x = "Credit Score", y = "Balance")

# Credit Score vs Estimated Salary
ggplot(train, aes(x = estimated_salary, y = credit_score)) +
  geom_point() +
  labs(x = "Estimated Salary", y = "Credit Score")


```

Podemos ver como siguen distribuciones bastante normales, con la excepción de los datos con valor 0 del balance, que como hay muchos, tienen mucho peso.

Vamos a ver qué nos dice nuestra variable objetivo

```{r visualización churn}
ggplot(data = train, aes(x = churn, fill = factor(churn))) + 
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "none") +
  ylab("Relative Frequency") +
  xlab("Churn") +
  theme_minimal() +  
  scale_fill_manual(values = c("skyblue", "tomato"), 
                    labels = c("No", "Yes")) + 
  labs(title = "Churn Distribution")
```

Podemos ver como la mayoría de los clientes no se van del banco pero hay un porcentaje significativo que si lo hace

Ahora veamos como estan distribuidos por género y país:

```{r churn vs genero}
train$churn <- factor(train$churn, levels = c(0, 1), labels = c("No", "Yes"))

train %>%
  count(gender, churn) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ggplot(aes(x = gender, y = Percentage, fill = churn)) +
  geom_col(position = "dodge") +
  labs(title = "Churn by Gender", x = "Gender", y = "Percentage") +
  theme_bw() +
  scale_fill_manual(values = c("skyblue", "tomato"), name = "Churn Status")

```

Aquí se observa que el porcentaje de hombres que no se marchan del banco es mayor que el de las mujeres

```{r}
train %>%
  count(country, churn) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ggplot(aes(x = country, y = Percentage, fill = churn)) +
  geom_col(position = "dodge") +
  labs(title = "Churn by country", x = "country", y = "Percentage") +
  theme_bw() +
  scale_fill_manual(values = c("skyblue", "tomato"), name = "Churn Status")
```

Vemos como los clientes franceses y los españoles son mucho menos propensos que los alemanes a marcharse del banco

Vamos a intentar contestar las preguntas que nos hicimos al inicio:

\- ¿Influye el salario en si el cliente deja el banco?

```{r churn vs salario}
ggplot(train, aes(x = factor(churn), y = estimated_salary, fill = factor(churn))) +
  geom_boxplot() +
  labs(title = "Salary vs. Churn", x = "Churn", y = "Estimated Salary") +
  scale_x_discrete(labels = c("No", "Yes")) +
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) 

```

Podemos ver como, a priori, el salario anual no influye en la decisión de abandonar el banco Vamos a comprobarlo con un contraste de hipótesis:

```{r distribucion salary}
train$churn <- as.factor(train$churn)
alpha <- 0.05
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(estimated_salary)$p.value)
if(p_value<alpha){
  
  print("No sigue una distribución normal")
}else{
  print("sigue una distribución normal")
}
```

Como no sigue una distribución normal debemos utilizar un test no paramétrico coomo el de wilcoxon para comprobar si existe una diferencia significativa.

```{r contraste churn-salario}

wilcox_result <- wilcox.test(estimated_salary ~ churn, data = train)
print(wilcox_result)
alpha <-  0.05
p_value <- wilcox_result$p.value
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa en el tiempo que son clientes entre aquellos que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```

Como ya podíamos intuir por la gráfica no hay una diferencia significativa respecto al salario

\- ¿Influye el tiempo que lleva el cliente en el banco en si este deja el banco?

```{r churn vs tenure}
ggplot(train, aes(x = factor(churn), y = tenure, fill = factor(churn))) + 
  geom_boxplot() +
  labs(title = "Tenure vs. Churn", x = "Churn", y = "Tenure (years)") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))  
```

Vamos a ver si hay una diferencia significativa entre los dos grupos. Para ello vamos a plantear un contraste de hipótesis:

Primero comprobamos si tenure sigue una distribución normal

```{r distribucion tenure}
train$churn <- as.factor(train$churn)
alpha <- 0.05
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(tenure)$p.value)
if(p_value<alpha){
  
  print("No sigue una distribución normal")
}else{
  print("sigue una distribución normal")
}
```

Sigue una distribución normal. Utilizamos un test t para comprobar si existe alguna diferencia significativa.

```{r contraste tenure-churn}
t_test_result <- t.test(tenure ~ churn, data = train)
print(t_test_result)

p_value <- t_test_result$p.value
alpha <- 0.5

if (p_value < alpha){
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa en el tiempo que son clientes entre aquellos que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```

Por tanto podemos decir que el hecho de abandonar o no el banco sí depende del tiempo que llevan los clientes en el mismo.

\- ¿Influye la edad?

```{r age vs churn}
ggplot(train, aes(x = factor(churn), y = age, fill = factor(churn))) + 
  geom_boxplot() +
  labs(title = "Age vs. Churn", x = "Churn", y = "Age (years)") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
```

```{r distribucion age}
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(age)$p.value)
```

Vemos que age no sigue una distribución normal, por lo que aplico un test de wilcoxon

```{r contraste age-churn}
wilcox_result <- wilcox.test(age ~ churn, data = train)
print(wilcox_result)
p_value <- wilcox_result$p.value
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa en el tiempo que son clientes entre aquellos que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```
Podemos ver como la edad importa a la hora de que un cliente abandone el banco o no, usualmente siendo los clientes mayores los que tienden a abandonar el banco

-   ¿Influye el balance?

```{r balance vs churn}
ggplot(train, aes(x = factor(churn), y = balance, fill = factor(churn))) + 
  geom_boxplot() +
  labs(title = "Balance vs. Churn", x = "Churn", y = "Balance") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
```

```{r distribucion balance}
Strain %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(age)$p.value)
if(p_value<alpha){
  print("No sigue una distribución normal")
}else{
  print("sigue una distribución normal")
}
```

```{r contraste balance-churn}
wilcox_result <- wilcox.test(balance ~ churn, data = train)
print(wilcox_result)
p_value <- wilcox_result$p.value
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa en el balance de los clientes entre aquellos que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```
Se observa que cuanto mayor es el balance, mayor es la probabilidad de que el cliente se cambie de banco

### Tecnica de reducción a la dimensión (PCA).

Con el siguiente código veremos cómo afecta el uso del PCA a nuestros datos.

```{r}
set.seed(1234)

data_cont <- train[, c("credit_score", "age", "tenure", "balance", "products_number")]
data_cont <- na.omit(data_cont)

pca <- prcomp(data_cont, center = TRUE, scale. = TRUE)

# Guardar nuevas varialbes
data_pca <- as.data.frame(pca$x)
colnames(data_pca) <- paste0("PC", 1:ncol(data_pca))  # Renombrar dinámicamente

# Gráfico de datos originales (matriz de dispersión)
p1 <- ggpairs(data_cont) + 
  ggtitle("Datos originales (variables continuas)")

# Gráfico de componentes principales
p2 <- ggplot(data_pca, aes(x = PC1, y = PC2)) +
  geom_point(color = "red", alpha = 0.6) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_fixed() +
  ggtitle("Datos transformados (PCA)")

# Mostrar gráficos lado a lado
# Mostrar gráficos de forma separada
print(p1)  # Matriz de dispersión creada con ggpairs
print(p2)  # Gráfico de componentes principales
```

Ahora los resultados de aplicar el PCA usando el summary para ver el resumen:

```{r}
# Resumen del PCA (sin sobrescribir el objeto original)
pca_summary <- summary(pca)
print(pca$sdev)
print(pca$rotation)
print(pca$center)
print(pca$scale)
print(pca_summary)
```

```{r}
prcomp(Data$credit_score)
```

Sin usar ninguna función de R para reducir la dimensionalidad lo haremos de la siguiente manera: 1er paso: Realizaremos la normal de nuestro conjunto de datos:

```{r}
set.seed(1234)

data_cont <- train[, c("credit_score", "age", "tenure", "balance", "products_number")]
data_cont <- na.omit(data_cont)

X_data <- as.matrix(data_cont)

# Calculamos las medias de cada columna.
medias_data <- colMeans(X_data)
# Calculamos la desviación típica de cada columna.
desv_data <- apply(X_data,2,sd)

# Una vez tenemos los datos podemos normalizar nuestra variable.
# Primero restamos la media y después dividimos por la norma.
X_data <- sweep(X_data, 2, medias_data, "-")
X_norm <- sweep(X_data, 2, desv_data, "/")
```

2º paso: Calculamos la matriz de covarianza.

```{r}
C <- cov(X_norm)
print(C)
```

3er paso: Calculamos los autovalores y autovectores, usamos "eigen"

```{r}
# Descomponemos
eig <- eigen(C, TRUE, only.values = FALSE, EISPACK = FALSE)

# Accedemos a los autovalores y autovectores
autovalores <- eig$values
autovectores <- eig$vectors
print(autovectores)
print(autovalores)
```

4º paso: Proyectamos los datos normalizados en el espacio principal.

```{r}
Z <- X_norm %*% autovectores
```

5º paso: Reducimos la dimensionalidad.

```{r}
# Guardamos solo PC1 y PC2
Z_red <- Z[,1:2]
print(Z_red)
```

# Análisis no supervisado

En primer lugar vamos a calcular la distancia Euclídea entre las observaciones de la base de datos. También mostraremos la matriz de distancias.

```{r}
# Hacemos un escalado de las variables continuas
datos_escalados <- scale(data_cont)
```

```{r}
head(datos_escalados)
```

Matriz de distancias

```{r}
distance <- get_dist(datos_escalados, method = "euclidean")
print(as.matrix(distance)[1:5, 1:5])

# para visualizarlo, vamos a utilizar una muestra para optimizar el rendimiento
tamaño_muestra <- floor(0.1 * nrow(datos_escalados))
sampled_data <- datos_escalados[sample(seq_len(nrow(datos_escalados)), size = tamaño_muestra), ]

# Calculate the distance matrix on the sampled data
distance_matrix_sampled <- get_dist(sampled_data, method = "euclidean")

# Visualize the distance matrix
fviz_dist(distance_matrix_sampled) 

```

A partir de esto, podemos observar como se cumplen las condiciones para ser una métrica o medida de desemejanza:

-   Coincidencia: podemos ver que cuando x=y el valor es 0

-   No negatividad: el valor más bajo de nuestra matriz de distancias es 0

-   Simetría: con el gráfico podemos ver como la matriz es simétrica respecto al eje formado por x = y

S Podemos aplicar el algoritmo de las k-medias con k = 2

```{r}
k_medias <- kmeans(datos_escalados, centers = 2, nstart = 25)
str(k_medias)
```

Si imprimimos los resultados vemos que la técnica de agrupaciones dio lugar a 2 conglomerados o medias para los dos grupos en las variables que haya. También obtenemos la asignación de conglomerados para cada observación.

```{r}
set.seed(595)
k_medias
k_medias$cluster
k_medias$totss
k_medias$betweenss
k_medias$withinss
k_medias$tot.withinss
k_medias$size

```

Visualización de los clusters

```{r}
fviz_cluster(k_medias, data = datos_escalados)
```

Vamos a realizar los 3 métodos más populares para determinar el número óptimo de clústeres:

MÉTODO DEL CODO

```{r}
set.seed(123)

fviz_nbclust(datos_escalados, kmeans, method = "wss")
```

En este análisis, a partir de 4 clusters, la reducción en la suma total de cuadrados internos parece estabilizarse, indicando que k = 4 es una buena opción.

```{r}
k4 <- kmeans(datos_escalados, centers = 4, nstart = 25)

fviz_cluster(k4, data = datos_escalados)
k4

```

```{r}
set.seed(595)
table(k_medias$cluster, k4$cluster)

```

MÉTODO DE LA SILUETA

```{r}

fviz_nbclust(datos_escalados, kmeans, method = "silhouette")

```

Con esta técnica, en nuestros datos, parece que la mejor elección es k igual a 2.

```{r}
sil <-  silhouette(k_medias$cluster, dist(datos_escalados))

fviz_silhouette(sil)
```

La interpretación del coeficiente de la silueta la entendemos como que un valor positivo significa que la obervación está bien agrupada. Cuanto más se acerque al coeficiente a 1, mejor agrupada estará la observacíon. En cambio, un valor negativo significa que la obervación está mal agrupada. Finalmente, un valor igual a 0 significa que la observación se encuentra entre dos conglomerados.

El gráfico anterior y el coeficiente de silueta medio ayudan a determinar si la agrupación es buena o no.

MÉTODO GAP

El proceso es el siguiente: se aplica el algoritmo de clustering a los datos con diferentes valores de k, se generan conjuntos de datos de referencia aleatorios, se calcula el estadístico Gap para cada valor de k y la selección del número óptimo de clusters.

```{r}
set.seed(123)

gap <-  clusGap(datos_escalados, FUN = kmeans, nstart = 25, K.max = 10, B = 50)

print(gap, method = "firstmax")
```

```{r}
fviz_gap_stat(gap)
```

Parece ser que el número óptimo de clusters es 6.

```{r}
k6 <-  kmeans(datos_escalados, centers = 6, iter.max = 10, nstart = 25)

fviz_cluster(k6, data = datos_escalados)
```

Esquema de agrupación a partir de los diferentes resultados obtenidos variando todas las combinaciones de clústeres, medias de diatancia y métodos de agrupación

```{r}
# nb <-  NbClust(datos_escalados, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans")
```

```{r}
n_clust <-  n_clusters(as.data.frame(datos_escalados), 
                       package = c("easystarts", "NbClust", "mclust"),
                       standardize = FALSE)
n_clust
```

```{r}
plot(n_clust)
```

```{r}
data_cont %>%
  mutate(clusters = k6$cluster) %>%
  group_by(clusters) %>%
  summarise_all("mean")
```

Podemos interpretar esta tabla de diferenciación de las medias de la siguiente manera: - Edad más alta en el clúster 2 (60.86 años): Este grupo podría representar clientes mayores, tal vez personas cercanas a la jubilación. - Balance muy bajo en el clúster 3 (3134.47): Podría tratarse de clientes con cuentas nuevas o de bajo poder adquisitivo. - Balance alto en clúster 1 y 4 (121k-124k): Probablemente clientes con mayor poder adquisitivo o que mantienen altos saldos en sus cuentas. - Credit score más bajo en clúster 5 (640.05): Este grupo podría representar clientes con menor solvencia o más riesgo crediticio. - Tenure más alto en clúster 5 (7.59 años): Clientes que han estado más tiempo con la entidad financiera, pero con saldo bajo.

```{r}
res_kmeans <-  cluster_analysis(datos_escalados, 
                                n = 6,
                                method = "kmeans")

plot(summary(res_kmeans))
```

```{r}
set.seed(4321)
pam_clusters <- pam(x = datos_escalados, k = 6, metric = "manhattan")
pam_clusters

fviz_cluster(object = pam_cluster, data = datos_escalados, ellipse = "t", 
             repel = TRUE) +
  theme_bw() +
  labs(title = " Resultados clustering PAM") + theme(legend.position = "none")
```

Una de las limitaciones del método de clusters es que se requiere mucha memoria RAM, por lo que cuando el set de datos contiene muchas observaciones es imposible, igual que nuestro dataset.

Entonces vamos a simular un set de datos bidimensional con 500 observaciones, de las cuales 200 pertenecen a un grupo y 300 a otro.

```{r}
set.seed(1234)
grupo_1 <- cbind(rnorm(n = 200, mean = 0, sd = 8),
                 rnorm(n = 200, mean = 0, sd = 8))
grupo_2 <- cbind(rnorm(n = 300, mean = 30, sd = 8),
                 rnorm(n = 300, mean = 30, sd = 8))
datos <- rbind(grupo_1, grupo_2)
colnames(datos) <- c("x", "y")
head(datos)
```

```{r}
clara_clusters <- clara(x = datos_escalados, k = 2, metric = "manhattan", stand = TRUE,
                        samples = 50, pamLike = TRUE)
clara_clusters
```

```{r}
fviz_cluster(object = clara_clusters, ellipse.type = "t", geom = "point",
             pointsize = 2.5) +
  theme_bw() +
  labs(title = "Resultados clustering CLARA") +
  theme(legend.position = "none")
```
