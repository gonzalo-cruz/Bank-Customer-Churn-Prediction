---
title: "Bank Customer Churn"
author: "Gonzalo Cruz Gómez, Samuel Martínez Lorente, Jorge Tordesillas García"
output: 
  html_document:
    theme: flatly
    toc: yes
    toc_float:
      collapsed: true
---

### Introducción

El conjunto de datos contiene datos sobre los clientes de un banco. Estos datos han sido extraídos del repositorio Bank Customer Churn Dataset de Kaggle. Objetivo Descripcion datos Tipo de problema

### Business understanding

Planteamos preguntas sobre nuestros datos:

-   ¿Influye el salario en si el cliente deja el banco?

-   ¿Influye el tiempo que lleva el cliente en el banco en si este deja este banco?

-   ¿Influye la edad?

### Data understanding

Importamos las librerías

```{r librerias}
library(ggplot2)
library(readr)
library(tidyr)
library(dplyr)
library(gridExtra)
library(GGally)
library(factoextra)
```

Leemos los datos

```{r lectura de datos}
Data <- read_csv("Bank Customer Churn Prediction.csv")
ntotal <- dim(Data)[1]
ptotal <- dim(Data)[2]
```

```{r visualizar primeros datos de la tabla}
head(Data)
```

Podemos ver que tenemos n = 10000 observaciones y 12 variables en el dataset

Dividimos los datos en train-test-validate

```{r division train-test-val}
set.seed(123)

# creamos los indices
indices <- 1:ntotal
ntrain <- ntotal *.6
ntest <- ntotal* .2
nval <- ntotal-(ntrain+ntest)
indices.train <- sample(indices, ntrain, replace= FALSE)
indices.test <- sample(indices[-indices.train], ntest, replace= FALSE)
indices.val <- indices[-c(indices.train, indices.test)]

# 60% para train, 20% para test y 20% para validate

train <- Data[indices.train,]
test <- Data[indices.test,]
validate <- Data[indices.val,]
```

Veamos las variables:

```{r visualizar variables}
str(train)
```

Podemos observar que la mayoria de nuestras variables son continuas, a excepción de aquellas que son char como country y gender, que son variables categóricas, y active_member, churn (variable objetivo) y credit card, que se trata de variables binarias.

### Exploratory Data Analysis

Veamos un resumen de algunos estadísticos de cada columna

```{r estadísticos resumen}
summary(train)
```

Comprobamos si hay valores faltantes:

```{r valores faltantes}
colSums(is.na(Data))
```

Vemos que no tenemos valores faltantes en ninguna columna.

Visualizamos nuestros datos para ver cómo son y cómo están distribuidos

```{r visualizacion variables continuas}
train_long <- train %>%
  dplyr::select(estimated_salary, balance, credit_score) %>%  
  tidyr::gather(key = "Variable", value = "Value")

ggplot(train_long, aes(x = Value, fill = Variable)) +
  geom_histogram(bins = 20, color = "black", alpha = 0.5, aes(y =..density..)) + 
  geom_density(aes(y =..density.., color = Variable), linewidth = 0.5, alpha = 0.2) + 
  facet_wrap(~ Variable, scales = "free") +
  labs(title = "Continuous variables", x = "Value", y = "Density") +
  scale_fill_manual(values = c("skyblue", "lightgreen", "lightcoral")) +
  scale_color_manual(values = c("darkblue", "darkgreen", "darkred")) + 
  theme_bw() +
  theme(legend.position = "none")
```

Tenemos muchos valores en 0 de balance

```{r vsualizacion variables discretas}
  train_long <- train %>%
    tidyr::pivot_longer(cols = c(products_number, active_member, churn),
                        names_to = "variable", values_to = "value")

  combined_plot <- ggplot(train_long, aes(x = value)) +
    geom_bar(fill = "lightblue", color = "black") +
    facet_wrap(~ variable, scales = "free_x") +
    labs(title = "Discrete variables",
         x = "Value",
         y = "Count") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  
  print(combined_plot)
```

```{r visualizacion tenure}
plot_tenure_hist <- ggplot(train, aes(x = tenure)) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +  
  labs(title = "Distribution of Tenure",
       x = "Tenure",
       y = "Count") +
  theme_bw()

print(plot_tenure_hist)
```

Vamos a buscar si hay algún tipo de relación entre nuestras variables, para ello vamos a utilizar scatter plots:

\*\* preguntar en clase \*\*

```{r}
# Estimated salary vs Balance
ggplot(train, aes(x = estimated_salary, y = balance)) +
  geom_point() +
  labs(x = "Estimated Salary", y = "Balance")

# aplicamos una transformación logaritmica
# utilizamos log(1+p) para no perder precisión aunque p sea muy pequeño

ggplot(train, aes(x = log1p(estimated_salary), y = log1p(balance))) +
  geom_point() +
  labs(x = "Log(Estimated Salary)", y = "Log(Balance)")

# aplicamos una raíz como transformacion
ggplot(train, aes(x = sqrt(estimated_salary), y = sqrt(balance))) +
  geom_point() +
  labs(x = "Sqrt(Estimated Salary)", y = "Sqrt(Balance)")

# Balance vs Credit Score
ggplot(train, aes(x = credit_score, y = balance)) +
  geom_point() +
  labs(x = "Credit Score", y = "Balance")

# aplicamos una transformación logaritmica
ggplot(train, aes(x = credit_score, y = log1p(balance))) +
  geom_point() +
  labs(x = "Credit Score", y = "Log(Balance)")

# aplicamos una raíz como transformacion
ggplot(train, aes(x = sqrt(credit_score), y = sqrt(balance))) +
  geom_point() +
  labs(x = "Sqrt(Credit score)", y = "Sqrt(Balance)")


# Credit Score vs Estimated Salary
ggplot(train, aes(x = estimated_salary, y = credit_score)) +
  geom_point() +
  labs(x = "Estimated Salary", y = "Credit Score")

# aplicamos una transformación logaritmica
ggplot(train, aes(x = log1p(estimated_salary), y = credit_score)) +
  geom_point() +
  labs(x = "Log(Estimated Salary)", y = "Credit Score")

# aplicamos una raíz como transformacion
ggplot(train, aes(x = sqrt(estimated_salary), y = sqrt(credit_score))) +
  geom_point() +
  labs(x = "Sqrt(Estimated Salary)", y = "Sqrt(credit score)")


```

Vamos a ver qué nos dice nuestra variable objetivo

```{r visualización churn}
ggplot(data = train, aes(x = churn, fill = factor(churn))) + 
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "none") +
  ylab("Relative Frequency") +
  xlab("Churn") +
  theme_minimal() +  
  scale_fill_manual(values = c("skyblue", "tomato"), 
                    labels = c("No", "Yes")) + 
  labs(title = "Churn Distribution")
```

Podemos ver como la mayoría de los clientes no se van del banco pero hay un porcentaje significativo que si lo hace

Ahora veamos como estan distribuidos por género:

```{r churn vs genero}
train$churn <- factor(train$churn, levels = c(0, 1), labels = c("No", "Yes"))

train %>%
  count(gender, churn) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  ggplot(aes(x = gender, y = Percentage, fill = churn)) +
  geom_col(position = "dodge") +
  labs(title = "Churn by Gender", x = "Gender", y = "Percentage") +
  theme_bw() +
  scale_fill_manual(values = c("skyblue", "tomato"), name = "Churn Status")


```

Vamos a intentar contestar las preguntas que nos hicimos al inicio:

\- ¿Influye el salario en si el cliente deja el banco?

```{r churn vs salario}
ggplot(train, aes(x = factor(churn), y = estimated_salary, fill = factor(churn))) +
  geom_boxplot() +
  labs(title = "Salary vs. Churn", x = "Churn", y = "Estimated Salary") +
  scale_x_discrete(labels = c("No", "Yes")) +
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5)) 

```

Podemos ver como, a priori, el salario anual no influye en la decisión de abandonar el banco Vamos a comprobarlo con un contraste de hipótesis:

```{r contraste churn-salario}

wilcox_result <- wilcox.test(estimated_salary ~ churn, data = train)
print(wilcox_result)
p_value <- wilcox_result$p.value
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa en el tiempo que son clientes entre aquellos que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```

\- ¿Influye el tiempo que lleva el cliente en el banco en si este deja el banco?

```{r churn vs tenure}
ggplot(train, aes(x = factor(churn), y = tenure, fill = factor(churn))) + 
  geom_boxplot() +
  labs(title = "Tenure vs. Churn", x = "Churn", y = "Tenure (years)") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))  
```

Vamos a ver si hay una diferencia significativa entre los dos grupos. Para ello vamos a plantear un contraste de hipótesis:

Primero comprobamos si tenure sigue una distribución normal

```{r distribucion tenure}
train$churn <- as.factor(train$churn)
alpha <- 0.05
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(tenure)$p.value)
if(p_value<alpha){
  
  print("No sigue una distribución normal")
}else{
  print("sigue una distribución normal")
}
```

No sigue una distribución normal. Utilizamos un test no paramétrico (en este caso el test de wilcoxon) para ver si existe una diferencia significativa entre los dos grupos

```{r contraste tenure-churn}
wilcox_result <- wilcox.test(tenure ~ churn, data = train)
print(wilcox_result)
p_value <- wilcox_result$p.value
alpha <- 0.5
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa en el tiempo que son clientes entre aquellos que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```

\- ¿Influye la edad?

```{r age vs churn}
ggplot(train, aes(x = factor(churn), y = age, fill = factor(churn))) + 
  geom_boxplot() +
  labs(title = "Age vs. Churn", x = "Churn", y = "Age (years)") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
```

```{r distribucion age}
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(age)$p.value)
if(p_value<alpha){
  print("No sigue una distribución normal")
}else{
  print("sigue una distribución normal")
}
```

Vemos que age no sigue tampoco una distribución normal

```{r contraste age-churn}
wilcox_result <- wilcox.test(age ~ churn, data = train)
print(wilcox_result)
p_value <- wilcox_result$p.value
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa en el tiempo que son clientes entre aquellos que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```

- ¿Influye el balance?
```{r balance vs churn}
ggplot(train, aes(x = factor(churn), y = balance, fill = factor(churn))) + 
  geom_boxplot() +
  labs(title = "Balance vs. Churn", x = "Churn", y = "Balance") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("No", "Yes")) +
  theme_bw() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
```
```{r distribucion balance}
train %>%
  group_by(churn) %>%
  summarize(p_value = shapiro.test(age)$p.value)
if(p_value<alpha){
  print("No sigue una distribución normal")
}else{
  print("sigue una distribución normal")
}
```

```{r contraste balance-churn}
wilcox_result <- wilcox.test(balance ~ churn, data = train)
print(wilcox_result)
p_value <- wilcox_result$p.value
if (p_value < alpha) {
  print("Rechazamos la hipótesis nula. Hay una diferencia significativa en el balance de los clientes entre aquellos que abandonan el banco y aquellos que no")
} else {
  print("No podemos rechazar la hipótesis nula. No existe esta diferencia significativa")
}
```

### Tecnica de reducción a la dimensión (PCA).
Con el siguiente código veremos cómo afecta el uso del PCA a nuestros datos.
```{r}
set.seed(1234)

data_cont <- Data[, c("credit_score", "age", "tenure", "balance", "products_number")]
data_cont <- na.omit(data_cont)

pca <- prcomp(data_cont, center = TRUE, scale. = TRUE)

# Guardar nuevas varialbes
data_pca <- as.data.frame(pca$x)
colnames(data_pca) <- paste0("PC", 1:ncol(data_pca))  # Renombrar dinámicamente

# Gráfico de datos originales (matriz de dispersión)
p1 <- ggpairs(data_cont) + 
  ggtitle("Datos originales (variables continuas)")

# Gráfico de componentes principales
p2 <- ggplot(data_pca, aes(x = PC1, y = PC2)) +
  geom_point(color = "red", alpha = 0.6) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_fixed() +
  ggtitle("Datos transformados (PCA)")

# Mostrar gráficos lado a lado
# Mostrar gráficos de forma separada
print(p1)  # Matriz de dispersión creada con ggpairs
print(p2)  # Gráfico de componentes principales
```
Ahora los resultados de aplicar el PCA usando el summary para ver el resumen:
```{r}
# Resumen del PCA (sin sobrescribir el objeto original)
pca_summary <- summary(pca)
print(pca$sdev)
print(pca$rotation)
print(pca$center)
print(pca$scale)
print(pca_summary)
```
```{r}
prcomp(Data$credit_score)
```
Sin usar ninguna función de R para reducir la dimensionalidad lo haremos de la siguiente manera:
1er paso: Realizaremos la normal de nuestro conjunto de datos:
```{r}
set.seed(1234)

data_cont <- Data[, c("credit_score", "age", "tenure", "balance", "products_number")]
data_cont <- na.omit(data_cont)

X_data <- as.matrix(data_cont)

# Calculamos las medias de cada columna.
medias_data <- colMeans(X_data)
# Calculamos la desviación típica de cada columna.
desv_data <- apply(X_data,2,sd)

# Una vez tenemos los datos podemos normalizar nuestra variable.
# Primero restamos la media y después dividimos por la norma.
X_data <- sweep(X_data, 2, medias_data, "-")
X_norm <- sweep(X_data, 2, desv_data, "/")
```


2º paso: Calculamos la matriz de covarianza.
```{r}
C <- cov(X_norm)
print(C)
```
3er paso: Calculamos los autovalores y autovectores, usamos "eigen"
```{r}
# Descomponemos
eig <- eigen(C, TRUE, only.values = FALSE, EISPACK = FALSE)

# Accedemos a los autovalores y autovectores
autovalores <- eig$values
autovectores <- eig$vectors
print(autovectores)
print(autovalores)
```
4º paso: Proyectamos los datos normalizados en el espacio principal.

```{r}
Z <- X_norm %*% autovectores
```

5º paso: Reducimos la dimensionalidad.
```{r}
# Guardamos solo PC1 y PC2
Z_red <- Z[,1:2]
print(Z_red)
```
# Análisis no supervisado

En primer lugar vamos a calcular la distancia Euclídea entre las observaciones de la base de datos. También mostraremos la matriz de distancias.
```{r}

# Hacemos un escalado de las variables continuas
datos_escalados <- scale(data_cont)

head(datos_escalados)

distance <- get_dist(datos_escalados)

fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

Podemos aplicar el algoritmo de las k-medias con k = 2 

```{r}
k_medias <- kmeans(datos_escalados, centers = 2, nstart = 25)
str(k_medias)
```

Si imprimimos los resultados vemos que la técnica de agrupaciones dio lugar a 2 conglomerados o medias para los dos grupos en las variables que haya. También obtenemos la asignación de conglomerados para cada observación.

```{r}
k_medias
```

Visualización de los clusters
```{r}
fviz_cluster(k_medias, data = datos_escalados)
```

Vamos a realizar los 3 métodos más populares para determinar el número óptimo de clústeres:

MÉTODO DEL CODO

```{r}
set.seed(123)

fviz_nbclust(datos_escalados, kmeans, method = "wss")
```


Parece en este caso que 10 es una buena elección para el número de clusters

```{r}
k10 <- kmeans(datos_escalados, centers = 10, nstart = 25)

fviz_cluster(k10, data = datos_escalados)
```






